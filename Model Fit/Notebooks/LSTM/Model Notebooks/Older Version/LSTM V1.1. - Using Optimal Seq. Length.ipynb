{
 "cells": [
  {
   "cell_type": "code",
   "id": "914819af9bb112c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T16:38:55.705593Z",
     "start_time": "2025-05-11T16:36:11.252784Z"
    }
   },
   "source": [
    "pip install pandas numpy scikit-learn tqdm torch geneticalgorithm matplotlib optuna"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\azorb\\pycharmprojects\\predicting the yield curve\\.venv\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\azorb\\pycharmprojects\\predicting the yield curve\\.venv\\lib\\site-packages (2.2.5)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\azorb\\pycharmprojects\\predicting the yield curve\\.venv\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\azorb\\pycharmprojects\\predicting the yield curve\\.venv\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: torch in c:\\users\\azorb\\pycharmprojects\\predicting the yield curve\\.venv\\lib\\site-packages (2.7.0)\n",
      "Requirement already satisfied: geneticalgorithm in c:\\users\\azorb\\pycharmprojects\\predicting the yield curve\\.venv\\lib\\site-packages (1.0.2)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\azorb\\pycharmprojects\\predicting the yield curve\\.venv\\lib\\site-packages (3.10.3)\n",
      "Collecting optuna\n",
      "  Obtaining dependency information for optuna from https://files.pythonhosted.org/packages/d9/dd/0b593d1a5ee431b33a1fdf4ddb5911c312ed3bb598ef9e17457af2ee7b34/optuna-4.3.0-py3-none-any.whl.metadata\n",
      "  Using cached optuna-4.3.0-py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\azorb\\pycharmprojects\\predicting the yield curve\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\azorb\\pycharmprojects\\predicting the yield curve\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\azorb\\pycharmprojects\\predicting the yield curve\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\azorb\\pycharmprojects\\predicting the yield curve\\.venv\\lib\\site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\azorb\\pycharmprojects\\predicting the yield curve\\.venv\\lib\\site-packages (from scikit-learn) (1.5.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\azorb\\pycharmprojects\\predicting the yield curve\\.venv\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\azorb\\pycharmprojects\\predicting the yield curve\\.venv\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: filelock in c:\\users\\azorb\\pycharmprojects\\predicting the yield curve\\.venv\\lib\\site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\azorb\\pycharmprojects\\predicting the yield curve\\.venv\\lib\\site-packages (from torch) (4.13.2)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Obtaining dependency information for sympy>=1.13.3 from https://files.pythonhosted.org/packages/a2/09/77d55d46fd61b4a135c444fc97158ef34a095e5681d0a6c10b75bf356191/sympy-1.14.0-py3-none-any.whl.metadata\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: networkx in c:\\users\\azorb\\pycharmprojects\\predicting the yield curve\\.venv\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\azorb\\pycharmprojects\\predicting the yield curve\\.venv\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\azorb\\pycharmprojects\\predicting the yield curve\\.venv\\lib\\site-packages (from torch) (2025.3.2)\n",
      "Requirement already satisfied: func-timeout in c:\\users\\azorb\\pycharmprojects\\predicting the yield curve\\.venv\\lib\\site-packages (from geneticalgorithm) (4.3.5)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\azorb\\pycharmprojects\\predicting the yield curve\\.venv\\lib\\site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\azorb\\pycharmprojects\\predicting the yield curve\\.venv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\azorb\\pycharmprojects\\predicting the yield curve\\.venv\\lib\\site-packages (from matplotlib) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\azorb\\pycharmprojects\\predicting the yield curve\\.venv\\lib\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\azorb\\pycharmprojects\\predicting the yield curve\\.venv\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\azorb\\pycharmprojects\\predicting the yield curve\\.venv\\lib\\site-packages (from matplotlib) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\azorb\\pycharmprojects\\predicting the yield curve\\.venv\\lib\\site-packages (from matplotlib) (3.2.3)\n",
      "Collecting alembic>=1.5.0 (from optuna)\n",
      "  Obtaining dependency information for alembic>=1.5.0 from https://files.pythonhosted.org/packages/41/18/d89a443ed1ab9bcda16264716f809c663866d4ca8de218aa78fd50b38ead/alembic-1.15.2-py3-none-any.whl.metadata\n",
      "  Using cached alembic-1.15.2-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting colorlog (from optuna)\n",
      "  Obtaining dependency information for colorlog from https://files.pythonhosted.org/packages/e3/51/9b208e85196941db2f0654ad0357ca6388ab3ed67efdbfc799f35d1f83aa/colorlog-6.9.0-py3-none-any.whl.metadata\n",
      "  Using cached colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting sqlalchemy>=1.4.2 (from optuna)\n",
      "  Obtaining dependency information for sqlalchemy>=1.4.2 from https://files.pythonhosted.org/packages/9a/48/440946bf9dc4dc231f4f31ef0d316f7135bf41d4b86aaba0c0655150d370/sqlalchemy-2.0.40-cp311-cp311-win_amd64.whl.metadata\n",
      "  Using cached sqlalchemy-2.0.40-cp311-cp311-win_amd64.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\azorb\\pycharmprojects\\predicting the yield curve\\.venv\\lib\\site-packages (from optuna) (6.0.2)\n",
      "Collecting Mako (from alembic>=1.5.0->optuna)\n",
      "  Obtaining dependency information for Mako from https://files.pythonhosted.org/packages/87/fb/99f81ac72ae23375f22b7afdb7642aba97c00a713c217124420147681a2f/mako-1.3.10-py3-none-any.whl.metadata\n",
      "  Using cached mako-1.3.10-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\azorb\\pycharmprojects\\predicting the yield curve\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Collecting greenlet>=1 (from sqlalchemy>=1.4.2->optuna)\n",
      "  Obtaining dependency information for greenlet>=1 from https://files.pythonhosted.org/packages/c5/eb/7551c751a2ea6498907b2fcbe31d7a54b602ba5e8eb9550a9695ca25d25c/greenlet-3.2.2-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading greenlet-3.2.2-cp311-cp311-win_amd64.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\azorb\\pycharmprojects\\predicting the yield curve\\.venv\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\azorb\\pycharmprojects\\predicting the yield curve\\.venv\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Using cached optuna-4.3.0-py3-none-any.whl (386 kB)\n",
      "Using cached alembic-1.15.2-py3-none-any.whl (231 kB)\n",
      "Using cached sqlalchemy-2.0.40-cp311-cp311-win_amd64.whl (2.1 MB)\n",
      "Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Using cached colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
      "Downloading greenlet-3.2.2-cp311-cp311-win_amd64.whl (295 kB)\n",
      "   ---------------------------------------- 0.0/295.4 kB ? eta -:--:--\n",
      "   -------------------------------------- - 286.7/295.4 kB 8.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 295.4/295.4 kB 4.5 MB/s eta 0:00:00\n",
      "Using cached mako-1.3.10-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: sympy, Mako, greenlet, colorlog, sqlalchemy, alembic, optuna\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.11\n",
      "    Uninstalling sympy-1.11:\n",
      "      Successfully uninstalled sympy-1.11\n",
      "Successfully installed Mako-1.3.10 alembic-1.15.2 colorlog-6.9.0 greenlet-3.2.2 optuna-4.3.0 sqlalchemy-2.0.40 sympy-1.14.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "51eb54fc85eec128",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T12:10:53.784456Z",
     "start_time": "2025-05-12T12:10:53.672642Z"
    }
   },
   "source": [
    "# ---------------------- Importing Packages ---------------------- #\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import optuna\n",
    "import random\n",
    "\n",
    "# ---------------------- Reproducibility ---------------------- #\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed()\n",
    "\n",
    "# Select CPU or GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ---------------------- Configuration Settings ---------------------- #\n",
    "val_window_num_sequences = 252\n",
    "holdout_base = 756\n",
    "forecast_horizons = [1, 5, 21, 63, 252]\n",
    "num_epochs = 50\n",
    "patience = 10\n",
    "\n",
    "# Optimal sequence lengths per horizon\n",
    "sequence_length_map = {\n",
    "    1: 1512,\n",
    "    5: 1197,\n",
    "    21: 1323,\n",
    "    63: 1260,\n",
    "    252: 1323\n",
    "}\n",
    "\n",
    "# Hyperparameter ranges\n",
    "batch_size_options = [16, 32, 64, 128]\n",
    "hidden_dim_range = (32, 128)\n",
    "num_layers_options = [1, 2, 3]\n",
    "dropout_range = (0.0, 0.5)\n",
    "learning_rate_range = (1e-4, 1e-2)\n",
    "\n",
    "# ---------------------- LSTM Classifier ---------------------- #\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim, dropout=0.0):\n",
    "        super().__init__()\n",
    "        effective_dropout = dropout if num_layers > 1 else 0.0\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, dropout=effective_dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, (hn, _) = self.lstm(x)\n",
    "        return self.fc(hn[-1])\n",
    "\n",
    "# ---------------------- Generate X Sequences from Y Index ---------------------- #\n",
    "def generate_X_sequences_from_Y(X_df, Y_df_fold, sequence_length, forecast_horizon):\n",
    "    X_arr = X_df.values.astype(np.float32)\n",
    "    Y_arr = Y_df_fold.reindex(X_df.index).values.astype(np.float32)\n",
    "\n",
    "    X_seq, Y_seq, valid_timestamps = [], [], []\n",
    "    idx_map = {ts: i for i, ts in enumerate(X_df.index)}\n",
    "\n",
    "    for t in Y_df_fold.index:\n",
    "        target_idx = idx_map.get(t, None)\n",
    "        if target_idx is None:\n",
    "            continue\n",
    "\n",
    "        x_end = target_idx - forecast_horizon + 1\n",
    "        x_start = x_end - sequence_length\n",
    "\n",
    "        if x_start < 0 or x_end > len(X_arr):\n",
    "            continue\n",
    "\n",
    "        x_window = X_arr[x_start:x_end]\n",
    "        if x_window.shape[0] != sequence_length:\n",
    "            continue\n",
    "\n",
    "        if np.isnan(x_window).any():\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            Y_seq.append(Y_arr[target_idx])\n",
    "        except IndexError:\n",
    "            continue\n",
    "\n",
    "        X_seq.append(x_window)\n",
    "        valid_timestamps.append(t)\n",
    "\n",
    "    if len(X_seq) == 0:\n",
    "        return np.empty((0, sequence_length, X_df.shape[1])), np.empty((0, Y_arr.shape[1] if Y_arr.ndim > 1 else 1)), []\n",
    "\n",
    "    return np.array(X_seq), np.array(Y_seq), valid_timestamps\n",
    "\n",
    "# ---------------------- Feature Shifting ---------------------- #\n",
    "def shift_X_by_horizon(X_df, horizon):\n",
    "    return X_df.shift(horizon).dropna()\n",
    "\n",
    "# ---------------------- Standardization ---------------------- #\n",
    "def standardize_fold(X_train, X_val):\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), index=X_train.index, columns=X_train.columns)\n",
    "    X_val_scaled = pd.DataFrame(scaler.transform(X_val), index=X_val.index, columns=X_val.columns)\n",
    "    return X_train_scaled, X_val_scaled\n",
    "\n",
    "# ---------------------- Generate Expanding Folds ---------------------- #\n",
    "def get_expanding_folds(X_df, Y_df, forecast_horizon, sequence_length_map, val_window_num_sequences, holdout_base):\n",
    "    assert X_df.index.equals(Y_df.index)\n",
    "    sequence_length = sequence_length_map[forecast_horizon]\n",
    "    total_days = len(X_df)\n",
    "    val_window = val_window_num_sequences\n",
    "    min_train_window = sequence_length + forecast_horizon\n",
    "\n",
    "    folds = []\n",
    "    i = min_train_window\n",
    "    while i + val_window + holdout_base <= total_days:\n",
    "        train_end = i\n",
    "        val_start = i\n",
    "        val_end = i + val_window\n",
    "\n",
    "        X_train = X_df.iloc[:train_end].copy()\n",
    "        Y_train = Y_df.iloc[:train_end].copy()\n",
    "        X_val = X_df.iloc[val_start - sequence_length - forecast_horizon + 1:val_end].copy()\n",
    "        Y_val = Y_df.iloc[val_start:val_end].copy()\n",
    "\n",
    "        print(f\"[DEBUG] Fold window i={i}, val_start={val_start}, val_end={val_end}, X_val.shape={X_val.shape}, Y_val.shape={Y_val.shape}\")\n",
    "\n",
    "        if len(X_val) == 0 or len(Y_val) == 0:\n",
    "            i += val_window\n",
    "            continue\n",
    "\n",
    "        folds.append({\n",
    "            \"X_train\": X_train,\n",
    "            \"Y_train\": Y_train,\n",
    "            \"X_val\": X_val,\n",
    "            \"Y_val\": Y_val,\n",
    "            \"train_start_date\": X_train.index[0],\n",
    "            \"train_end_date\": X_train.index[-1],\n",
    "            \"val_start_date\": Y_val.index[0],\n",
    "            \"val_end_date\": Y_val.index[-1],\n",
    "            \"sequence_length\": sequence_length\n",
    "        })\n",
    "        i += val_window\n",
    "\n",
    "    print(f\"[INFO] Generated {len(folds)} non-overlapping folds for forecast horizon {forecast_horizon}\")\n",
    "    return folds\n",
    "\n",
    "# ---------------------- Objective Function for Optuna ---------------------- #\n",
    "def objective(trial, folds, forecast_horizon):\n",
    "    hidden_dim = trial.suggest_int(\"hidden_dim\", hidden_dim_range[0], hidden_dim_range[1])\n",
    "    num_layers = trial.suggest_categorical(\"num_layers\", num_layers_options)\n",
    "    dropout = trial.suggest_float(\"dropout\", dropout_range[0], dropout_range[1])\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", learning_rate_range[0], learning_rate_range[1], log=True)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", batch_size_options)\n",
    "\n",
    "    total_f1 = []\n",
    "\n",
    "    for fold in folds:\n",
    "        X_train_std, X_val_std = standardize_fold(fold[\"X_train\"], fold[\"X_val\"])\n",
    "        X_train_seq, Y_train_seq, _ = generate_X_sequences_from_Y(X_train_std, fold[\"Y_train\"], sequence_length_map[forecast_horizon], forecast_horizon)\n",
    "        X_val_seq, Y_val_seq, _ = generate_X_sequences_from_Y(X_val_std, fold[\"Y_val\"], sequence_length_map[forecast_horizon], forecast_horizon)\n",
    "\n",
    "        if len(X_train_seq) == 0 or len(X_val_seq) == 0:\n",
    "            continue\n",
    "\n",
    "        model = LSTMClassifier(X_train_seq.shape[2], hidden_dim, num_layers, Y_train_seq.shape[1], dropout).to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "        y_train_tensor = torch.tensor(Y_train_seq).float().to(device)\n",
    "        positive = y_train_tensor.sum()\n",
    "        negative = len(y_train_tensor) - positive\n",
    "        pos_weight = negative / (positive + 1e-6) if positive > 0 else torch.tensor(1.0).to(device)\n",
    "        criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "        train_loader = DataLoader(TensorDataset(torch.tensor(X_train_seq), y_train_tensor), batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(TensorDataset(torch.tensor(X_val_seq), torch.tensor(Y_val_seq)), batch_size=batch_size)\n",
    "\n",
    "        best_f1 = 0\n",
    "        patience_counter = 0\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            for xb, yb in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                loss = criterion(model(xb.to(device)), yb.to(device))\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            model.eval()\n",
    "            preds = []\n",
    "            with torch.no_grad():\n",
    "                for xb, _ in val_loader:\n",
    "                    preds.append(torch.sigmoid(model(xb.to(device))))\n",
    "            pred_tensor = torch.cat(preds, dim=0).squeeze()\n",
    "            preds_binary = (pred_tensor > 0.5).int().cpu().numpy()\n",
    "            y_true = torch.tensor(Y_val_seq).int().cpu().numpy()\n",
    "\n",
    "            f1 = f1_score(y_true, preds_binary, average=\"macro\")\n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    break\n",
    "\n",
    "        total_f1.append(best_f1)\n",
    "\n",
    "    return -np.mean(total_f1)\n",
    "\n",
    "# ---------------------- Run Optuna Optimization ---------------------- #\n",
    "def run_optuna_optimization(folds, forecast_horizon, n_trials=20):\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(lambda trial: objective(trial, folds, forecast_horizon), n_trials=n_trials)\n",
    "    return study.best_params, study.best_value\n",
    "\n",
    "# ---------------------- Run Optimization for All Horizons ---------------------- #\n",
    "def run_for_all_horizons(X_df, Y_df_dict):\n",
    "    all_results = {}\n",
    "    for h in forecast_horizons:\n",
    "        print(f\"\\n=== Forecast Horizon: {h} ===\")\n",
    "        Y_df = Y_df_dict[h]\n",
    "        X_shifted = shift_X_by_horizon(X_df, h)\n",
    "        Y_aligned = Y_df.loc[X_shifted.index]\n",
    "        X_final, Y_final = X_shifted, Y_aligned\n",
    "        folds = get_expanding_folds(X_final, Y_final, h, sequence_length_map, val_window_num_sequences, holdout_base)\n",
    "        best_params, best_score = run_optuna_optimization(folds, h)\n",
    "        print(f\"[RESULT] Horizon {h}: Best Params = {best_params}, Best F1 = {-best_score:.4f}\")\n",
    "        all_results[h] = {\"best_params\": best_params, \"best_score\": best_score}\n",
    "    return all_results\n"
   ],
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ---------------------- Importing Packages ---------------------- #\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "from tqdm import tqdm, trange\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import optuna\n",
    "import random\n",
    "import time\n",
    "import sys\n",
    "import gc\n",
    "\n",
    "# Force tqdm to flush output\n",
    "sys.stdout.flush()\n",
    "\n",
    "# ---------------------- Reproducibility ---------------------- #\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed()\n",
    "\n",
    "# Select CPU or GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"[INFO] Using device: {device}\")\n",
    "if device.type == \"cuda\":\n",
    "    print(f\"  - GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"  - Memory Allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "    print(f\"  - Memory Cached   : {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n",
    "\n",
    "# ---------------------- Configuration Settings ---------------------- #\n",
    "val_window_num_sequences = 252\n",
    "holdout_base = 756\n",
    "forecast_horizons = [5, 21, 63, 252]\n",
    "num_epochs = 30\n",
    "patience = 10\n",
    "\n",
    "sequence_length_map = {\n",
    "    1: 1512,\n",
    "    5: 1197,\n",
    "    21: 1323,\n",
    "    63: 1260,\n",
    "    252: 1323\n",
    "}\n",
    "\n",
    "batch_size_options = [32, 64]\n",
    "hidden_dim_range = (64, 256)\n",
    "num_layers_options = [1, 2, 3]\n",
    "dropout_range = (0.0, 0.5)\n",
    "learning_rate_range = (1e-4, 5e-2)\n",
    "\n",
    "# ---------------------- LSTM Classifier ---------------------- #\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim, dropout=0.0):\n",
    "        super().__init__()\n",
    "        effective_dropout = dropout if num_layers > 1 else 0.0\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, dropout=effective_dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, (hn, _) = self.lstm(x)\n",
    "        return self.fc(hn[-1])\n",
    "\n",
    "# ---------------------- Generate X Sequences from Y Index ---------------------- #\n",
    "def generate_X_sequences_from_Y(X_df, Y_df_fold, sequence_length, forecast_horizon):\n",
    "    X_arr = X_df.values.astype(np.float32)\n",
    "    Y_arr = Y_df_fold.reindex(X_df.index).values.astype(np.float32)\n",
    "\n",
    "    X_seq, Y_seq, valid_timestamps = [], [], []\n",
    "    idx_map = {ts: i for i, ts in enumerate(X_df.index)}\n",
    "\n",
    "    for t in Y_df_fold.index:\n",
    "        target_idx = idx_map.get(t, None)\n",
    "        if target_idx is None:\n",
    "            continue\n",
    "\n",
    "        x_end = target_idx - forecast_horizon + 1\n",
    "        x_start = x_end - sequence_length\n",
    "\n",
    "        if x_start < 0 or x_end > len(X_arr):\n",
    "            continue\n",
    "\n",
    "        x_window = X_arr[x_start:x_end]\n",
    "        if x_window.shape[0] != sequence_length:\n",
    "            continue\n",
    "\n",
    "        if np.isnan(x_window).any():\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            Y_seq.append(Y_arr[target_idx])\n",
    "        except IndexError:\n",
    "            continue\n",
    "\n",
    "        X_seq.append(x_window)\n",
    "        valid_timestamps.append(t)\n",
    "\n",
    "    if len(X_seq) == 0:\n",
    "        return np.empty((0, sequence_length, X_df.shape[1])), np.empty((0, Y_arr.shape[1] if Y_arr.ndim > 1 else 1)), []\n",
    "\n",
    "    return np.array(X_seq), np.array(Y_seq), valid_timestamps\n",
    "\n",
    "# ---------------------- Standardization ---------------------- #\n",
    "def standardize_fold(X_train, X_val):\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), index=X_train.index, columns=X_train.columns)\n",
    "    X_val_scaled = pd.DataFrame(scaler.transform(X_val), index=X_val.index, columns=X_val.columns)\n",
    "    return X_train_scaled, X_val_scaled\n",
    "\n",
    "# ---------------------- Generate Expanding Folds ---------------------- #\n",
    "def get_expanding_folds(X_df, Y_df, forecast_horizon, sequence_length_map, val_window_num_sequences, holdout_base):\n",
    "    assert X_df.index.equals(Y_df.index)\n",
    "    sequence_length = sequence_length_map[forecast_horizon]\n",
    "    total_days = len(X_df)\n",
    "    val_window = val_window_num_sequences\n",
    "    min_train_window = sequence_length + forecast_horizon\n",
    "\n",
    "    folds = []\n",
    "    i = min_train_window\n",
    "    while i + val_window + holdout_base <= total_days:\n",
    "        train_end = i\n",
    "        val_start = i\n",
    "        val_end = i + val_window\n",
    "\n",
    "        X_train = X_df.iloc[:train_end].copy()\n",
    "        Y_train = Y_df.iloc[:train_end].copy()\n",
    "        X_val = X_df.iloc[val_start - sequence_length - forecast_horizon + 1:val_end - forecast_horizon].copy()\n",
    "        Y_val = Y_df.iloc[val_start:val_end].copy()\n",
    "\n",
    "        print(f\"[DEBUG] Fold window i={i}, val_start={val_start}, val_end={val_end}, X_val.shape={X_val.shape}, Y_val.shape={Y_val.shape}\")\n",
    "\n",
    "        if len(X_val) == 0 or len(Y_val) == 0:\n",
    "            i += val_window\n",
    "            continue\n",
    "\n",
    "        folds.append({\n",
    "            \"X_train\": X_train,\n",
    "            \"Y_train\": Y_train,\n",
    "            \"X_val\": X_val,\n",
    "            \"Y_val\": Y_val,\n",
    "            \"train_start_date\": X_train.index[0],\n",
    "            \"train_end_date\": X_train.index[-1],\n",
    "            \"val_start_date\": Y_val.index[0],\n",
    "            \"val_end_date\": Y_val.index[-1],\n",
    "            \"sequence_length\": sequence_length\n",
    "        })\n",
    "        i += val_window\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    print(f\"[INFO] Generated {len(folds)} non-overlapping folds for forecast horizon {forecast_horizon}\")\n",
    "    return folds\n",
    "\n",
    "# ---------------------- Objective Function for Optuna ---------------------- #\n",
    "def objective(trial, folds, forecast_horizon):\n",
    "    hidden_dim = trial.suggest_int(\"hidden_dim\", hidden_dim_range[0], hidden_dim_range[1])\n",
    "    num_layers = trial.suggest_categorical(\"num_layers\", num_layers_options)\n",
    "    dropout = trial.suggest_float(\"dropout\", dropout_range[0], dropout_range[1])\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", learning_rate_range[0], learning_rate_range[1], log=True)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", batch_size_options)\n",
    "\n",
    "    total_f1, total_acc, total_prec, total_rec = [], [], [], []\n",
    "\n",
    "    for fold in tqdm(folds, desc=f\"Horizon {forecast_horizon} folds\", leave=False):\n",
    "        X_train_std, X_val_std = standardize_fold(fold[\"X_train\"], fold[\"X_val\"])\n",
    "        X_train_seq, Y_train_seq, _ = generate_X_sequences_from_Y(X_train_std, fold[\"Y_train\"], sequence_length_map[forecast_horizon], forecast_horizon)\n",
    "        X_val_seq, Y_val_seq, _ = generate_X_sequences_from_Y(X_val_std, fold[\"Y_val\"], sequence_length_map[forecast_horizon], forecast_horizon)\n",
    "\n",
    "        if len(X_train_seq) == 0 or len(X_val_seq) == 0:\n",
    "            continue\n",
    "\n",
    "        model = LSTMClassifier(X_train_seq.shape[2], hidden_dim, num_layers, Y_train_seq.shape[1], dropout).to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "        y_train_tensor = torch.tensor(Y_train_seq).float().to(device)\n",
    "        positive = y_train_tensor.sum()\n",
    "        negative = len(y_train_tensor) - positive\n",
    "        pos_weight = negative / (positive + 1e-6) if positive > 0 else torch.tensor(1.0).to(device)\n",
    "        criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "        train_loader = DataLoader(TensorDataset(torch.tensor(X_train_seq), y_train_tensor), batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(TensorDataset(torch.tensor(X_val_seq), torch.tensor(Y_val_seq)), batch_size=batch_size)\n",
    "\n",
    "        best_f1 = 0\n",
    "        patience_counter = 0\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            for xb, yb in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                loss = criterion(model(xb.to(device)), yb.to(device))\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            model.eval()\n",
    "            preds = []\n",
    "            with torch.no_grad():\n",
    "                for xb, _ in val_loader:\n",
    "                    preds.append(torch.sigmoid(model(xb.to(device))))\n",
    "            pred_tensor = torch.cat(preds, dim=0).squeeze()\n",
    "            preds_binary = (pred_tensor > 0.5).int().cpu().numpy()\n",
    "            y_true = torch.tensor(Y_val_seq).int().cpu().numpy()\n",
    "\n",
    "            f1 = f1_score(y_true, preds_binary, average=\"macro\")\n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    break\n",
    "\n",
    "        acc = accuracy_score(y_true, preds_binary)\n",
    "        prec = precision_score(y_true, preds_binary, average=\"macro\", zero_division=0)\n",
    "        rec = recall_score(y_true, preds_binary, average=\"macro\", zero_division=0)\n",
    "\n",
    "        total_f1.append(best_f1)\n",
    "        total_acc.append(acc)\n",
    "        total_prec.append(prec)\n",
    "        total_rec.append(rec)\n",
    "\n",
    "    print(f\"[METRICS] F1: {np.mean(total_f1):.4f} | Acc: {np.mean(total_acc):.4f} | Prec: {np.mean(total_prec):.4f} | Rec: {np.mean(total_rec):.4f}\")\n",
    "    return -np.mean(total_f1)\n",
    "\n",
    "# ---------------------- Run Optuna Optimization ---------------------- #\n",
    "def run_optuna_optimization(folds, forecast_horizon, n_trials=4):\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(lambda trial: objective(trial, folds, forecast_horizon), n_trials=n_trials)\n",
    "    return study.best_params, study.best_value\n",
    "\n",
    "# ---------------------- Run Optimization for All Horizons ---------------------- #\n",
    "def run_for_all_horizons(X_df, Y_df_dict):\n",
    "    all_results = {}\n",
    "    for h in trange(len(forecast_horizons), desc=\"Forecast Horizons\"):\n",
    "        horizon = forecast_horizons[h]\n",
    "        print(f\"\\n=== Forecast Horizon: {horizon} ===\")\n",
    "        Y_df = Y_df_dict[horizon]\n",
    "        X_final, Y_final = X_df.copy(), Y_df.copy()\n",
    "        folds = get_expanding_folds(X_final, Y_final, horizon, sequence_length_map, val_window_num_sequences, holdout_base)\n",
    "        best_params, best_score = run_optuna_optimization(folds, horizon)\n",
    "        print(f\"[RESULT] Horizon {horizon}: Best Params = {best_params}, Best F1 = {-best_score:.4f}\")\n",
    "        all_results[horizon] = {\"best_params\": best_params, \"best_score\": best_score}\n",
    "    return all_results"
   ],
   "id": "f805e66617f6d623"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load your prepared feature and target data\n",
    "X_df = pd.read_csv(\"X_df.csv\", index_col=0, parse_dates=True)\n",
    "Y_df_dict = {\n",
    "    1: pd.read_csv(\"Y_df_1.csv\", index_col=0, parse_dates=True),\n",
    "    5: pd.read_csv(\"Y_df_5.csv\", index_col=0, parse_dates=True),\n",
    "    21: pd.read_csv(\"Y_df_21.csv\", index_col=0, parse_dates=True),\n",
    "    63: pd.read_csv(\"Y_df_63.csv\", index_col=0, parse_dates=True),\n",
    "    252: pd.read_csv(\"Y_df_252.csv\", index_col=0, parse_dates=True)\n",
    "}\n",
    "\n",
    "# Run the full optimization pipeline\n",
    "results = run_for_all_horizons(X_df, Y_df_dict)\n"
   ],
   "id": "e2ec020dc1ecc876"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T12:11:42.400343Z",
     "start_time": "2025-05-12T12:10:57.087965Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ---------------------- Run Diagnostics ---------------------- #\n",
    "for h in forecast_horizons:\n",
    "    print(f\"\\n=== Forecast Horizon: {h} ===\")\n",
    "    sequence_length = sequence_length_map[h]\n",
    "\n",
    "    # Load X and Y\n",
    "    X_df = pd.read_csv(r'C:\\Users\\azorb\\PycharmProjects\\Predicting the Yield Curve\\Data Processing\\Output\\Independent\\X_df_filtered.csv', index_col=0, parse_dates=True)\n",
    "    Y_df = pd.read_csv(fr'C:\\Users\\azorb\\PycharmProjects\\Predicting the Yield Curve\\Data Processing\\Output\\Dependent\\Classification\\Y_df_change_dir_{h}.csv', index_col=0, parse_dates=True)\n",
    "    X_df.index = pd.to_datetime(X_df.index)\n",
    "    Y_df.index = pd.to_datetime(Y_df.index)\n",
    "\n",
    "    X_shifted = shift_X_by_horizon(X_df, h)\n",
    "    common_idx = X_shifted.index.intersection(Y_df.index)\n",
    "    X_aligned = X_shifted.loc[common_idx]\n",
    "    Y_aligned = Y_df.loc[common_idx]\n",
    "\n",
    "    folds = get_expanding_folds(X_aligned, Y_aligned, h, sequence_length_map, val_window_num_sequences, holdout_base)\n",
    "\n",
    "    for i, fold in enumerate(folds):\n",
    "        print(f\"[FOLD {i+1}]\")\n",
    "        print(f\"    ➤ Train: {fold['train_start_date']} → {fold['train_end_date']}\")\n",
    "        print(f\"    ➤ Val:   {fold['val_start_date']} → {fold['val_end_date']}\")\n",
    "\n",
    "        X_train_std, X_val_std = standardize_fold(fold[\"X_train\"], fold[\"X_val\"])\n",
    "        X_train_seq, Y_train_seq, ts_train = generate_X_sequences_from_Y(X_train_std, fold[\"Y_train\"], sequence_length, h)\n",
    "        X_val_seq, Y_val_seq, ts_val = generate_X_sequences_from_Y(X_val_std, fold[\"Y_val\"], sequence_length, h)\n",
    "\n",
    "        print(f\"    🧠 X_train_seq.shape = {X_train_seq.shape}\")\n",
    "        print(f\"    🧠 Y_train_seq.shape = {Y_train_seq.shape}\")\n",
    "        print(f\"    🧠 X_val_seq.shape   = {X_val_seq.shape}\")\n",
    "        print(f\"    🧠 Y_val_seq.shape   = {Y_val_seq.shape}\")\n",
    "        if len(ts_train):\n",
    "            print(f\"    ✔️ Train target window: {ts_train[0]} → {ts_train[-1]}\")\n",
    "        if len(ts_val):\n",
    "            print(f\"    ✔️ Val target window:   {ts_val[0]} → {ts_val[-1]}\")\n"
   ],
   "id": "becf7186409cbab1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Forecast Horizon: 1 ===\n",
      "[DEBUG] Fold window i=1513, val_start=1513, val_end=1765, X_val.shape=(1764, 74), Y_val.shape=(252, 6)\n",
      "[DEBUG] Fold window i=1765, val_start=1765, val_end=2017, X_val.shape=(1764, 74), Y_val.shape=(252, 6)\n",
      "[DEBUG] Fold window i=2017, val_start=2017, val_end=2269, X_val.shape=(1764, 74), Y_val.shape=(252, 6)\n",
      "[DEBUG] Fold window i=2269, val_start=2269, val_end=2521, X_val.shape=(1764, 74), Y_val.shape=(252, 6)\n",
      "[DEBUG] Fold window i=2521, val_start=2521, val_end=2773, X_val.shape=(1764, 74), Y_val.shape=(252, 6)\n",
      "[DEBUG] Fold window i=2773, val_start=2773, val_end=3025, X_val.shape=(1764, 74), Y_val.shape=(252, 6)\n",
      "[DEBUG] Fold window i=3025, val_start=3025, val_end=3277, X_val.shape=(1764, 74), Y_val.shape=(252, 6)\n",
      "[DEBUG] Fold window i=3277, val_start=3277, val_end=3529, X_val.shape=(1764, 74), Y_val.shape=(252, 6)\n",
      "[DEBUG] Fold window i=3529, val_start=3529, val_end=3781, X_val.shape=(1764, 74), Y_val.shape=(252, 6)\n",
      "[DEBUG] Fold window i=3781, val_start=3781, val_end=4033, X_val.shape=(1764, 74), Y_val.shape=(252, 6)\n",
      "[DEBUG] Fold window i=4033, val_start=4033, val_end=4285, X_val.shape=(1764, 74), Y_val.shape=(252, 6)\n",
      "[DEBUG] Fold window i=4285, val_start=4285, val_end=4537, X_val.shape=(1764, 74), Y_val.shape=(252, 6)\n",
      "[DEBUG] Fold window i=4537, val_start=4537, val_end=4789, X_val.shape=(1764, 74), Y_val.shape=(252, 6)\n",
      "[INFO] Generated 13 non-overlapping folds for forecast horizon 1\n",
      "[FOLD 1]\n",
      "    ➤ Train: 2003-09-26 00:00:00 → 2009-07-14 00:00:00\n",
      "    ➤ Val:   2009-07-15 00:00:00 → 2010-07-01 00:00:00\n",
      "    🧠 X_train_seq.shape = (1, 1512, 74)\n",
      "    🧠 Y_train_seq.shape = (1, 6)\n",
      "    🧠 X_val_seq.shape   = (252, 1512, 74)\n",
      "    🧠 Y_val_seq.shape   = (252, 6)\n",
      "    ✔️ Train target window: 2009-07-14 00:00:00 → 2009-07-14 00:00:00\n",
      "    ✔️ Val target window:   2009-07-15 00:00:00 → 2010-07-01 00:00:00\n",
      "[FOLD 2]\n",
      "    ➤ Train: 2003-09-26 00:00:00 → 2010-07-01 00:00:00\n",
      "    ➤ Val:   2010-07-02 00:00:00 → 2011-06-20 00:00:00\n",
      "    🧠 X_train_seq.shape = (253, 1512, 74)\n",
      "    🧠 Y_train_seq.shape = (253, 6)\n",
      "    🧠 X_val_seq.shape   = (252, 1512, 74)\n",
      "    🧠 Y_val_seq.shape   = (252, 6)\n",
      "    ✔️ Train target window: 2009-07-14 00:00:00 → 2010-07-01 00:00:00\n",
      "    ✔️ Val target window:   2010-07-02 00:00:00 → 2011-06-20 00:00:00\n",
      "[FOLD 3]\n",
      "    ➤ Train: 2003-09-26 00:00:00 → 2011-06-20 00:00:00\n",
      "    ➤ Val:   2011-06-21 00:00:00 → 2012-06-06 00:00:00\n",
      "    🧠 X_train_seq.shape = (505, 1512, 74)\n",
      "    🧠 Y_train_seq.shape = (505, 6)\n",
      "    🧠 X_val_seq.shape   = (252, 1512, 74)\n",
      "    🧠 Y_val_seq.shape   = (252, 6)\n",
      "    ✔️ Train target window: 2009-07-14 00:00:00 → 2011-06-20 00:00:00\n",
      "    ✔️ Val target window:   2011-06-21 00:00:00 → 2012-06-06 00:00:00\n",
      "[FOLD 4]\n",
      "    ➤ Train: 2003-09-26 00:00:00 → 2012-06-06 00:00:00\n",
      "    ➤ Val:   2012-06-07 00:00:00 → 2013-05-24 00:00:00\n",
      "    🧠 X_train_seq.shape = (757, 1512, 74)\n",
      "    🧠 Y_train_seq.shape = (757, 6)\n",
      "    🧠 X_val_seq.shape   = (252, 1512, 74)\n",
      "    🧠 Y_val_seq.shape   = (252, 6)\n",
      "    ✔️ Train target window: 2009-07-14 00:00:00 → 2012-06-06 00:00:00\n",
      "    ✔️ Val target window:   2012-06-07 00:00:00 → 2013-05-24 00:00:00\n",
      "[FOLD 5]\n",
      "    ➤ Train: 2003-09-26 00:00:00 → 2013-05-24 00:00:00\n",
      "    ➤ Val:   2013-05-27 00:00:00 → 2014-05-13 00:00:00\n",
      "    🧠 X_train_seq.shape = (1009, 1512, 74)\n",
      "    🧠 Y_train_seq.shape = (1009, 6)\n",
      "    🧠 X_val_seq.shape   = (252, 1512, 74)\n",
      "    🧠 Y_val_seq.shape   = (252, 6)\n",
      "    ✔️ Train target window: 2009-07-14 00:00:00 → 2013-05-24 00:00:00\n",
      "    ✔️ Val target window:   2013-05-27 00:00:00 → 2014-05-13 00:00:00\n",
      "[FOLD 6]\n",
      "    ➤ Train: 2003-09-26 00:00:00 → 2014-05-13 00:00:00\n",
      "    ➤ Val:   2014-05-14 00:00:00 → 2015-04-30 00:00:00\n",
      "    🧠 X_train_seq.shape = (1261, 1512, 74)\n",
      "    🧠 Y_train_seq.shape = (1261, 6)\n",
      "    🧠 X_val_seq.shape   = (252, 1512, 74)\n",
      "    🧠 Y_val_seq.shape   = (252, 6)\n",
      "    ✔️ Train target window: 2009-07-14 00:00:00 → 2014-05-13 00:00:00\n",
      "    ✔️ Val target window:   2014-05-14 00:00:00 → 2015-04-30 00:00:00\n",
      "[FOLD 7]\n",
      "    ➤ Train: 2003-09-26 00:00:00 → 2015-04-30 00:00:00\n",
      "    ➤ Val:   2015-05-01 00:00:00 → 2016-04-18 00:00:00\n",
      "    🧠 X_train_seq.shape = (1513, 1512, 74)\n",
      "    🧠 Y_train_seq.shape = (1513, 6)\n",
      "    🧠 X_val_seq.shape   = (252, 1512, 74)\n",
      "    🧠 Y_val_seq.shape   = (252, 6)\n",
      "    ✔️ Train target window: 2009-07-14 00:00:00 → 2015-04-30 00:00:00\n",
      "    ✔️ Val target window:   2015-05-01 00:00:00 → 2016-04-18 00:00:00\n",
      "[FOLD 8]\n",
      "    ➤ Train: 2003-09-26 00:00:00 → 2016-04-18 00:00:00\n",
      "    ➤ Val:   2016-04-19 00:00:00 → 2017-04-05 00:00:00\n",
      "    🧠 X_train_seq.shape = (1765, 1512, 74)\n",
      "    🧠 Y_train_seq.shape = (1765, 6)\n",
      "    🧠 X_val_seq.shape   = (252, 1512, 74)\n",
      "    🧠 Y_val_seq.shape   = (252, 6)\n",
      "    ✔️ Train target window: 2009-07-14 00:00:00 → 2016-04-18 00:00:00\n",
      "    ✔️ Val target window:   2016-04-19 00:00:00 → 2017-04-05 00:00:00\n",
      "[FOLD 9]\n",
      "    ➤ Train: 2003-09-26 00:00:00 → 2017-04-05 00:00:00\n",
      "    ➤ Val:   2017-04-06 00:00:00 → 2018-03-23 00:00:00\n",
      "    🧠 X_train_seq.shape = (2017, 1512, 74)\n",
      "    🧠 Y_train_seq.shape = (2017, 6)\n",
      "    🧠 X_val_seq.shape   = (252, 1512, 74)\n",
      "    🧠 Y_val_seq.shape   = (252, 6)\n",
      "    ✔️ Train target window: 2009-07-14 00:00:00 → 2017-04-05 00:00:00\n",
      "    ✔️ Val target window:   2017-04-06 00:00:00 → 2018-03-23 00:00:00\n",
      "[FOLD 10]\n",
      "    ➤ Train: 2003-09-26 00:00:00 → 2018-03-23 00:00:00\n",
      "    ➤ Val:   2018-03-26 00:00:00 → 2019-03-12 00:00:00\n",
      "    🧠 X_train_seq.shape = (2269, 1512, 74)\n",
      "    🧠 Y_train_seq.shape = (2269, 6)\n",
      "    🧠 X_val_seq.shape   = (252, 1512, 74)\n",
      "    🧠 Y_val_seq.shape   = (252, 6)\n",
      "    ✔️ Train target window: 2009-07-14 00:00:00 → 2018-03-23 00:00:00\n",
      "    ✔️ Val target window:   2018-03-26 00:00:00 → 2019-03-12 00:00:00\n",
      "[FOLD 11]\n",
      "    ➤ Train: 2003-09-26 00:00:00 → 2019-03-12 00:00:00\n",
      "    ➤ Val:   2019-03-13 00:00:00 → 2020-02-27 00:00:00\n",
      "    🧠 X_train_seq.shape = (2521, 1512, 74)\n",
      "    🧠 Y_train_seq.shape = (2521, 6)\n",
      "    🧠 X_val_seq.shape   = (252, 1512, 74)\n",
      "    🧠 Y_val_seq.shape   = (252, 6)\n",
      "    ✔️ Train target window: 2009-07-14 00:00:00 → 2019-03-12 00:00:00\n",
      "    ✔️ Val target window:   2019-03-13 00:00:00 → 2020-02-27 00:00:00\n",
      "[FOLD 12]\n",
      "    ➤ Train: 2003-09-26 00:00:00 → 2020-02-27 00:00:00\n",
      "    ➤ Val:   2020-02-28 00:00:00 → 2021-02-15 00:00:00\n",
      "    🧠 X_train_seq.shape = (2773, 1512, 74)\n",
      "    🧠 Y_train_seq.shape = (2773, 6)\n",
      "    🧠 X_val_seq.shape   = (252, 1512, 74)\n",
      "    🧠 Y_val_seq.shape   = (252, 6)\n",
      "    ✔️ Train target window: 2009-07-14 00:00:00 → 2020-02-27 00:00:00\n",
      "    ✔️ Val target window:   2020-02-28 00:00:00 → 2021-02-15 00:00:00\n",
      "[FOLD 13]\n",
      "    ➤ Train: 2003-09-26 00:00:00 → 2021-02-15 00:00:00\n",
      "    ➤ Val:   2021-02-16 00:00:00 → 2022-02-02 00:00:00\n",
      "    🧠 X_train_seq.shape = (3025, 1512, 74)\n",
      "    🧠 Y_train_seq.shape = (3025, 6)\n",
      "    🧠 X_val_seq.shape   = (252, 1512, 74)\n",
      "    🧠 Y_val_seq.shape   = (252, 6)\n",
      "    ✔️ Train target window: 2009-07-14 00:00:00 → 2021-02-15 00:00:00\n",
      "    ✔️ Val target window:   2021-02-16 00:00:00 → 2022-02-02 00:00:00\n",
      "\n",
      "=== Forecast Horizon: 5 ===\n",
      "[DEBUG] Fold window i=1202, val_start=1202, val_end=1454, X_val.shape=(1453, 74), Y_val.shape=(252, 6)\n",
      "[DEBUG] Fold window i=1454, val_start=1454, val_end=1706, X_val.shape=(1453, 74), Y_val.shape=(252, 6)\n",
      "[DEBUG] Fold window i=1706, val_start=1706, val_end=1958, X_val.shape=(1453, 74), Y_val.shape=(252, 6)\n",
      "[DEBUG] Fold window i=1958, val_start=1958, val_end=2210, X_val.shape=(1453, 74), Y_val.shape=(252, 6)\n",
      "[DEBUG] Fold window i=2210, val_start=2210, val_end=2462, X_val.shape=(1453, 74), Y_val.shape=(252, 6)\n",
      "[DEBUG] Fold window i=2462, val_start=2462, val_end=2714, X_val.shape=(1453, 74), Y_val.shape=(252, 6)\n",
      "[DEBUG] Fold window i=2714, val_start=2714, val_end=2966, X_val.shape=(1453, 74), Y_val.shape=(252, 6)\n",
      "[DEBUG] Fold window i=2966, val_start=2966, val_end=3218, X_val.shape=(1453, 74), Y_val.shape=(252, 6)\n",
      "[DEBUG] Fold window i=3218, val_start=3218, val_end=3470, X_val.shape=(1453, 74), Y_val.shape=(252, 6)\n",
      "[DEBUG] Fold window i=3470, val_start=3470, val_end=3722, X_val.shape=(1453, 74), Y_val.shape=(252, 6)\n",
      "[DEBUG] Fold window i=3722, val_start=3722, val_end=3974, X_val.shape=(1453, 74), Y_val.shape=(252, 6)\n",
      "[DEBUG] Fold window i=3974, val_start=3974, val_end=4226, X_val.shape=(1453, 74), Y_val.shape=(252, 6)\n",
      "[DEBUG] Fold window i=4226, val_start=4226, val_end=4478, X_val.shape=(1453, 74), Y_val.shape=(252, 6)\n",
      "[DEBUG] Fold window i=4478, val_start=4478, val_end=4730, X_val.shape=(1453, 74), Y_val.shape=(252, 6)\n",
      "[INFO] Generated 14 non-overlapping folds for forecast horizon 5\n",
      "[FOLD 1]\n",
      "    ➤ Train: 2003-10-02 00:00:00 → 2008-05-09 00:00:00\n",
      "    ➤ Val:   2008-05-12 00:00:00 → 2009-04-28 00:00:00\n",
      "    🧠 X_train_seq.shape = (1, 1197, 74)\n",
      "    🧠 Y_train_seq.shape = (1, 6)\n",
      "    🧠 X_val_seq.shape   = (252, 1197, 74)\n",
      "    🧠 Y_val_seq.shape   = (252, 6)\n",
      "    ✔️ Train target window: 2008-05-09 00:00:00 → 2008-05-09 00:00:00\n",
      "    ✔️ Val target window:   2008-05-12 00:00:00 → 2009-04-28 00:00:00\n",
      "[FOLD 2]\n",
      "    ➤ Train: 2003-10-02 00:00:00 → 2009-04-28 00:00:00\n",
      "    ➤ Val:   2009-04-29 00:00:00 → 2010-04-15 00:00:00\n",
      "    🧠 X_train_seq.shape = (253, 1197, 74)\n",
      "    🧠 Y_train_seq.shape = (253, 6)\n",
      "    🧠 X_val_seq.shape   = (252, 1197, 74)\n",
      "    🧠 Y_val_seq.shape   = (252, 6)\n",
      "    ✔️ Train target window: 2008-05-09 00:00:00 → 2009-04-28 00:00:00\n",
      "    ✔️ Val target window:   2009-04-29 00:00:00 → 2010-04-15 00:00:00\n",
      "[FOLD 3]\n",
      "    ➤ Train: 2003-10-02 00:00:00 → 2010-04-15 00:00:00\n",
      "    ➤ Val:   2010-04-16 00:00:00 → 2011-04-04 00:00:00\n",
      "    🧠 X_train_seq.shape = (505, 1197, 74)\n",
      "    🧠 Y_train_seq.shape = (505, 6)\n",
      "    🧠 X_val_seq.shape   = (252, 1197, 74)\n",
      "    🧠 Y_val_seq.shape   = (252, 6)\n",
      "    ✔️ Train target window: 2008-05-09 00:00:00 → 2010-04-15 00:00:00\n",
      "    ✔️ Val target window:   2010-04-16 00:00:00 → 2011-04-04 00:00:00\n",
      "[FOLD 4]\n",
      "    ➤ Train: 2003-10-02 00:00:00 → 2011-04-04 00:00:00\n",
      "    ➤ Val:   2011-04-05 00:00:00 → 2012-03-21 00:00:00\n",
      "    🧠 X_train_seq.shape = (757, 1197, 74)\n",
      "    🧠 Y_train_seq.shape = (757, 6)\n",
      "    🧠 X_val_seq.shape   = (252, 1197, 74)\n",
      "    🧠 Y_val_seq.shape   = (252, 6)\n",
      "    ✔️ Train target window: 2008-05-09 00:00:00 → 2011-04-04 00:00:00\n",
      "    ✔️ Val target window:   2011-04-05 00:00:00 → 2012-03-21 00:00:00\n",
      "[FOLD 5]\n",
      "    ➤ Train: 2003-10-02 00:00:00 → 2012-03-21 00:00:00\n",
      "    ➤ Val:   2012-03-22 00:00:00 → 2013-03-08 00:00:00\n",
      "    🧠 X_train_seq.shape = (1009, 1197, 74)\n",
      "    🧠 Y_train_seq.shape = (1009, 6)\n",
      "    🧠 X_val_seq.shape   = (252, 1197, 74)\n",
      "    🧠 Y_val_seq.shape   = (252, 6)\n",
      "    ✔️ Train target window: 2008-05-09 00:00:00 → 2012-03-21 00:00:00\n",
      "    ✔️ Val target window:   2012-03-22 00:00:00 → 2013-03-08 00:00:00\n",
      "[FOLD 6]\n",
      "    ➤ Train: 2003-10-02 00:00:00 → 2013-03-08 00:00:00\n",
      "    ➤ Val:   2013-03-11 00:00:00 → 2014-02-25 00:00:00\n",
      "    🧠 X_train_seq.shape = (1261, 1197, 74)\n",
      "    🧠 Y_train_seq.shape = (1261, 6)\n",
      "    🧠 X_val_seq.shape   = (252, 1197, 74)\n",
      "    🧠 Y_val_seq.shape   = (252, 6)\n",
      "    ✔️ Train target window: 2008-05-09 00:00:00 → 2013-03-08 00:00:00\n",
      "    ✔️ Val target window:   2013-03-11 00:00:00 → 2014-02-25 00:00:00\n",
      "[FOLD 7]\n",
      "    ➤ Train: 2003-10-02 00:00:00 → 2014-02-25 00:00:00\n",
      "    ➤ Val:   2014-02-26 00:00:00 → 2015-02-12 00:00:00\n",
      "    🧠 X_train_seq.shape = (1513, 1197, 74)\n",
      "    🧠 Y_train_seq.shape = (1513, 6)\n",
      "    🧠 X_val_seq.shape   = (252, 1197, 74)\n",
      "    🧠 Y_val_seq.shape   = (252, 6)\n",
      "    ✔️ Train target window: 2008-05-09 00:00:00 → 2014-02-25 00:00:00\n",
      "    ✔️ Val target window:   2014-02-26 00:00:00 → 2015-02-12 00:00:00\n",
      "[FOLD 8]\n",
      "    ➤ Train: 2003-10-02 00:00:00 → 2015-02-12 00:00:00\n",
      "    ➤ Val:   2015-02-13 00:00:00 → 2016-02-01 00:00:00\n",
      "    🧠 X_train_seq.shape = (1765, 1197, 74)\n",
      "    🧠 Y_train_seq.shape = (1765, 6)\n",
      "    🧠 X_val_seq.shape   = (252, 1197, 74)\n",
      "    🧠 Y_val_seq.shape   = (252, 6)\n",
      "    ✔️ Train target window: 2008-05-09 00:00:00 → 2015-02-12 00:00:00\n",
      "    ✔️ Val target window:   2015-02-13 00:00:00 → 2016-02-01 00:00:00\n",
      "[FOLD 9]\n",
      "    ➤ Train: 2003-10-02 00:00:00 → 2016-02-01 00:00:00\n",
      "    ➤ Val:   2016-02-02 00:00:00 → 2017-01-18 00:00:00\n",
      "    🧠 X_train_seq.shape = (2017, 1197, 74)\n",
      "    🧠 Y_train_seq.shape = (2017, 6)\n",
      "    🧠 X_val_seq.shape   = (252, 1197, 74)\n",
      "    🧠 Y_val_seq.shape   = (252, 6)\n",
      "    ✔️ Train target window: 2008-05-09 00:00:00 → 2016-02-01 00:00:00\n",
      "    ✔️ Val target window:   2016-02-02 00:00:00 → 2017-01-18 00:00:00\n",
      "[FOLD 10]\n",
      "    ➤ Train: 2003-10-02 00:00:00 → 2017-01-18 00:00:00\n",
      "    ➤ Val:   2017-01-19 00:00:00 → 2018-01-05 00:00:00\n",
      "    🧠 X_train_seq.shape = (2269, 1197, 74)\n",
      "    🧠 Y_train_seq.shape = (2269, 6)\n",
      "    🧠 X_val_seq.shape   = (252, 1197, 74)\n",
      "    🧠 Y_val_seq.shape   = (252, 6)\n",
      "    ✔️ Train target window: 2008-05-09 00:00:00 → 2017-01-18 00:00:00\n",
      "    ✔️ Val target window:   2017-01-19 00:00:00 → 2018-01-05 00:00:00\n",
      "[FOLD 11]\n",
      "    ➤ Train: 2003-10-02 00:00:00 → 2018-01-05 00:00:00\n",
      "    ➤ Val:   2018-01-08 00:00:00 → 2018-12-25 00:00:00\n",
      "    🧠 X_train_seq.shape = (2521, 1197, 74)\n",
      "    🧠 Y_train_seq.shape = (2521, 6)\n",
      "    🧠 X_val_seq.shape   = (252, 1197, 74)\n",
      "    🧠 Y_val_seq.shape   = (252, 6)\n",
      "    ✔️ Train target window: 2008-05-09 00:00:00 → 2018-01-05 00:00:00\n",
      "    ✔️ Val target window:   2018-01-08 00:00:00 → 2018-12-25 00:00:00\n",
      "[FOLD 12]\n",
      "    ➤ Train: 2003-10-02 00:00:00 → 2018-12-25 00:00:00\n",
      "    ➤ Val:   2018-12-26 00:00:00 → 2019-12-12 00:00:00\n",
      "    🧠 X_train_seq.shape = (2773, 1197, 74)\n",
      "    🧠 Y_train_seq.shape = (2773, 6)\n",
      "    🧠 X_val_seq.shape   = (252, 1197, 74)\n",
      "    🧠 Y_val_seq.shape   = (252, 6)\n",
      "    ✔️ Train target window: 2008-05-09 00:00:00 → 2018-12-25 00:00:00\n",
      "    ✔️ Val target window:   2018-12-26 00:00:00 → 2019-12-12 00:00:00\n",
      "[FOLD 13]\n",
      "    ➤ Train: 2003-10-02 00:00:00 → 2019-12-12 00:00:00\n",
      "    ➤ Val:   2019-12-13 00:00:00 → 2020-11-30 00:00:00\n",
      "    🧠 X_train_seq.shape = (3025, 1197, 74)\n",
      "    🧠 Y_train_seq.shape = (3025, 6)\n",
      "    🧠 X_val_seq.shape   = (252, 1197, 74)\n",
      "    🧠 Y_val_seq.shape   = (252, 6)\n",
      "    ✔️ Train target window: 2008-05-09 00:00:00 → 2019-12-12 00:00:00\n",
      "    ✔️ Val target window:   2019-12-13 00:00:00 → 2020-11-30 00:00:00\n",
      "[FOLD 14]\n",
      "    ➤ Train: 2003-10-02 00:00:00 → 2020-11-30 00:00:00\n",
      "    ➤ Val:   2020-12-01 00:00:00 → 2021-11-17 00:00:00\n",
      "    🧠 X_train_seq.shape = (3277, 1197, 74)\n",
      "    🧠 Y_train_seq.shape = (3277, 6)\n",
      "    🧠 X_val_seq.shape   = (252, 1197, 74)\n",
      "    🧠 Y_val_seq.shape   = (252, 6)\n",
      "    ✔️ Train target window: 2008-05-09 00:00:00 → 2020-11-30 00:00:00\n",
      "    ✔️ Val target window:   2020-12-01 00:00:00 → 2021-11-17 00:00:00\n",
      "\n",
      "=== Forecast Horizon: 21 ===\n",
      "[DEBUG] Fold window i=1344, val_start=1344, val_end=1596, X_val.shape=(1595, 74), Y_val.shape=(252, 6)\n",
      "[DEBUG] Fold window i=1596, val_start=1596, val_end=1848, X_val.shape=(1595, 74), Y_val.shape=(252, 6)\n",
      "[DEBUG] Fold window i=1848, val_start=1848, val_end=2100, X_val.shape=(1595, 74), Y_val.shape=(252, 6)\n",
      "[DEBUG] Fold window i=2100, val_start=2100, val_end=2352, X_val.shape=(1595, 74), Y_val.shape=(252, 6)\n",
      "[DEBUG] Fold window i=2352, val_start=2352, val_end=2604, X_val.shape=(1595, 74), Y_val.shape=(252, 6)\n",
      "[DEBUG] Fold window i=2604, val_start=2604, val_end=2856, X_val.shape=(1595, 74), Y_val.shape=(252, 6)\n",
      "[DEBUG] Fold window i=2856, val_start=2856, val_end=3108, X_val.shape=(1595, 74), Y_val.shape=(252, 6)\n",
      "[DEBUG] Fold window i=3108, val_start=3108, val_end=3360, X_val.shape=(1595, 74), Y_val.shape=(252, 6)\n",
      "[DEBUG] Fold window i=3360, val_start=3360, val_end=3612, X_val.shape=(1595, 74), Y_val.shape=(252, 6)\n",
      "[DEBUG] Fold window i=3612, val_start=3612, val_end=3864, X_val.shape=(1595, 74), Y_val.shape=(252, 6)\n",
      "[DEBUG] Fold window i=3864, val_start=3864, val_end=4116, X_val.shape=(1595, 74), Y_val.shape=(252, 6)\n",
      "[DEBUG] Fold window i=4116, val_start=4116, val_end=4368, X_val.shape=(1595, 74), Y_val.shape=(252, 6)\n",
      "[DEBUG] Fold window i=4368, val_start=4368, val_end=4620, X_val.shape=(1595, 74), Y_val.shape=(252, 6)\n",
      "[INFO] Generated 13 non-overlapping folds for forecast horizon 21\n",
      "[FOLD 1]\n",
      "    ➤ Train: 2003-10-24 00:00:00 → 2008-12-17 00:00:00\n",
      "    ➤ Val:   2008-12-18 00:00:00 → 2009-12-04 00:00:00\n",
      "    🧠 X_train_seq.shape = (1, 1323, 74)\n",
      "    🧠 Y_train_seq.shape = (1, 6)\n",
      "    🧠 X_val_seq.shape   = (252, 1323, 74)\n",
      "    🧠 Y_val_seq.shape   = (252, 6)\n",
      "    ✔️ Train target window: 2008-12-17 00:00:00 → 2008-12-17 00:00:00\n",
      "    ✔️ Val target window:   2008-12-18 00:00:00 → 2009-12-04 00:00:00\n",
      "[FOLD 2]\n",
      "    ➤ Train: 2003-10-24 00:00:00 → 2009-12-04 00:00:00\n",
      "    ➤ Val:   2009-12-07 00:00:00 → 2010-11-23 00:00:00\n",
      "    🧠 X_train_seq.shape = (253, 1323, 74)\n",
      "    🧠 Y_train_seq.shape = (253, 6)\n",
      "    🧠 X_val_seq.shape   = (252, 1323, 74)\n",
      "    🧠 Y_val_seq.shape   = (252, 6)\n",
      "    ✔️ Train target window: 2008-12-17 00:00:00 → 2009-12-04 00:00:00\n",
      "    ✔️ Val target window:   2009-12-07 00:00:00 → 2010-11-23 00:00:00\n",
      "[FOLD 3]\n",
      "    ➤ Train: 2003-10-24 00:00:00 → 2010-11-23 00:00:00\n",
      "    ➤ Val:   2010-11-24 00:00:00 → 2011-11-10 00:00:00\n",
      "    🧠 X_train_seq.shape = (505, 1323, 74)\n",
      "    🧠 Y_train_seq.shape = (505, 6)\n",
      "    🧠 X_val_seq.shape   = (252, 1323, 74)\n",
      "    🧠 Y_val_seq.shape   = (252, 6)\n",
      "    ✔️ Train target window: 2008-12-17 00:00:00 → 2010-11-23 00:00:00\n",
      "    ✔️ Val target window:   2010-11-24 00:00:00 → 2011-11-10 00:00:00\n",
      "[FOLD 4]\n",
      "    ➤ Train: 2003-10-24 00:00:00 → 2011-11-10 00:00:00\n",
      "    ➤ Val:   2011-11-11 00:00:00 → 2012-10-29 00:00:00\n",
      "    🧠 X_train_seq.shape = (757, 1323, 74)\n",
      "    🧠 Y_train_seq.shape = (757, 6)\n",
      "    🧠 X_val_seq.shape   = (252, 1323, 74)\n",
      "    🧠 Y_val_seq.shape   = (252, 6)\n",
      "    ✔️ Train target window: 2008-12-17 00:00:00 → 2011-11-10 00:00:00\n",
      "    ✔️ Val target window:   2011-11-11 00:00:00 → 2012-10-29 00:00:00\n",
      "[FOLD 5]\n",
      "    ➤ Train: 2003-10-24 00:00:00 → 2012-10-29 00:00:00\n",
      "    ➤ Val:   2012-10-30 00:00:00 → 2013-10-16 00:00:00\n",
      "    🧠 X_train_seq.shape = (1009, 1323, 74)\n",
      "    🧠 Y_train_seq.shape = (1009, 6)\n",
      "    🧠 X_val_seq.shape   = (252, 1323, 74)\n",
      "    🧠 Y_val_seq.shape   = (252, 6)\n",
      "    ✔️ Train target window: 2008-12-17 00:00:00 → 2012-10-29 00:00:00\n",
      "    ✔️ Val target window:   2012-10-30 00:00:00 → 2013-10-16 00:00:00\n",
      "[FOLD 6]\n",
      "    ➤ Train: 2003-10-24 00:00:00 → 2013-10-16 00:00:00\n",
      "    ➤ Val:   2013-10-17 00:00:00 → 2014-10-03 00:00:00\n",
      "    🧠 X_train_seq.shape = (1261, 1323, 74)\n",
      "    🧠 Y_train_seq.shape = (1261, 6)\n",
      "    🧠 X_val_seq.shape   = (252, 1323, 74)\n",
      "    🧠 Y_val_seq.shape   = (252, 6)\n",
      "    ✔️ Train target window: 2008-12-17 00:00:00 → 2013-10-16 00:00:00\n",
      "    ✔️ Val target window:   2013-10-17 00:00:00 → 2014-10-03 00:00:00\n",
      "[FOLD 7]\n",
      "    ➤ Train: 2003-10-24 00:00:00 → 2014-10-03 00:00:00\n",
      "    ➤ Val:   2014-10-06 00:00:00 → 2015-09-22 00:00:00\n",
      "    🧠 X_train_seq.shape = (1513, 1323, 74)\n",
      "    🧠 Y_train_seq.shape = (1513, 6)\n",
      "    🧠 X_val_seq.shape   = (252, 1323, 74)\n",
      "    🧠 Y_val_seq.shape   = (252, 6)\n",
      "    ✔️ Train target window: 2008-12-17 00:00:00 → 2014-10-03 00:00:00\n",
      "    ✔️ Val target window:   2014-10-06 00:00:00 → 2015-09-22 00:00:00\n",
      "[FOLD 8]\n",
      "    ➤ Train: 2003-10-24 00:00:00 → 2015-09-22 00:00:00\n",
      "    ➤ Val:   2015-09-23 00:00:00 → 2016-09-08 00:00:00\n",
      "    🧠 X_train_seq.shape = (1765, 1323, 74)\n",
      "    🧠 Y_train_seq.shape = (1765, 6)\n",
      "    🧠 X_val_seq.shape   = (252, 1323, 74)\n",
      "    🧠 Y_val_seq.shape   = (252, 6)\n",
      "    ✔️ Train target window: 2008-12-17 00:00:00 → 2015-09-22 00:00:00\n",
      "    ✔️ Val target window:   2015-09-23 00:00:00 → 2016-09-08 00:00:00\n",
      "[FOLD 9]\n",
      "    ➤ Train: 2003-10-24 00:00:00 → 2016-09-08 00:00:00\n",
      "    ➤ Val:   2016-09-09 00:00:00 → 2017-08-28 00:00:00\n",
      "    🧠 X_train_seq.shape = (2017, 1323, 74)\n",
      "    🧠 Y_train_seq.shape = (2017, 6)\n",
      "    🧠 X_val_seq.shape   = (252, 1323, 74)\n",
      "    🧠 Y_val_seq.shape   = (252, 6)\n",
      "    ✔️ Train target window: 2008-12-17 00:00:00 → 2016-09-08 00:00:00\n",
      "    ✔️ Val target window:   2016-09-09 00:00:00 → 2017-08-28 00:00:00\n",
      "[FOLD 10]\n",
      "    ➤ Train: 2003-10-24 00:00:00 → 2017-08-28 00:00:00\n",
      "    ➤ Val:   2017-08-29 00:00:00 → 2018-08-15 00:00:00\n",
      "    🧠 X_train_seq.shape = (2269, 1323, 74)\n",
      "    🧠 Y_train_seq.shape = (2269, 6)\n",
      "    🧠 X_val_seq.shape   = (252, 1323, 74)\n",
      "    🧠 Y_val_seq.shape   = (252, 6)\n",
      "    ✔️ Train target window: 2008-12-17 00:00:00 → 2017-08-28 00:00:00\n",
      "    ✔️ Val target window:   2017-08-29 00:00:00 → 2018-08-15 00:00:00\n",
      "[FOLD 11]\n",
      "    ➤ Train: 2003-10-24 00:00:00 → 2018-08-15 00:00:00\n",
      "    ➤ Val:   2018-08-16 00:00:00 → 2019-08-02 00:00:00\n",
      "    🧠 X_train_seq.shape = (2521, 1323, 74)\n",
      "    🧠 Y_train_seq.shape = (2521, 6)\n",
      "    🧠 X_val_seq.shape   = (252, 1323, 74)\n",
      "    🧠 Y_val_seq.shape   = (252, 6)\n",
      "    ✔️ Train target window: 2008-12-17 00:00:00 → 2018-08-15 00:00:00\n",
      "    ✔️ Val target window:   2018-08-16 00:00:00 → 2019-08-02 00:00:00\n",
      "[FOLD 12]\n",
      "    ➤ Train: 2003-10-24 00:00:00 → 2019-08-02 00:00:00\n",
      "    ➤ Val:   2019-08-05 00:00:00 → 2020-07-21 00:00:00\n",
      "    🧠 X_train_seq.shape = (2773, 1323, 74)\n",
      "    🧠 Y_train_seq.shape = (2773, 6)\n",
      "    🧠 X_val_seq.shape   = (252, 1323, 74)\n",
      "    🧠 Y_val_seq.shape   = (252, 6)\n",
      "    ✔️ Train target window: 2008-12-17 00:00:00 → 2019-08-02 00:00:00\n",
      "    ✔️ Val target window:   2019-08-05 00:00:00 → 2020-07-21 00:00:00\n",
      "[FOLD 13]\n",
      "    ➤ Train: 2003-10-24 00:00:00 → 2020-07-21 00:00:00\n",
      "    ➤ Val:   2020-07-22 00:00:00 → 2021-07-08 00:00:00\n",
      "    🧠 X_train_seq.shape = (3025, 1323, 74)\n",
      "    🧠 Y_train_seq.shape = (3025, 6)\n",
      "    🧠 X_val_seq.shape   = (252, 1323, 74)\n",
      "    🧠 Y_val_seq.shape   = (252, 6)\n",
      "    ✔️ Train target window: 2008-12-17 00:00:00 → 2020-07-21 00:00:00\n",
      "    ✔️ Val target window:   2020-07-22 00:00:00 → 2021-07-08 00:00:00\n",
      "\n",
      "=== Forecast Horizon: 63 ===\n",
      "[DEBUG] Fold window i=1323, val_start=1323, val_end=1575, X_val.shape=(1574, 74), Y_val.shape=(252, 6)\n",
      "[DEBUG] Fold window i=1575, val_start=1575, val_end=1827, X_val.shape=(1574, 74), Y_val.shape=(252, 6)\n",
      "[DEBUG] Fold window i=1827, val_start=1827, val_end=2079, X_val.shape=(1574, 74), Y_val.shape=(252, 6)\n",
      "[DEBUG] Fold window i=2079, val_start=2079, val_end=2331, X_val.shape=(1574, 74), Y_val.shape=(252, 6)\n",
      "[DEBUG] Fold window i=2331, val_start=2331, val_end=2583, X_val.shape=(1574, 74), Y_val.shape=(252, 6)\n",
      "[DEBUG] Fold window i=2583, val_start=2583, val_end=2835, X_val.shape=(1574, 74), Y_val.shape=(252, 6)\n",
      "[DEBUG] Fold window i=2835, val_start=2835, val_end=3087, X_val.shape=(1574, 74), Y_val.shape=(252, 6)\n",
      "[DEBUG] Fold window i=3087, val_start=3087, val_end=3339, X_val.shape=(1574, 74), Y_val.shape=(252, 6)\n",
      "[DEBUG] Fold window i=3339, val_start=3339, val_end=3591, X_val.shape=(1574, 74), Y_val.shape=(252, 6)\n",
      "[DEBUG] Fold window i=3591, val_start=3591, val_end=3843, X_val.shape=(1574, 74), Y_val.shape=(252, 6)\n",
      "[DEBUG] Fold window i=3843, val_start=3843, val_end=4095, X_val.shape=(1574, 74), Y_val.shape=(252, 6)\n",
      "[DEBUG] Fold window i=4095, val_start=4095, val_end=4347, X_val.shape=(1574, 74), Y_val.shape=(252, 6)\n",
      "[DEBUG] Fold window i=4347, val_start=4347, val_end=4599, X_val.shape=(1574, 74), Y_val.shape=(252, 6)\n",
      "[INFO] Generated 13 non-overlapping folds for forecast horizon 63\n",
      "[FOLD 1]\n",
      "    ➤ Train: 2003-12-23 00:00:00 → 2009-01-15 00:00:00\n",
      "    ➤ Val:   2009-01-16 00:00:00 → 2010-01-04 00:00:00\n",
      "    🧠 X_train_seq.shape = (1, 1260, 74)\n",
      "    🧠 Y_train_seq.shape = (1, 6)\n",
      "    🧠 X_val_seq.shape   = (252, 1260, 74)\n",
      "    🧠 Y_val_seq.shape   = (252, 6)\n",
      "    ✔️ Train target window: 2009-01-15 00:00:00 → 2009-01-15 00:00:00\n",
      "    ✔️ Val target window:   2009-01-16 00:00:00 → 2010-01-04 00:00:00\n",
      "[FOLD 2]\n",
      "    ➤ Train: 2003-12-23 00:00:00 → 2010-01-04 00:00:00\n",
      "    ➤ Val:   2010-01-05 00:00:00 → 2010-12-22 00:00:00\n",
      "    🧠 X_train_seq.shape = (253, 1260, 74)\n",
      "    🧠 Y_train_seq.shape = (253, 6)\n",
      "    🧠 X_val_seq.shape   = (252, 1260, 74)\n",
      "    🧠 Y_val_seq.shape   = (252, 6)\n",
      "    ✔️ Train target window: 2009-01-15 00:00:00 → 2010-01-04 00:00:00\n",
      "    ✔️ Val target window:   2010-01-05 00:00:00 → 2010-12-22 00:00:00\n",
      "[FOLD 3]\n",
      "    ➤ Train: 2003-12-23 00:00:00 → 2010-12-22 00:00:00\n",
      "    ➤ Val:   2010-12-23 00:00:00 → 2011-12-09 00:00:00\n",
      "    🧠 X_train_seq.shape = (505, 1260, 74)\n",
      "    🧠 Y_train_seq.shape = (505, 6)\n",
      "    🧠 X_val_seq.shape   = (252, 1260, 74)\n",
      "    🧠 Y_val_seq.shape   = (252, 6)\n",
      "    ✔️ Train target window: 2009-01-15 00:00:00 → 2010-12-22 00:00:00\n",
      "    ✔️ Val target window:   2010-12-23 00:00:00 → 2011-12-09 00:00:00\n",
      "[FOLD 4]\n",
      "    ➤ Train: 2003-12-23 00:00:00 → 2011-12-09 00:00:00\n",
      "    ➤ Val:   2011-12-12 00:00:00 → 2012-11-27 00:00:00\n",
      "    🧠 X_train_seq.shape = (757, 1260, 74)\n",
      "    🧠 Y_train_seq.shape = (757, 6)\n",
      "    🧠 X_val_seq.shape   = (252, 1260, 74)\n",
      "    🧠 Y_val_seq.shape   = (252, 6)\n",
      "    ✔️ Train target window: 2009-01-15 00:00:00 → 2011-12-09 00:00:00\n",
      "    ✔️ Val target window:   2011-12-12 00:00:00 → 2012-11-27 00:00:00\n",
      "[FOLD 5]\n",
      "    ➤ Train: 2003-12-23 00:00:00 → 2012-11-27 00:00:00\n",
      "    ➤ Val:   2012-11-28 00:00:00 → 2013-11-14 00:00:00\n",
      "    🧠 X_train_seq.shape = (1009, 1260, 74)\n",
      "    🧠 Y_train_seq.shape = (1009, 6)\n",
      "    🧠 X_val_seq.shape   = (252, 1260, 74)\n",
      "    🧠 Y_val_seq.shape   = (252, 6)\n",
      "    ✔️ Train target window: 2009-01-15 00:00:00 → 2012-11-27 00:00:00\n",
      "    ✔️ Val target window:   2012-11-28 00:00:00 → 2013-11-14 00:00:00\n",
      "[FOLD 6]\n",
      "    ➤ Train: 2003-12-23 00:00:00 → 2013-11-14 00:00:00\n",
      "    ➤ Val:   2013-11-15 00:00:00 → 2014-11-03 00:00:00\n",
      "    🧠 X_train_seq.shape = (1261, 1260, 74)\n",
      "    🧠 Y_train_seq.shape = (1261, 6)\n",
      "    🧠 X_val_seq.shape   = (252, 1260, 74)\n",
      "    🧠 Y_val_seq.shape   = (252, 6)\n",
      "    ✔️ Train target window: 2009-01-15 00:00:00 → 2013-11-14 00:00:00\n",
      "    ✔️ Val target window:   2013-11-15 00:00:00 → 2014-11-03 00:00:00\n",
      "[FOLD 7]\n",
      "    ➤ Train: 2003-12-23 00:00:00 → 2014-11-03 00:00:00\n",
      "    ➤ Val:   2014-11-04 00:00:00 → 2015-10-21 00:00:00\n",
      "    🧠 X_train_seq.shape = (1513, 1260, 74)\n",
      "    🧠 Y_train_seq.shape = (1513, 6)\n",
      "    🧠 X_val_seq.shape   = (252, 1260, 74)\n",
      "    🧠 Y_val_seq.shape   = (252, 6)\n",
      "    ✔️ Train target window: 2009-01-15 00:00:00 → 2014-11-03 00:00:00\n",
      "    ✔️ Val target window:   2014-11-04 00:00:00 → 2015-10-21 00:00:00\n",
      "[FOLD 8]\n",
      "    ➤ Train: 2003-12-23 00:00:00 → 2015-10-21 00:00:00\n",
      "    ➤ Val:   2015-10-22 00:00:00 → 2016-10-07 00:00:00\n",
      "    🧠 X_train_seq.shape = (1765, 1260, 74)\n",
      "    🧠 Y_train_seq.shape = (1765, 6)\n",
      "    🧠 X_val_seq.shape   = (252, 1260, 74)\n",
      "    🧠 Y_val_seq.shape   = (252, 6)\n",
      "    ✔️ Train target window: 2009-01-15 00:00:00 → 2015-10-21 00:00:00\n",
      "    ✔️ Val target window:   2015-10-22 00:00:00 → 2016-10-07 00:00:00\n",
      "[FOLD 9]\n",
      "    ➤ Train: 2003-12-23 00:00:00 → 2016-10-07 00:00:00\n",
      "    ➤ Val:   2016-10-10 00:00:00 → 2017-09-26 00:00:00\n",
      "    🧠 X_train_seq.shape = (2017, 1260, 74)\n",
      "    🧠 Y_train_seq.shape = (2017, 6)\n",
      "    🧠 X_val_seq.shape   = (252, 1260, 74)\n",
      "    🧠 Y_val_seq.shape   = (252, 6)\n",
      "    ✔️ Train target window: 2009-01-15 00:00:00 → 2016-10-07 00:00:00\n",
      "    ✔️ Val target window:   2016-10-10 00:00:00 → 2017-09-26 00:00:00\n",
      "[FOLD 10]\n",
      "    ➤ Train: 2003-12-23 00:00:00 → 2017-09-26 00:00:00\n",
      "    ➤ Val:   2017-09-27 00:00:00 → 2018-09-13 00:00:00\n",
      "    🧠 X_train_seq.shape = (2269, 1260, 74)\n",
      "    🧠 Y_train_seq.shape = (2269, 6)\n",
      "    🧠 X_val_seq.shape   = (252, 1260, 74)\n",
      "    🧠 Y_val_seq.shape   = (252, 6)\n",
      "    ✔️ Train target window: 2009-01-15 00:00:00 → 2017-09-26 00:00:00\n",
      "    ✔️ Val target window:   2017-09-27 00:00:00 → 2018-09-13 00:00:00\n",
      "[FOLD 11]\n",
      "    ➤ Train: 2003-12-23 00:00:00 → 2018-09-13 00:00:00\n",
      "    ➤ Val:   2018-09-14 00:00:00 → 2019-09-02 00:00:00\n",
      "    🧠 X_train_seq.shape = (2521, 1260, 74)\n",
      "    🧠 Y_train_seq.shape = (2521, 6)\n",
      "    🧠 X_val_seq.shape   = (252, 1260, 74)\n",
      "    🧠 Y_val_seq.shape   = (252, 6)\n",
      "    ✔️ Train target window: 2009-01-15 00:00:00 → 2018-09-13 00:00:00\n",
      "    ✔️ Val target window:   2018-09-14 00:00:00 → 2019-09-02 00:00:00\n",
      "[FOLD 12]\n",
      "    ➤ Train: 2003-12-23 00:00:00 → 2019-09-02 00:00:00\n",
      "    ➤ Val:   2019-09-03 00:00:00 → 2020-08-19 00:00:00\n",
      "    🧠 X_train_seq.shape = (2773, 1260, 74)\n",
      "    🧠 Y_train_seq.shape = (2773, 6)\n",
      "    🧠 X_val_seq.shape   = (252, 1260, 74)\n",
      "    🧠 Y_val_seq.shape   = (252, 6)\n",
      "    ✔️ Train target window: 2009-01-15 00:00:00 → 2019-09-02 00:00:00\n",
      "    ✔️ Val target window:   2019-09-03 00:00:00 → 2020-08-19 00:00:00\n",
      "[FOLD 13]\n",
      "    ➤ Train: 2003-12-23 00:00:00 → 2020-08-19 00:00:00\n",
      "    ➤ Val:   2020-08-20 00:00:00 → 2021-08-06 00:00:00\n",
      "    🧠 X_train_seq.shape = (3025, 1260, 74)\n",
      "    🧠 Y_train_seq.shape = (3025, 6)\n",
      "    🧠 X_val_seq.shape   = (252, 1260, 74)\n",
      "    🧠 Y_val_seq.shape   = (252, 6)\n",
      "    ✔️ Train target window: 2009-01-15 00:00:00 → 2020-08-19 00:00:00\n",
      "    ✔️ Val target window:   2020-08-20 00:00:00 → 2021-08-06 00:00:00\n",
      "\n",
      "=== Forecast Horizon: 252 ===\n",
      "[DEBUG] Fold window i=1575, val_start=1575, val_end=1827, X_val.shape=(1826, 74), Y_val.shape=(252, 6)\n",
      "[DEBUG] Fold window i=1827, val_start=1827, val_end=2079, X_val.shape=(1826, 74), Y_val.shape=(252, 6)\n",
      "[DEBUG] Fold window i=2079, val_start=2079, val_end=2331, X_val.shape=(1826, 74), Y_val.shape=(252, 6)\n",
      "[DEBUG] Fold window i=2331, val_start=2331, val_end=2583, X_val.shape=(1826, 74), Y_val.shape=(252, 6)\n",
      "[DEBUG] Fold window i=2583, val_start=2583, val_end=2835, X_val.shape=(1826, 74), Y_val.shape=(252, 6)\n",
      "[DEBUG] Fold window i=2835, val_start=2835, val_end=3087, X_val.shape=(1826, 74), Y_val.shape=(252, 6)\n",
      "[DEBUG] Fold window i=3087, val_start=3087, val_end=3339, X_val.shape=(1826, 74), Y_val.shape=(252, 6)\n",
      "[DEBUG] Fold window i=3339, val_start=3339, val_end=3591, X_val.shape=(1826, 74), Y_val.shape=(252, 6)\n",
      "[DEBUG] Fold window i=3591, val_start=3591, val_end=3843, X_val.shape=(1826, 74), Y_val.shape=(252, 6)\n",
      "[DEBUG] Fold window i=3843, val_start=3843, val_end=4095, X_val.shape=(1826, 74), Y_val.shape=(252, 6)\n",
      "[DEBUG] Fold window i=4095, val_start=4095, val_end=4347, X_val.shape=(1826, 74), Y_val.shape=(252, 6)\n",
      "[INFO] Generated 11 non-overlapping folds for forecast horizon 252\n",
      "[FOLD 1]\n",
      "    ➤ Train: 2004-09-13 00:00:00 → 2010-09-24 00:00:00\n",
      "    ➤ Val:   2010-09-27 00:00:00 → 2011-09-13 00:00:00\n",
      "    🧠 X_train_seq.shape = (1, 1323, 74)\n",
      "    🧠 Y_train_seq.shape = (1, 6)\n",
      "    🧠 X_val_seq.shape   = (252, 1323, 74)\n",
      "    🧠 Y_val_seq.shape   = (252, 6)\n",
      "    ✔️ Train target window: 2010-09-24 00:00:00 → 2010-09-24 00:00:00\n",
      "    ✔️ Val target window:   2010-09-27 00:00:00 → 2011-09-13 00:00:00\n",
      "[FOLD 2]\n",
      "    ➤ Train: 2004-09-13 00:00:00 → 2011-09-13 00:00:00\n",
      "    ➤ Val:   2011-09-14 00:00:00 → 2012-08-30 00:00:00\n",
      "    🧠 X_train_seq.shape = (253, 1323, 74)\n",
      "    🧠 Y_train_seq.shape = (253, 6)\n",
      "    🧠 X_val_seq.shape   = (252, 1323, 74)\n",
      "    🧠 Y_val_seq.shape   = (252, 6)\n",
      "    ✔️ Train target window: 2010-09-24 00:00:00 → 2011-09-13 00:00:00\n",
      "    ✔️ Val target window:   2011-09-14 00:00:00 → 2012-08-30 00:00:00\n",
      "[FOLD 3]\n",
      "    ➤ Train: 2004-09-13 00:00:00 → 2012-08-30 00:00:00\n",
      "    ➤ Val:   2012-08-31 00:00:00 → 2013-08-19 00:00:00\n",
      "    🧠 X_train_seq.shape = (505, 1323, 74)\n",
      "    🧠 Y_train_seq.shape = (505, 6)\n",
      "    🧠 X_val_seq.shape   = (252, 1323, 74)\n",
      "    🧠 Y_val_seq.shape   = (252, 6)\n",
      "    ✔️ Train target window: 2010-09-24 00:00:00 → 2012-08-30 00:00:00\n",
      "    ✔️ Val target window:   2012-08-31 00:00:00 → 2013-08-19 00:00:00\n",
      "[FOLD 4]\n",
      "    ➤ Train: 2004-09-13 00:00:00 → 2013-08-19 00:00:00\n",
      "    ➤ Val:   2013-08-20 00:00:00 → 2014-08-06 00:00:00\n",
      "    🧠 X_train_seq.shape = (757, 1323, 74)\n",
      "    🧠 Y_train_seq.shape = (757, 6)\n",
      "    🧠 X_val_seq.shape   = (252, 1323, 74)\n",
      "    🧠 Y_val_seq.shape   = (252, 6)\n",
      "    ✔️ Train target window: 2010-09-24 00:00:00 → 2013-08-19 00:00:00\n",
      "    ✔️ Val target window:   2013-08-20 00:00:00 → 2014-08-06 00:00:00\n",
      "[FOLD 5]\n",
      "    ➤ Train: 2004-09-13 00:00:00 → 2014-08-06 00:00:00\n",
      "    ➤ Val:   2014-08-07 00:00:00 → 2015-07-24 00:00:00\n",
      "    🧠 X_train_seq.shape = (1009, 1323, 74)\n",
      "    🧠 Y_train_seq.shape = (1009, 6)\n",
      "    🧠 X_val_seq.shape   = (252, 1323, 74)\n",
      "    🧠 Y_val_seq.shape   = (252, 6)\n",
      "    ✔️ Train target window: 2010-09-24 00:00:00 → 2014-08-06 00:00:00\n",
      "    ✔️ Val target window:   2014-08-07 00:00:00 → 2015-07-24 00:00:00\n",
      "[FOLD 6]\n",
      "    ➤ Train: 2004-09-13 00:00:00 → 2015-07-24 00:00:00\n",
      "    ➤ Val:   2015-07-27 00:00:00 → 2016-07-12 00:00:00\n",
      "    🧠 X_train_seq.shape = (1261, 1323, 74)\n",
      "    🧠 Y_train_seq.shape = (1261, 6)\n",
      "    🧠 X_val_seq.shape   = (252, 1323, 74)\n",
      "    🧠 Y_val_seq.shape   = (252, 6)\n",
      "    ✔️ Train target window: 2010-09-24 00:00:00 → 2015-07-24 00:00:00\n",
      "    ✔️ Val target window:   2015-07-27 00:00:00 → 2016-07-12 00:00:00\n",
      "[FOLD 7]\n",
      "    ➤ Train: 2004-09-13 00:00:00 → 2016-07-12 00:00:00\n",
      "    ➤ Val:   2016-07-13 00:00:00 → 2017-06-29 00:00:00\n",
      "    🧠 X_train_seq.shape = (1513, 1323, 74)\n",
      "    🧠 Y_train_seq.shape = (1513, 6)\n",
      "    🧠 X_val_seq.shape   = (252, 1323, 74)\n",
      "    🧠 Y_val_seq.shape   = (252, 6)\n",
      "    ✔️ Train target window: 2010-09-24 00:00:00 → 2016-07-12 00:00:00\n",
      "    ✔️ Val target window:   2016-07-13 00:00:00 → 2017-06-29 00:00:00\n",
      "[FOLD 8]\n",
      "    ➤ Train: 2004-09-13 00:00:00 → 2017-06-29 00:00:00\n",
      "    ➤ Val:   2017-06-30 00:00:00 → 2018-06-18 00:00:00\n",
      "    🧠 X_train_seq.shape = (1765, 1323, 74)\n",
      "    🧠 Y_train_seq.shape = (1765, 6)\n",
      "    🧠 X_val_seq.shape   = (252, 1323, 74)\n",
      "    🧠 Y_val_seq.shape   = (252, 6)\n",
      "    ✔️ Train target window: 2010-09-24 00:00:00 → 2017-06-29 00:00:00\n",
      "    ✔️ Val target window:   2017-06-30 00:00:00 → 2018-06-18 00:00:00\n",
      "[FOLD 9]\n",
      "    ➤ Train: 2004-09-13 00:00:00 → 2018-06-18 00:00:00\n",
      "    ➤ Val:   2018-06-19 00:00:00 → 2019-06-05 00:00:00\n",
      "    🧠 X_train_seq.shape = (2017, 1323, 74)\n",
      "    🧠 Y_train_seq.shape = (2017, 6)\n",
      "    🧠 X_val_seq.shape   = (252, 1323, 74)\n",
      "    🧠 Y_val_seq.shape   = (252, 6)\n",
      "    ✔️ Train target window: 2010-09-24 00:00:00 → 2018-06-18 00:00:00\n",
      "    ✔️ Val target window:   2018-06-19 00:00:00 → 2019-06-05 00:00:00\n",
      "[FOLD 10]\n",
      "    ➤ Train: 2004-09-13 00:00:00 → 2019-06-05 00:00:00\n",
      "    ➤ Val:   2019-06-06 00:00:00 → 2020-05-22 00:00:00\n",
      "    🧠 X_train_seq.shape = (2269, 1323, 74)\n",
      "    🧠 Y_train_seq.shape = (2269, 6)\n",
      "    🧠 X_val_seq.shape   = (252, 1323, 74)\n",
      "    🧠 Y_val_seq.shape   = (252, 6)\n",
      "    ✔️ Train target window: 2010-09-24 00:00:00 → 2019-06-05 00:00:00\n",
      "    ✔️ Val target window:   2019-06-06 00:00:00 → 2020-05-22 00:00:00\n",
      "[FOLD 11]\n",
      "    ➤ Train: 2004-09-13 00:00:00 → 2020-05-22 00:00:00\n",
      "    ➤ Val:   2020-05-25 00:00:00 → 2021-05-11 00:00:00\n",
      "    🧠 X_train_seq.shape = (2521, 1323, 74)\n",
      "    🧠 Y_train_seq.shape = (2521, 6)\n",
      "    🧠 X_val_seq.shape   = (252, 1323, 74)\n",
      "    🧠 Y_val_seq.shape   = (252, 6)\n",
      "    ✔️ Train target window: 2010-09-24 00:00:00 → 2020-05-22 00:00:00\n",
      "    ✔️ Val target window:   2020-05-25 00:00:00 → 2021-05-11 00:00:00\n"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T17:33:28.624145Z",
     "start_time": "2025-05-11T17:33:27.619672Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ---------------------- Run Diagnostics ---------------------- #\n",
    "for h in forecast_horizons:\n",
    "    print(f\"\\n=== Forecast Horizon: {h} ===\")\n",
    "    sequence_length = sequence_length_map[h]\n",
    "\n",
    "    # Load features and target\n",
    "    X_df = pd.read_csv(r'C:\\Users\\azorb\\PycharmProjects\\Predicting the Yield Curve\\Data Processing\\Output\\Independent\\X_df_filtered.csv', index_col=0, parse_dates=True)\n",
    "    Y_df = pd.read_csv(fr'C:\\Users\\azorb\\PycharmProjects\\Predicting the Yield Curve\\Data Processing\\Output\\Dependent\\Classification\\Y_df_change_dir_{h}.csv', index_col=0, parse_dates=True)\n",
    "\n",
    "    X_df.index = pd.to_datetime(X_df.index)\n",
    "    Y_df.index = pd.to_datetime(Y_df.index)\n",
    "\n",
    "    # Align\n",
    "    X_shifted = shift_X_by_horizon(X_df, h)\n",
    "    common_idx = X_shifted.index.intersection(Y_df.index)\n",
    "    X_aligned = X_shifted.loc[common_idx]\n",
    "    Y_aligned = Y_df.loc[common_idx]\n",
    "\n",
    "    # Create folds\n",
    "    folds = get_expanding_folds(X_aligned, Y_aligned, h, sequence_length_map, val_window_num_sequences, holdout_base)\n",
    "\n",
    "    for i, fold in enumerate(folds):\n",
    "        print(f\"[FOLD {i+1}]\")\n",
    "        print(f\"    ➤ Train: {fold['train_start_date']} → {fold['train_end_date']}\")\n",
    "        print(f\"    ➤ Val:   {fold['val_start_date']} → {fold['val_end_date']}\")\n",
    "\n",
    "        # Scale\n",
    "        X_train_std, X_val_std = standardize_fold(fold['X_train'], fold['X_val'])\n",
    "\n",
    "        # Sequence generation\n",
    "        X_train_seq, Y_train_seq, ts_train = generate_X_sequences_from_Y(X_train_std, fold['Y_train'], sequence_length, h)\n",
    "        X_val_seq, Y_val_seq, ts_val = generate_X_sequences_from_Y(X_val_std, fold['Y_val'], sequence_length, h)\n",
    "\n",
    "        print(f\"    🧠 X_train_seq.shape = {X_train_seq.shape}\")\n",
    "        print(f\"    🧠 Y_train_seq.shape = {Y_train_seq.shape}\")\n",
    "        print(f\"    🧠 X_val_seq.shape   = {X_val_seq.shape}\")\n",
    "        print(f\"    🧠 Y_val_seq.shape   = {Y_val_seq.shape}\")\n",
    "\n",
    "        if len(ts_train) > 0:\n",
    "            print(f\"    ✔️ First train target date: {ts_train[0]}\")\n",
    "            print(f\"    ✔️ Last  train target date: {ts_train[-1]}\")\n",
    "        if len(ts_val) > 0:\n",
    "            print(f\"    ✔️ First val target date:   {ts_val[0]}\")\n",
    "            print(f\"    ✔️ Last  val target date:   {ts_val[-1]}\")\n"
   ],
   "id": "94beaadfd0bb4bcf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Forecast Horizon: 1 ===\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'X_df_filtered.csv'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mFileNotFoundError\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[31]\u001B[39m\u001B[32m, line 7\u001B[39m\n\u001B[32m      4\u001B[39m sequence_length = sequence_length_map[h]\n\u001B[32m      6\u001B[39m \u001B[38;5;66;03m# Load X and Y\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m7\u001B[39m X_df = \u001B[43mpd\u001B[49m\u001B[43m.\u001B[49m\u001B[43mread_csv\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43mf\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mX_df_filtered.csv\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindex_col\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparse_dates\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[32m      8\u001B[39m Y_df = pd.read_csv(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mY_df_change_dir_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mh\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m.csv\u001B[39m\u001B[33m\"\u001B[39m, index_col=\u001B[32m0\u001B[39m, parse_dates=\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[32m      9\u001B[39m X_df.index = pd.to_datetime(X_df.index)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\Predicting the Yield Curve\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001B[39m, in \u001B[36mread_csv\u001B[39m\u001B[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001B[39m\n\u001B[32m   1013\u001B[39m kwds_defaults = _refine_defaults_read(\n\u001B[32m   1014\u001B[39m     dialect,\n\u001B[32m   1015\u001B[39m     delimiter,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1022\u001B[39m     dtype_backend=dtype_backend,\n\u001B[32m   1023\u001B[39m )\n\u001B[32m   1024\u001B[39m kwds.update(kwds_defaults)\n\u001B[32m-> \u001B[39m\u001B[32m1026\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\Predicting the Yield Curve\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001B[39m, in \u001B[36m_read\u001B[39m\u001B[34m(filepath_or_buffer, kwds)\u001B[39m\n\u001B[32m    617\u001B[39m _validate_names(kwds.get(\u001B[33m\"\u001B[39m\u001B[33mnames\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m))\n\u001B[32m    619\u001B[39m \u001B[38;5;66;03m# Create the parser.\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m620\u001B[39m parser = \u001B[43mTextFileReader\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    622\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m chunksize \u001B[38;5;129;01mor\u001B[39;00m iterator:\n\u001B[32m    623\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m parser\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\Predicting the Yield Curve\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001B[39m, in \u001B[36mTextFileReader.__init__\u001B[39m\u001B[34m(self, f, engine, **kwds)\u001B[39m\n\u001B[32m   1617\u001B[39m     \u001B[38;5;28mself\u001B[39m.options[\u001B[33m\"\u001B[39m\u001B[33mhas_index_names\u001B[39m\u001B[33m\"\u001B[39m] = kwds[\u001B[33m\"\u001B[39m\u001B[33mhas_index_names\u001B[39m\u001B[33m\"\u001B[39m]\n\u001B[32m   1619\u001B[39m \u001B[38;5;28mself\u001B[39m.handles: IOHandles | \u001B[38;5;28;01mNone\u001B[39;00m = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1620\u001B[39m \u001B[38;5;28mself\u001B[39m._engine = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_make_engine\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mengine\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\Predicting the Yield Curve\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001B[39m, in \u001B[36mTextFileReader._make_engine\u001B[39m\u001B[34m(self, f, engine)\u001B[39m\n\u001B[32m   1878\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33mb\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m mode:\n\u001B[32m   1879\u001B[39m         mode += \u001B[33m\"\u001B[39m\u001B[33mb\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m-> \u001B[39m\u001B[32m1880\u001B[39m \u001B[38;5;28mself\u001B[39m.handles = \u001B[43mget_handle\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1881\u001B[39m \u001B[43m    \u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1882\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1883\u001B[39m \u001B[43m    \u001B[49m\u001B[43mencoding\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43moptions\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mencoding\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1884\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcompression\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43moptions\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mcompression\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1885\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmemory_map\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43moptions\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mmemory_map\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1886\u001B[39m \u001B[43m    \u001B[49m\u001B[43mis_text\u001B[49m\u001B[43m=\u001B[49m\u001B[43mis_text\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1887\u001B[39m \u001B[43m    \u001B[49m\u001B[43merrors\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43moptions\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mencoding_errors\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mstrict\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1888\u001B[39m \u001B[43m    \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43moptions\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mstorage_options\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1889\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1890\u001B[39m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m.handles \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1891\u001B[39m f = \u001B[38;5;28mself\u001B[39m.handles.handle\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\Predicting the Yield Curve\\.venv\\Lib\\site-packages\\pandas\\io\\common.py:873\u001B[39m, in \u001B[36mget_handle\u001B[39m\u001B[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[39m\n\u001B[32m    868\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(handle, \u001B[38;5;28mstr\u001B[39m):\n\u001B[32m    869\u001B[39m     \u001B[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001B[39;00m\n\u001B[32m    870\u001B[39m     \u001B[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001B[39;00m\n\u001B[32m    871\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m ioargs.encoding \u001B[38;5;129;01mand\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33mb\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m ioargs.mode:\n\u001B[32m    872\u001B[39m         \u001B[38;5;66;03m# Encoding\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m873\u001B[39m         handle = \u001B[38;5;28mopen\u001B[39m(\n\u001B[32m    874\u001B[39m             handle,\n\u001B[32m    875\u001B[39m             ioargs.mode,\n\u001B[32m    876\u001B[39m             encoding=ioargs.encoding,\n\u001B[32m    877\u001B[39m             errors=errors,\n\u001B[32m    878\u001B[39m             newline=\u001B[33m\"\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    879\u001B[39m         )\n\u001B[32m    880\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    881\u001B[39m         \u001B[38;5;66;03m# Binary mode\u001B[39;00m\n\u001B[32m    882\u001B[39m         handle = \u001B[38;5;28mopen\u001B[39m(handle, ioargs.mode)\n",
      "\u001B[31mFileNotFoundError\u001B[39m: [Errno 2] No such file or directory: 'X_df_filtered.csv'"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cebc013986906094",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-11 11:28:07,326] A new study created in memory with name: no-name-af171a09-e814-4e7e-be0d-3b412b82f251\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Forecast Horizon: 1 ===\n",
      "[INFO] Generated 8 folds for forecast horizon 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-11 11:28:36,273] Trial 0 finished with value: -0.4247086622213306 and parameters: {'hidden_dim': 98, 'num_layers': 1, 'dropout': 0.2276490796352143, 'learning_rate': 0.0004122960660443855, 'batch_size': 128}. Best is trial 0 with value: -0.4247086622213306.\n",
      "[I 2025-05-11 11:29:32,680] Trial 1 finished with value: -0.4863466531338707 and parameters: {'hidden_dim': 126, 'num_layers': 2, 'dropout': 0.2895212610734162, 'learning_rate': 0.0035656209620512943, 'batch_size': 16}. Best is trial 1 with value: -0.4863466531338707.\n",
      "[I 2025-05-11 11:29:59,235] Trial 2 finished with value: -0.47561975036998483 and parameters: {'hidden_dim': 69, 'num_layers': 2, 'dropout': 0.32774522966985487, 'learning_rate': 0.0002023610070712248, 'batch_size': 128}. Best is trial 1 with value: -0.4863466531338707.\n",
      "[I 2025-05-11 11:30:30,935] Trial 3 finished with value: -0.45252246447470207 and parameters: {'hidden_dim': 70, 'num_layers': 2, 'dropout': 0.172800444770923, 'learning_rate': 0.0006842077861165199, 'batch_size': 64}. Best is trial 1 with value: -0.4863466531338707.\n",
      "[I 2025-05-11 11:30:57,069] Trial 4 finished with value: -0.4073950520735591 and parameters: {'hidden_dim': 102, 'num_layers': 1, 'dropout': 0.465670823934781, 'learning_rate': 0.0011471735921989446, 'batch_size': 128}. Best is trial 1 with value: -0.4863466531338707.\n",
      "[I 2025-05-11 11:31:33,199] Trial 5 finished with value: -0.542395619668946 and parameters: {'hidden_dim': 44, 'num_layers': 2, 'dropout': 0.37182586652106697, 'learning_rate': 0.003683427723693766, 'batch_size': 32}. Best is trial 5 with value: -0.542395619668946.\n",
      "[I 2025-05-11 11:32:26,882] Trial 6 finished with value: -0.36366266508011286 and parameters: {'hidden_dim': 57, 'num_layers': 1, 'dropout': 0.4013286100829695, 'learning_rate': 0.00010706897930051511, 'batch_size': 16}. Best is trial 5 with value: -0.542395619668946.\n",
      "[I 2025-05-11 11:33:13,656] Trial 7 finished with value: -0.3794413815537883 and parameters: {'hidden_dim': 53, 'num_layers': 1, 'dropout': 0.4653613370530977, 'learning_rate': 0.00024252042305606493, 'batch_size': 16}. Best is trial 5 with value: -0.542395619668946.\n",
      "[I 2025-05-11 11:33:55,679] Trial 8 finished with value: -0.4409998220926702 and parameters: {'hidden_dim': 37, 'num_layers': 1, 'dropout': 0.3455474223685915, 'learning_rate': 0.0014640970710719453, 'batch_size': 32}. Best is trial 5 with value: -0.542395619668946.\n",
      "[I 2025-05-11 11:34:39,604] Trial 9 finished with value: -0.3508068610037843 and parameters: {'hidden_dim': 123, 'num_layers': 1, 'dropout': 0.13985357153313097, 'learning_rate': 0.0018351221615148835, 'batch_size': 32}. Best is trial 5 with value: -0.542395619668946.\n",
      "[I 2025-05-11 11:35:15,802] Trial 10 finished with value: -0.48115246471893985 and parameters: {'hidden_dim': 38, 'num_layers': 2, 'dropout': 0.39895158317300594, 'learning_rate': 0.0098036460637825, 'batch_size': 32}. Best is trial 5 with value: -0.542395619668946.\n",
      "[I 2025-05-11 11:36:13,994] Trial 11 finished with value: -0.5337341337120267 and parameters: {'hidden_dim': 124, 'num_layers': 2, 'dropout': 0.26154056600253356, 'learning_rate': 0.0053647679271085985, 'batch_size': 16}. Best is trial 5 with value: -0.542395619668946.\n",
      "[I 2025-05-11 11:36:44,469] Trial 12 finished with value: -0.4573456020913468 and parameters: {'hidden_dim': 92, 'num_layers': 2, 'dropout': 0.24588587224945457, 'learning_rate': 0.006190263427460322, 'batch_size': 64}. Best is trial 5 with value: -0.542395619668946.\n",
      "[I 2025-05-11 11:37:24,948] Trial 13 finished with value: -0.48850680106227223 and parameters: {'hidden_dim': 85, 'num_layers': 2, 'dropout': 0.2547223946102023, 'learning_rate': 0.004090581101446843, 'batch_size': 32}. Best is trial 5 with value: -0.542395619668946.\n",
      "[I 2025-05-11 11:38:23,584] Trial 14 finished with value: -0.5508163671502837 and parameters: {'hidden_dim': 111, 'num_layers': 2, 'dropout': 0.382515337195703, 'learning_rate': 0.0026744242996887867, 'batch_size': 16}. Best is trial 14 with value: -0.5508163671502837.\n",
      "[I 2025-05-11 11:39:27,594] Trial 15 finished with value: -0.48514580760519055 and parameters: {'hidden_dim': 108, 'num_layers': 2, 'dropout': 0.3860522776695834, 'learning_rate': 0.002382539431542395, 'batch_size': 16}. Best is trial 14 with value: -0.5508163671502837.\n",
      "[I 2025-05-11 11:40:09,815] Trial 16 finished with value: -0.5395415996242655 and parameters: {'hidden_dim': 75, 'num_layers': 2, 'dropout': 0.3586449566995066, 'learning_rate': 0.0029813888908597607, 'batch_size': 32}. Best is trial 14 with value: -0.5508163671502837.\n",
      "[I 2025-05-11 11:40:47,834] Trial 17 finished with value: -0.3710838262200759 and parameters: {'hidden_dim': 112, 'num_layers': 2, 'dropout': 0.436281632529667, 'learning_rate': 0.0008587600960319674, 'batch_size': 64}. Best is trial 14 with value: -0.5508163671502837.\n",
      "[I 2025-05-11 11:41:24,708] Trial 18 finished with value: -0.5413143791825815 and parameters: {'hidden_dim': 53, 'num_layers': 2, 'dropout': 0.3039807426145141, 'learning_rate': 0.007801929526883494, 'batch_size': 32}. Best is trial 14 with value: -0.5508163671502837.\n",
      "[I 2025-05-11 11:42:17,242] Trial 19 finished with value: -0.5516305400778261 and parameters: {'hidden_dim': 84, 'num_layers': 2, 'dropout': 0.424931900458637, 'learning_rate': 0.0022486783930516746, 'batch_size': 16}. Best is trial 19 with value: -0.5516305400778261.\n",
      "[I 2025-05-11 11:43:16,430] Trial 20 finished with value: -0.5537320976479521 and parameters: {'hidden_dim': 87, 'num_layers': 2, 'dropout': 0.49644774829525107, 'learning_rate': 0.002040176001804273, 'batch_size': 16}. Best is trial 20 with value: -0.5537320976479521.\n",
      "[I 2025-05-11 11:44:05,609] Trial 21 finished with value: -0.5518282541547677 and parameters: {'hidden_dim': 83, 'num_layers': 2, 'dropout': 0.4905458725447064, 'learning_rate': 0.0021083396844553317, 'batch_size': 16}. Best is trial 20 with value: -0.5537320976479521.\n",
      "[I 2025-05-11 11:44:54,966] Trial 22 finished with value: -0.5204160183688047 and parameters: {'hidden_dim': 84, 'num_layers': 2, 'dropout': 0.47811602909709716, 'learning_rate': 0.001773379421354805, 'batch_size': 16}. Best is trial 20 with value: -0.5537320976479521.\n",
      "[I 2025-05-11 11:45:51,437] Trial 23 finished with value: -0.47593540989737015 and parameters: {'hidden_dim': 90, 'num_layers': 2, 'dropout': 0.4860185676405147, 'learning_rate': 0.0006310939907100848, 'batch_size': 16}. Best is trial 20 with value: -0.5537320976479521.\n",
      "[I 2025-05-11 11:46:42,159] Trial 24 finished with value: -0.4647574149891582 and parameters: {'hidden_dim': 78, 'num_layers': 2, 'dropout': 0.4364092319461347, 'learning_rate': 0.0014461630787260583, 'batch_size': 16}. Best is trial 20 with value: -0.5537320976479521.\n",
      "[I 2025-05-11 11:47:34,629] Trial 25 finished with value: -0.5513988136272476 and parameters: {'hidden_dim': 65, 'num_layers': 2, 'dropout': 0.49502472459534735, 'learning_rate': 0.0023185625112591667, 'batch_size': 16}. Best is trial 20 with value: -0.5537320976479521.\n",
      "[I 2025-05-11 11:48:32,564] Trial 26 finished with value: -0.46513979864153276 and parameters: {'hidden_dim': 94, 'num_layers': 2, 'dropout': 0.43213228842490103, 'learning_rate': 0.0009622749728567902, 'batch_size': 16}. Best is trial 20 with value: -0.5537320976479521.\n",
      "[I 2025-05-11 11:49:24,746] Trial 27 finished with value: -0.5548958508540118 and parameters: {'hidden_dim': 84, 'num_layers': 2, 'dropout': 0.45710576033956674, 'learning_rate': 0.004688535399076738, 'batch_size': 16}. Best is trial 27 with value: -0.5548958508540118.\n",
      "[I 2025-05-11 11:50:32,538] Trial 28 finished with value: -0.49436891392523913 and parameters: {'hidden_dim': 103, 'num_layers': 2, 'dropout': 0.49886180239933764, 'learning_rate': 0.0049724410768192005, 'batch_size': 16}. Best is trial 27 with value: -0.5548958508540118.\n",
      "[I 2025-05-11 11:50:59,573] Trial 29 finished with value: -0.47653983208182404 and parameters: {'hidden_dim': 77, 'num_layers': 1, 'dropout': 0.45742694033233117, 'learning_rate': 0.007153752567255967, 'batch_size': 128}. Best is trial 27 with value: -0.5548958508540118.\n",
      "[I 2025-05-11 11:50:59,612] A new study created in memory with name: no-name-43720cb5-7b7c-438a-a166-9bcf4f3f6ad0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RESULT] Horizon 1: Best Params = {'hidden_dim': 84, 'num_layers': 2, 'dropout': 0.45710576033956674, 'learning_rate': 0.004688535399076738, 'batch_size': 16}, Best F1 = 0.5549\n",
      "\n",
      "=== Forecast Horizon: 5 ===\n",
      "[INFO] Generated 8 folds for forecast horizon 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-11 11:51:53,612] Trial 0 finished with value: -0.32030847099713206 and parameters: {'hidden_dim': 69, 'num_layers': 2, 'dropout': 0.3148187575916974, 'learning_rate': 0.0004258480821630382, 'batch_size': 16}. Best is trial 0 with value: -0.32030847099713206.\n",
      "[I 2025-05-11 11:53:18,725] Trial 1 finished with value: -0.5108031206681662 and parameters: {'hidden_dim': 110, 'num_layers': 2, 'dropout': 0.4595409993291274, 'learning_rate': 0.0013881241700925003, 'batch_size': 16}. Best is trial 1 with value: -0.5108031206681662.\n",
      "[I 2025-05-11 11:54:06,179] Trial 2 finished with value: -0.4090906835667759 and parameters: {'hidden_dim': 54, 'num_layers': 1, 'dropout': 0.19127230712937734, 'learning_rate': 0.0027655499093746216, 'batch_size': 16}. Best is trial 1 with value: -0.5108031206681662.\n",
      "[I 2025-05-11 11:54:52,791] Trial 3 finished with value: -0.40551460319060095 and parameters: {'hidden_dim': 32, 'num_layers': 2, 'dropout': 0.4940906514754819, 'learning_rate': 0.0014083285381945923, 'batch_size': 16}. Best is trial 1 with value: -0.5108031206681662.\n",
      "[I 2025-05-11 11:55:44,728] Trial 4 finished with value: -0.5482964955974565 and parameters: {'hidden_dim': 92, 'num_layers': 1, 'dropout': 0.21557525139220088, 'learning_rate': 0.0004883533160474792, 'batch_size': 16}. Best is trial 4 with value: -0.5482964955974565.\n",
      "[I 2025-05-11 11:56:37,750] Trial 5 finished with value: -0.38907904594421694 and parameters: {'hidden_dim': 62, 'num_layers': 2, 'dropout': 0.14504453970453846, 'learning_rate': 0.0028191052323359935, 'batch_size': 16}. Best is trial 4 with value: -0.5482964955974565.\n",
      "[I 2025-05-11 11:57:32,447] Trial 6 finished with value: -0.3428224960888501 and parameters: {'hidden_dim': 67, 'num_layers': 2, 'dropout': 0.13825318002446219, 'learning_rate': 0.00010854554636903587, 'batch_size': 16}. Best is trial 4 with value: -0.5482964955974565.\n",
      "[I 2025-05-11 11:58:03,318] Trial 7 finished with value: -0.37875736987682374 and parameters: {'hidden_dim': 42, 'num_layers': 1, 'dropout': 0.4245816099157754, 'learning_rate': 0.0017174807962681959, 'batch_size': 128}. Best is trial 4 with value: -0.5482964955974565.\n",
      "[I 2025-05-11 11:58:18,650] Trial 8 finished with value: -0.3747038539140163 and parameters: {'hidden_dim': 65, 'num_layers': 1, 'dropout': 0.36764298450034527, 'learning_rate': 0.0006073613382120015, 'batch_size': 128}. Best is trial 4 with value: -0.5482964955974565.\n",
      "[I 2025-05-11 11:59:18,146] Trial 9 finished with value: -0.5280174581943466 and parameters: {'hidden_dim': 89, 'num_layers': 2, 'dropout': 0.1826686584041527, 'learning_rate': 0.0005193343983442457, 'batch_size': 16}. Best is trial 4 with value: -0.5482964955974565.\n",
      "[I 2025-05-11 11:59:54,542] Trial 10 finished with value: -0.44412845375674975 and parameters: {'hidden_dim': 127, 'num_layers': 1, 'dropout': 0.2544727529007419, 'learning_rate': 0.008269422823644084, 'batch_size': 64}. Best is trial 4 with value: -0.5482964955974565.\n",
      "[I 2025-05-11 12:00:26,825] Trial 11 finished with value: -0.38850650999669056 and parameters: {'hidden_dim': 92, 'num_layers': 1, 'dropout': 0.2326539733966481, 'learning_rate': 0.00016772114610428437, 'batch_size': 32}. Best is trial 4 with value: -0.5482964955974565.\n",
      "[I 2025-05-11 12:00:59,996] Trial 12 finished with value: -0.4202293448879884 and parameters: {'hidden_dim': 91, 'num_layers': 2, 'dropout': 0.10948615205260623, 'learning_rate': 0.0003259914965299727, 'batch_size': 64}. Best is trial 4 with value: -0.5482964955974565.\n",
      "[I 2025-05-11 12:01:44,790] Trial 13 finished with value: -0.49470509065715146 and parameters: {'hidden_dim': 87, 'num_layers': 1, 'dropout': 0.20466390516347696, 'learning_rate': 0.0006954244015925634, 'batch_size': 32}. Best is trial 4 with value: -0.5482964955974565.\n",
      "[I 2025-05-11 12:02:39,200] Trial 14 finished with value: -0.4889427518498497 and parameters: {'hidden_dim': 105, 'num_layers': 1, 'dropout': 0.3027896693446828, 'learning_rate': 0.00024462097421865017, 'batch_size': 16}. Best is trial 4 with value: -0.5482964955974565.\n",
      "[I 2025-05-11 12:04:07,405] Trial 15 finished with value: -0.5183525825023996 and parameters: {'hidden_dim': 105, 'num_layers': 2, 'dropout': 0.18176590558664146, 'learning_rate': 0.0007311124853090881, 'batch_size': 16}. Best is trial 4 with value: -0.5482964955974565.\n",
      "[I 2025-05-11 12:04:36,093] Trial 16 finished with value: -0.41804524734781906 and parameters: {'hidden_dim': 79, 'num_layers': 1, 'dropout': 0.2514849902476427, 'learning_rate': 0.00041311843970192507, 'batch_size': 128}. Best is trial 4 with value: -0.5482964955974565.\n",
      "[I 2025-05-11 12:05:23,475] Trial 17 finished with value: -0.3965040733669276 and parameters: {'hidden_dim': 120, 'num_layers': 2, 'dropout': 0.35782900260867734, 'learning_rate': 0.0001842041789923965, 'batch_size': 32}. Best is trial 4 with value: -0.5482964955974565.\n",
      "[I 2025-05-11 12:05:55,105] Trial 18 finished with value: -0.49420358862011005 and parameters: {'hidden_dim': 79, 'num_layers': 2, 'dropout': 0.27271825757874524, 'learning_rate': 0.007303783838236285, 'batch_size': 64}. Best is trial 4 with value: -0.5482964955974565.\n",
      "[I 2025-05-11 12:06:56,694] Trial 19 finished with value: -0.5387685795333084 and parameters: {'hidden_dim': 101, 'num_layers': 1, 'dropout': 0.1592214462315923, 'learning_rate': 0.0008590902860078946, 'batch_size': 16}. Best is trial 4 with value: -0.5482964955974565.\n",
      "[I 2025-05-11 12:07:55,438] Trial 20 finished with value: -0.4357266470460114 and parameters: {'hidden_dim': 99, 'num_layers': 1, 'dropout': 0.10939436207052472, 'learning_rate': 0.0009646355124459833, 'batch_size': 16}. Best is trial 4 with value: -0.5482964955974565.\n",
      "[I 2025-05-11 12:08:57,629] Trial 21 finished with value: -0.47024826928182073 and parameters: {'hidden_dim': 115, 'num_layers': 1, 'dropout': 0.16170244357958863, 'learning_rate': 0.000550070768423101, 'batch_size': 16}. Best is trial 4 with value: -0.5482964955974565.\n",
      "[I 2025-05-11 12:09:58,006] Trial 22 finished with value: -0.5068330467937397 and parameters: {'hidden_dim': 98, 'num_layers': 1, 'dropout': 0.20773666347653494, 'learning_rate': 0.00092509968586336, 'batch_size': 16}. Best is trial 4 with value: -0.5482964955974565.\n",
      "[I 2025-05-11 12:10:58,259] Trial 23 finished with value: -0.4029592267918176 and parameters: {'hidden_dim': 85, 'num_layers': 1, 'dropout': 0.166470569493044, 'learning_rate': 0.0002970281418046604, 'batch_size': 16}. Best is trial 4 with value: -0.5482964955974565.\n",
      "[I 2025-05-11 12:11:50,723] Trial 24 finished with value: -0.5015550872630761 and parameters: {'hidden_dim': 96, 'num_layers': 1, 'dropout': 0.22577751381693978, 'learning_rate': 0.002063806421288825, 'batch_size': 16}. Best is trial 4 with value: -0.5482964955974565.\n",
      "[I 2025-05-11 12:12:40,360] Trial 25 finished with value: -0.43863620217507443 and parameters: {'hidden_dim': 76, 'num_layers': 1, 'dropout': 0.13352645078304223, 'learning_rate': 0.00443794093270938, 'batch_size': 16}. Best is trial 4 with value: -0.5482964955974565.\n",
      "[I 2025-05-11 12:13:31,360] Trial 26 finished with value: -0.5056152034207302 and parameters: {'hidden_dim': 103, 'num_layers': 2, 'dropout': 0.2761293557074084, 'learning_rate': 0.00046626275466761145, 'batch_size': 32}. Best is trial 4 with value: -0.5482964955974565.\n",
      "[I 2025-05-11 12:14:03,130] Trial 27 finished with value: -0.4616822092206109 and parameters: {'hidden_dim': 86, 'num_layers': 1, 'dropout': 0.21333343310517902, 'learning_rate': 0.0010632982757183382, 'batch_size': 64}. Best is trial 4 with value: -0.5482964955974565.\n",
      "[I 2025-05-11 12:14:31,169] Trial 28 finished with value: -0.3522267544122575 and parameters: {'hidden_dim': 112, 'num_layers': 2, 'dropout': 0.1705967558860694, 'learning_rate': 0.00021475864001864692, 'batch_size': 128}. Best is trial 4 with value: -0.5482964955974565.\n",
      "[I 2025-05-11 12:15:23,446] Trial 29 finished with value: -0.5073774591538954 and parameters: {'hidden_dim': 76, 'num_layers': 1, 'dropout': 0.3270635752379857, 'learning_rate': 0.00042147183229534037, 'batch_size': 16}. Best is trial 4 with value: -0.5482964955974565.\n",
      "[I 2025-05-11 12:15:23,475] A new study created in memory with name: no-name-dd8eb031-3c47-4d3c-b550-8f509e847ab7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RESULT] Horizon 5: Best Params = {'hidden_dim': 92, 'num_layers': 1, 'dropout': 0.21557525139220088, 'learning_rate': 0.0004883533160474792, 'batch_size': 16}, Best F1 = 0.5483\n",
      "\n",
      "=== Forecast Horizon: 21 ===\n",
      "[INFO] Generated 8 folds for forecast horizon 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-11 12:16:11,188] Trial 0 finished with value: -0.44044651986851063 and parameters: {'hidden_dim': 91, 'num_layers': 1, 'dropout': 0.21806228453940268, 'learning_rate': 0.007851756185983515, 'batch_size': 16}. Best is trial 0 with value: -0.44044651986851063.\n",
      "[I 2025-05-11 12:16:53,270] Trial 1 finished with value: -0.43664169096580063 and parameters: {'hidden_dim': 52, 'num_layers': 1, 'dropout': 0.16366168509839463, 'learning_rate': 0.0004151428371325672, 'batch_size': 16}. Best is trial 0 with value: -0.44044651986851063.\n",
      "[I 2025-05-11 12:17:26,153] Trial 2 finished with value: -0.5485618193240784 and parameters: {'hidden_dim': 79, 'num_layers': 2, 'dropout': 0.41867948098018837, 'learning_rate': 0.00027072268062802376, 'batch_size': 64}. Best is trial 2 with value: -0.5485618193240784.\n",
      "[I 2025-05-11 12:18:27,819] Trial 3 finished with value: -0.5671924723740861 and parameters: {'hidden_dim': 96, 'num_layers': 2, 'dropout': 0.45793954832430006, 'learning_rate': 0.00029665560470062323, 'batch_size': 16}. Best is trial 3 with value: -0.5671924723740861.\n",
      "[I 2025-05-11 12:18:51,458] Trial 4 finished with value: -0.5399082192838752 and parameters: {'hidden_dim': 33, 'num_layers': 1, 'dropout': 0.4131516648967831, 'learning_rate': 0.0012968209989763558, 'batch_size': 128}. Best is trial 3 with value: -0.5671924723740861.\n",
      "[I 2025-05-11 12:19:10,991] Trial 5 finished with value: -0.46862013241187295 and parameters: {'hidden_dim': 73, 'num_layers': 2, 'dropout': 0.28022047890014035, 'learning_rate': 0.00023818542840675, 'batch_size': 32}. Best is trial 3 with value: -0.5671924723740861.\n",
      "[I 2025-05-11 12:19:18,281] Trial 6 finished with value: -0.4655641366090608 and parameters: {'hidden_dim': 124, 'num_layers': 1, 'dropout': 0.45386483747768425, 'learning_rate': 0.00012713034484845373, 'batch_size': 128}. Best is trial 3 with value: -0.5671924723740861.\n",
      "[I 2025-05-11 12:19:26,113] Trial 7 finished with value: -0.4278695521611816 and parameters: {'hidden_dim': 35, 'num_layers': 2, 'dropout': 0.491886438646717, 'learning_rate': 0.00021681706018808223, 'batch_size': 128}. Best is trial 3 with value: -0.5671924723740861.\n",
      "[I 2025-05-11 12:20:22,679] Trial 8 finished with value: -0.4869331369841907 and parameters: {'hidden_dim': 123, 'num_layers': 2, 'dropout': 0.1027860550736929, 'learning_rate': 0.001135201888496935, 'batch_size': 16}. Best is trial 3 with value: -0.5671924723740861.\n",
      "[I 2025-05-11 12:20:28,182] Trial 9 finished with value: -0.562493567518673 and parameters: {'hidden_dim': 58, 'num_layers': 1, 'dropout': 0.38765302580581273, 'learning_rate': 0.004944898185176338, 'batch_size': 128}. Best is trial 3 with value: -0.5671924723740861.\n",
      "[I 2025-05-11 12:20:55,709] Trial 10 finished with value: -0.5822836016817544 and parameters: {'hidden_dim': 102, 'num_layers': 2, 'dropout': 0.32620679757452586, 'learning_rate': 0.0006218530326018577, 'batch_size': 32}. Best is trial 10 with value: -0.5822836016817544.\n",
      "[I 2025-05-11 12:21:17,738] Trial 11 finished with value: -0.5045211350125545 and parameters: {'hidden_dim': 100, 'num_layers': 2, 'dropout': 0.33314890584232953, 'learning_rate': 0.0006326848867330172, 'batch_size': 32}. Best is trial 10 with value: -0.5822836016817544.\n",
      "[I 2025-05-11 12:21:51,317] Trial 12 finished with value: -0.4946195407747259 and parameters: {'hidden_dim': 100, 'num_layers': 2, 'dropout': 0.3281615402011395, 'learning_rate': 0.0023124563182874403, 'batch_size': 32}. Best is trial 10 with value: -0.5822836016817544.\n",
      "[I 2025-05-11 12:22:06,971] Trial 13 finished with value: -0.5746389694380091 and parameters: {'hidden_dim': 109, 'num_layers': 2, 'dropout': 0.26399586342365833, 'learning_rate': 0.0005998655769136059, 'batch_size': 64}. Best is trial 10 with value: -0.5822836016817544.\n",
      "[I 2025-05-11 12:22:20,468] Trial 14 finished with value: -0.5507820219714471 and parameters: {'hidden_dim': 112, 'num_layers': 2, 'dropout': 0.26117101897848843, 'learning_rate': 0.0006636004134889246, 'batch_size': 64}. Best is trial 10 with value: -0.5822836016817544.\n",
      "[I 2025-05-11 12:22:41,240] Trial 15 finished with value: -0.5693499777999939 and parameters: {'hidden_dim': 107, 'num_layers': 2, 'dropout': 0.22781279258440099, 'learning_rate': 0.001991941167351176, 'batch_size': 64}. Best is trial 10 with value: -0.5822836016817544.\n",
      "[I 2025-05-11 12:22:57,486] Trial 16 finished with value: -0.5953609544051803 and parameters: {'hidden_dim': 87, 'num_layers': 2, 'dropout': 0.3618853403120836, 'learning_rate': 0.0005634761029617077, 'batch_size': 64}. Best is trial 16 with value: -0.5953609544051803.\n",
      "[I 2025-05-11 12:23:20,596] Trial 17 finished with value: -0.43366008768709374 and parameters: {'hidden_dim': 85, 'num_layers': 2, 'dropout': 0.3492159530263469, 'learning_rate': 0.00010912890798545674, 'batch_size': 32}. Best is trial 16 with value: -0.5953609544051803.\n",
      "[I 2025-05-11 12:23:44,330] Trial 18 finished with value: -0.5109389662072206 and parameters: {'hidden_dim': 69, 'num_layers': 2, 'dropout': 0.37108527721303725, 'learning_rate': 0.00264024915678185, 'batch_size': 64}. Best is trial 16 with value: -0.5953609544051803.\n",
      "[I 2025-05-11 12:24:14,864] Trial 19 finished with value: -0.479242857094389 and parameters: {'hidden_dim': 85, 'num_layers': 2, 'dropout': 0.3072463703861248, 'learning_rate': 0.000901976711633171, 'batch_size': 32}. Best is trial 16 with value: -0.5953609544051803.\n",
      "[I 2025-05-11 12:24:36,800] Trial 20 finished with value: -0.5879707934308637 and parameters: {'hidden_dim': 64, 'num_layers': 1, 'dropout': 0.20823050637944018, 'learning_rate': 0.0004241920617934889, 'batch_size': 64}. Best is trial 16 with value: -0.5953609544051803.\n",
      "[I 2025-05-11 12:25:06,730] Trial 21 finished with value: -0.4825168586773209 and parameters: {'hidden_dim': 58, 'num_layers': 1, 'dropout': 0.17547937707836825, 'learning_rate': 0.0004209831893578105, 'batch_size': 64}. Best is trial 16 with value: -0.5953609544051803.\n",
      "[I 2025-05-11 12:25:54,375] Trial 22 finished with value: -0.5547046743879361 and parameters: {'hidden_dim': 68, 'num_layers': 1, 'dropout': 0.2126847041851182, 'learning_rate': 0.000443055209643558, 'batch_size': 64}. Best is trial 16 with value: -0.5953609544051803.\n",
      "[I 2025-05-11 12:26:13,084] Trial 23 finished with value: -0.4896583782976256 and parameters: {'hidden_dim': 46, 'num_layers': 1, 'dropout': 0.3014748197143919, 'learning_rate': 0.0001657350470226196, 'batch_size': 64}. Best is trial 16 with value: -0.5953609544051803.\n",
      "[I 2025-05-11 12:26:47,036] Trial 24 finished with value: -0.5007946334889369 and parameters: {'hidden_dim': 79, 'num_layers': 1, 'dropout': 0.10937519028903073, 'learning_rate': 0.0015858172011950016, 'batch_size': 32}. Best is trial 16 with value: -0.5953609544051803.\n",
      "[I 2025-05-11 12:27:09,928] Trial 25 finished with value: -0.44163866446637545 and parameters: {'hidden_dim': 62, 'num_layers': 2, 'dropout': 0.3732350252243469, 'learning_rate': 0.000820460423916659, 'batch_size': 64}. Best is trial 16 with value: -0.5953609544051803.\n",
      "[I 2025-05-11 12:27:32,568] Trial 26 finished with value: -0.42695074983521086 and parameters: {'hidden_dim': 89, 'num_layers': 1, 'dropout': 0.24544761131126, 'learning_rate': 0.0003554308983046278, 'batch_size': 64}. Best is trial 16 with value: -0.5953609544051803.\n",
      "[I 2025-05-11 12:28:00,044] Trial 27 finished with value: -0.49739421732088107 and parameters: {'hidden_dim': 75, 'num_layers': 2, 'dropout': 0.18505983428471842, 'learning_rate': 0.0005299391599381381, 'batch_size': 32}. Best is trial 16 with value: -0.5953609544051803.\n",
      "[I 2025-05-11 12:28:22,958] Trial 28 finished with value: -0.4222654139256107 and parameters: {'hidden_dim': 95, 'num_layers': 1, 'dropout': 0.13990129052693637, 'learning_rate': 0.003469040638621916, 'batch_size': 64}. Best is trial 16 with value: -0.5953609544051803.\n",
      "[I 2025-05-11 12:29:26,675] Trial 29 finished with value: -0.4865769086317618 and parameters: {'hidden_dim': 116, 'num_layers': 2, 'dropout': 0.21577844013217418, 'learning_rate': 0.0001742440727716437, 'batch_size': 16}. Best is trial 16 with value: -0.5953609544051803.\n",
      "[I 2025-05-11 12:29:26,698] A new study created in memory with name: no-name-8771cc3d-2026-4c31-aef6-75f94abc3690\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RESULT] Horizon 21: Best Params = {'hidden_dim': 87, 'num_layers': 2, 'dropout': 0.3618853403120836, 'learning_rate': 0.0005634761029617077, 'batch_size': 64}, Best F1 = 0.5954\n",
      "\n",
      "=== Forecast Horizon: 63 ===\n",
      "[INFO] Generated 7 folds for forecast horizon 63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-11 12:29:56,397] Trial 0 finished with value: -0.4308276583330954 and parameters: {'hidden_dim': 93, 'num_layers': 2, 'dropout': 0.4828864432826748, 'learning_rate': 0.0003728019759414556, 'batch_size': 16}. Best is trial 0 with value: -0.4308276583330954.\n",
      "[I 2025-05-11 12:30:25,684] Trial 1 finished with value: -0.48518948833402314 and parameters: {'hidden_dim': 58, 'num_layers': 1, 'dropout': 0.2869298427868413, 'learning_rate': 0.00203125102369817, 'batch_size': 16}. Best is trial 1 with value: -0.48518948833402314.\n",
      "[I 2025-05-11 12:30:40,204] Trial 2 finished with value: -0.4362359558762244 and parameters: {'hidden_dim': 118, 'num_layers': 2, 'dropout': 0.3337429431787884, 'learning_rate': 0.005431819316835627, 'batch_size': 64}. Best is trial 1 with value: -0.48518948833402314.\n",
      "[I 2025-05-11 12:30:49,395] Trial 3 finished with value: -0.5068014503068874 and parameters: {'hidden_dim': 78, 'num_layers': 2, 'dropout': 0.22938497689400014, 'learning_rate': 0.0009918095789440181, 'batch_size': 64}. Best is trial 3 with value: -0.5068014503068874.\n",
      "[I 2025-05-11 12:31:06,653] Trial 4 finished with value: -0.4357231198323305 and parameters: {'hidden_dim': 50, 'num_layers': 1, 'dropout': 0.3681785838067436, 'learning_rate': 0.005420928611595724, 'batch_size': 128}. Best is trial 3 with value: -0.5068014503068874.\n",
      "[I 2025-05-11 12:31:24,689] Trial 5 finished with value: -0.2884819067158206 and parameters: {'hidden_dim': 53, 'num_layers': 2, 'dropout': 0.13887243947880137, 'learning_rate': 0.002004391763770127, 'batch_size': 128}. Best is trial 3 with value: -0.5068014503068874.\n",
      "[I 2025-05-11 12:31:42,086] Trial 6 finished with value: -0.4308276583330954 and parameters: {'hidden_dim': 72, 'num_layers': 2, 'dropout': 0.18983645654047995, 'learning_rate': 0.0002758963194329824, 'batch_size': 32}. Best is trial 3 with value: -0.5068014503068874.\n",
      "[I 2025-05-11 12:32:04,026] Trial 7 finished with value: -0.4520472135332331 and parameters: {'hidden_dim': 61, 'num_layers': 1, 'dropout': 0.44252940928672735, 'learning_rate': 0.003350839349353397, 'batch_size': 32}. Best is trial 3 with value: -0.5068014503068874.\n",
      "[I 2025-05-11 12:32:25,529] Trial 8 finished with value: -0.4139047329006062 and parameters: {'hidden_dim': 50, 'num_layers': 1, 'dropout': 0.17615706483872523, 'learning_rate': 0.00019836700574734075, 'batch_size': 32}. Best is trial 3 with value: -0.5068014503068874.\n",
      "[I 2025-05-11 12:32:58,213] Trial 9 finished with value: -0.5374993688699761 and parameters: {'hidden_dim': 105, 'num_layers': 1, 'dropout': 0.1181692339768953, 'learning_rate': 0.0002958967067012953, 'batch_size': 16}. Best is trial 9 with value: -0.5374993688699761.\n",
      "[I 2025-05-11 12:33:28,924] Trial 10 finished with value: -0.4984668994739138 and parameters: {'hidden_dim': 128, 'num_layers': 1, 'dropout': 0.2695824538021353, 'learning_rate': 0.00010348918057848374, 'batch_size': 16}. Best is trial 9 with value: -0.5374993688699761.\n",
      "[I 2025-05-11 12:33:49,386] Trial 11 finished with value: -0.4365175587598379 and parameters: {'hidden_dim': 96, 'num_layers': 2, 'dropout': 0.10321667479217994, 'learning_rate': 0.0007310765515383672, 'batch_size': 64}. Best is trial 9 with value: -0.5374993688699761.\n",
      "[I 2025-05-11 12:34:11,917] Trial 12 finished with value: -0.45217883175800155 and parameters: {'hidden_dim': 99, 'num_layers': 2, 'dropout': 0.22902362536825777, 'learning_rate': 0.0007090105997977354, 'batch_size': 64}. Best is trial 9 with value: -0.5374993688699761.\n",
      "[I 2025-05-11 12:34:32,331] Trial 13 finished with value: -0.4824587971622658 and parameters: {'hidden_dim': 34, 'num_layers': 1, 'dropout': 0.10003091237021393, 'learning_rate': 0.0010651392609208755, 'batch_size': 64}. Best is trial 9 with value: -0.5374993688699761.\n",
      "[I 2025-05-11 12:35:09,728] Trial 14 finished with value: -0.5008720874199771 and parameters: {'hidden_dim': 83, 'num_layers': 1, 'dropout': 0.2100872778033978, 'learning_rate': 0.0004315341940596828, 'batch_size': 16}. Best is trial 9 with value: -0.5374993688699761.\n",
      "[I 2025-05-11 12:35:32,950] Trial 15 finished with value: -0.29273753107329303 and parameters: {'hidden_dim': 109, 'num_layers': 2, 'dropout': 0.2526347156002843, 'learning_rate': 0.0001262067967050605, 'batch_size': 64}. Best is trial 9 with value: -0.5374993688699761.\n",
      "[I 2025-05-11 12:36:05,471] Trial 16 finished with value: -0.2932776407830779 and parameters: {'hidden_dim': 77, 'num_layers': 2, 'dropout': 0.15932828705083185, 'learning_rate': 0.001128085041810005, 'batch_size': 16}. Best is trial 9 with value: -0.5374993688699761.\n",
      "[I 2025-05-11 12:36:21,977] Trial 17 finished with value: -0.5285717122705028 and parameters: {'hidden_dim': 86, 'num_layers': 1, 'dropout': 0.33488292205194414, 'learning_rate': 0.0005364762700664355, 'batch_size': 128}. Best is trial 9 with value: -0.5374993688699761.\n",
      "[I 2025-05-11 12:36:41,023] Trial 18 finished with value: -0.5419828679402623 and parameters: {'hidden_dim': 109, 'num_layers': 1, 'dropout': 0.39267957978771995, 'learning_rate': 0.00018813096222396772, 'batch_size': 128}. Best is trial 18 with value: -0.5419828679402623.\n",
      "[I 2025-05-11 12:36:59,226] Trial 19 finished with value: -0.46042247691130017 and parameters: {'hidden_dim': 109, 'num_layers': 1, 'dropout': 0.4095730238753061, 'learning_rate': 0.00019083491898971155, 'batch_size': 128}. Best is trial 18 with value: -0.5419828679402623.\n",
      "[I 2025-05-11 12:37:16,841] Trial 20 finished with value: -0.386893498609462 and parameters: {'hidden_dim': 111, 'num_layers': 1, 'dropout': 0.41823351597770486, 'learning_rate': 0.00018738120435487997, 'batch_size': 128}. Best is trial 18 with value: -0.5419828679402623.\n",
      "[I 2025-05-11 12:37:33,579] Trial 21 finished with value: -0.29757295228020303 and parameters: {'hidden_dim': 90, 'num_layers': 1, 'dropout': 0.33229898184901413, 'learning_rate': 0.0004634268028078161, 'batch_size': 128}. Best is trial 18 with value: -0.5419828679402623.\n",
      "[I 2025-05-11 12:37:52,544] Trial 22 finished with value: -0.4316253303884957 and parameters: {'hidden_dim': 101, 'num_layers': 1, 'dropout': 0.35081180672739437, 'learning_rate': 0.0002870513882603799, 'batch_size': 128}. Best is trial 18 with value: -0.5419828679402623.\n",
      "[I 2025-05-11 12:38:11,653] Trial 23 finished with value: -0.4976956045409304 and parameters: {'hidden_dim': 123, 'num_layers': 1, 'dropout': 0.38042668878792285, 'learning_rate': 0.0005322122050676065, 'batch_size': 128}. Best is trial 18 with value: -0.5419828679402623.\n",
      "[I 2025-05-11 12:38:28,143] Trial 24 finished with value: -0.33756504806689547 and parameters: {'hidden_dim': 86, 'num_layers': 1, 'dropout': 0.3067325370070464, 'learning_rate': 0.00031836180466085604, 'batch_size': 128}. Best is trial 18 with value: -0.5419828679402623.\n",
      "[I 2025-05-11 12:38:57,375] Trial 25 finished with value: -0.39231251614619583 and parameters: {'hidden_dim': 106, 'num_layers': 1, 'dropout': 0.4790133288150716, 'learning_rate': 0.00014327758239119013, 'batch_size': 16}. Best is trial 18 with value: -0.5419828679402623.\n",
      "[I 2025-05-11 12:39:12,636] Trial 26 finished with value: -0.43398545530532534 and parameters: {'hidden_dim': 117, 'num_layers': 1, 'dropout': 0.3908899857081769, 'learning_rate': 0.00022888988228659304, 'batch_size': 128}. Best is trial 18 with value: -0.5419828679402623.\n",
      "[I 2025-05-11 12:39:23,985] Trial 27 finished with value: -0.5229193169772053 and parameters: {'hidden_dim': 71, 'num_layers': 1, 'dropout': 0.44115447819484893, 'learning_rate': 0.0006046538591212562, 'batch_size': 128}. Best is trial 18 with value: -0.5419828679402623.\n",
      "[I 2025-05-11 12:39:56,521] Trial 28 finished with value: -0.5125535806042233 and parameters: {'hidden_dim': 102, 'num_layers': 1, 'dropout': 0.3030557425157172, 'learning_rate': 0.009935683784109937, 'batch_size': 16}. Best is trial 18 with value: -0.5419828679402623.\n",
      "[I 2025-05-11 12:40:29,067] Trial 29 finished with value: -0.44367519083128293 and parameters: {'hidden_dim': 91, 'num_layers': 1, 'dropout': 0.32974691018467484, 'learning_rate': 0.0003912551121165099, 'batch_size': 16}. Best is trial 18 with value: -0.5419828679402623.\n",
      "[I 2025-05-11 12:40:29,082] A new study created in memory with name: no-name-de55c2b7-4295-4244-9a2a-4dbb62f4c784\n",
      "/venv/main/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/venv/main/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/venv/main/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/venv/main/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/venv/main/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/venv/main/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RESULT] Horizon 63: Best Params = {'hidden_dim': 109, 'num_layers': 1, 'dropout': 0.39267957978771995, 'learning_rate': 0.00018813096222396772, 'batch_size': 128}, Best F1 = 0.5420\n",
      "\n",
      "=== Forecast Horizon: 252 ===\n",
      "[INFO] Generated 5 folds for forecast horizon 252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/main/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/venv/main/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/venv/main/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/venv/main/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "[I 2025-05-11 12:40:41,091] Trial 0 finished with value: -0.4166321445615794 and parameters: {'hidden_dim': 125, 'num_layers': 2, 'dropout': 0.3018016204054782, 'learning_rate': 0.00063778304976431, 'batch_size': 128}. Best is trial 0 with value: -0.4166321445615794.\n",
      "/venv/main/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/venv/main/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/venv/main/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/venv/main/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/venv/main/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/venv/main/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/venv/main/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/venv/main/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/venv/main/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/venv/main/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "[I 2025-05-11 12:40:51,384] Trial 1 finished with value: -0.4024440957043466 and parameters: {'hidden_dim': 111, 'num_layers': 2, 'dropout': 0.2996203544832724, 'learning_rate': 0.00883393748896689, 'batch_size': 128}. Best is trial 0 with value: -0.4166321445615794.\n",
      "[I 2025-05-11 12:41:02,531] Trial 2 finished with value: -0.4049759458403662 and parameters: {'hidden_dim': 50, 'num_layers': 1, 'dropout': 0.21816290625902465, 'learning_rate': 0.007001705627540978, 'batch_size': 64}. Best is trial 0 with value: -0.4166321445615794.\n",
      "/venv/main/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/venv/main/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/venv/main/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "[I 2025-05-11 12:41:18,788] Trial 3 finished with value: -0.39533832553160064 and parameters: {'hidden_dim': 35, 'num_layers': 2, 'dropout': 0.16071658544665404, 'learning_rate': 0.005069280711385831, 'batch_size': 32}. Best is trial 0 with value: -0.4166321445615794.\n",
      "[I 2025-05-11 12:41:30,529] Trial 4 finished with value: -0.39066506165784853 and parameters: {'hidden_dim': 68, 'num_layers': 2, 'dropout': 0.444903125347045, 'learning_rate': 0.0036300634652135333, 'batch_size': 128}. Best is trial 0 with value: -0.4166321445615794.\n",
      "[I 2025-05-11 12:41:40,523] Trial 5 finished with value: -0.4018023501564564 and parameters: {'hidden_dim': 109, 'num_layers': 1, 'dropout': 0.2100748219031644, 'learning_rate': 0.00017534259541163467, 'batch_size': 128}. Best is trial 0 with value: -0.4166321445615794.\n",
      "/venv/main/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/venv/main/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/venv/main/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/venv/main/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/venv/main/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/venv/main/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/venv/main/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/venv/main/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/venv/main/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "[I 2025-05-11 12:41:57,735] Trial 6 finished with value: -0.3891302942246233 and parameters: {'hidden_dim': 67, 'num_layers': 2, 'dropout': 0.4354372904282061, 'learning_rate': 0.0032866241686266505, 'batch_size': 32}. Best is trial 0 with value: -0.4166321445615794.\n",
      "[I 2025-05-11 12:42:09,751] Trial 7 finished with value: -0.40218457662563933 and parameters: {'hidden_dim': 71, 'num_layers': 1, 'dropout': 0.28463134370594145, 'learning_rate': 0.003316535425608853, 'batch_size': 64}. Best is trial 0 with value: -0.4166321445615794.\n",
      "[I 2025-05-11 12:42:21,591] Trial 8 finished with value: -0.4019315564875665 and parameters: {'hidden_dim': 62, 'num_layers': 2, 'dropout': 0.2829261481008428, 'learning_rate': 0.00013534561118301377, 'batch_size': 64}. Best is trial 0 with value: -0.4166321445615794.\n",
      "/venv/main/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "[I 2025-05-11 12:42:35,908] Trial 9 finished with value: -0.40759945160573174 and parameters: {'hidden_dim': 63, 'num_layers': 2, 'dropout': 0.48974736881131975, 'learning_rate': 0.0010823848792825138, 'batch_size': 32}. Best is trial 0 with value: -0.4166321445615794.\n",
      "[I 2025-05-11 12:42:56,716] Trial 10 finished with value: -0.40086589530695804 and parameters: {'hidden_dim': 128, 'num_layers': 1, 'dropout': 0.10087627080649464, 'learning_rate': 0.00033618586702215456, 'batch_size': 16}. Best is trial 0 with value: -0.4166321445615794.\n",
      "[I 2025-05-11 12:43:12,868] Trial 11 finished with value: -0.4024440957043466 and parameters: {'hidden_dim': 90, 'num_layers': 2, 'dropout': 0.38552337805642334, 'learning_rate': 0.0007260473706729399, 'batch_size': 32}. Best is trial 0 with value: -0.4166321445615794.\n",
      "[I 2025-05-11 12:43:37,848] Trial 12 finished with value: -0.4018023501564564 and parameters: {'hidden_dim': 89, 'num_layers': 2, 'dropout': 0.4966988353908473, 'learning_rate': 0.0011558502366089617, 'batch_size': 16}. Best is trial 0 with value: -0.4166321445615794.\n",
      "/venv/main/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/venv/main/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/venv/main/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/venv/main/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/venv/main/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/venv/main/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/venv/main/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "[I 2025-05-11 12:43:52,702] Trial 13 finished with value: -0.4038618467239621 and parameters: {'hidden_dim': 48, 'num_layers': 2, 'dropout': 0.3519527974483177, 'learning_rate': 0.0011541646391849244, 'batch_size': 32}. Best is trial 0 with value: -0.4166321445615794.\n",
      "[I 2025-05-11 12:44:05,773] Trial 14 finished with value: -0.4018023501564564 and parameters: {'hidden_dim': 128, 'num_layers': 2, 'dropout': 0.3624284604299796, 'learning_rate': 0.0005841228217950014, 'batch_size': 128}. Best is trial 0 with value: -0.4166321445615794.\n",
      "/venv/main/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/venv/main/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/venv/main/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/venv/main/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/venv/main/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/venv/main/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/venv/main/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/venv/main/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/venv/main/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/venv/main/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "[I 2025-05-11 12:44:20,366] Trial 15 finished with value: -0.4018023501564564 and parameters: {'hidden_dim': 86, 'num_layers': 2, 'dropout': 0.499262434709011, 'learning_rate': 0.0017807973563245569, 'batch_size': 32}. Best is trial 0 with value: -0.4166321445615794.\n",
      "[I 2025-05-11 12:44:33,442] Trial 16 finished with value: -0.41196356168552334 and parameters: {'hidden_dim': 104, 'num_layers': 2, 'dropout': 0.42827090608047713, 'learning_rate': 0.0003828547113237829, 'batch_size': 128}. Best is trial 0 with value: -0.4166321445615794.\n",
      "[I 2025-05-11 12:44:43,728] Trial 17 finished with value: -0.4018023501564564 and parameters: {'hidden_dim': 110, 'num_layers': 1, 'dropout': 0.40095311904335623, 'learning_rate': 0.0003193625301813413, 'batch_size': 128}. Best is trial 0 with value: -0.4166321445615794.\n",
      "[I 2025-05-11 12:44:55,385] Trial 18 finished with value: -0.4018023501564564 and parameters: {'hidden_dim': 101, 'num_layers': 2, 'dropout': 0.3296687129915193, 'learning_rate': 0.00036201899051641425, 'batch_size': 128}. Best is trial 0 with value: -0.4166321445615794.\n",
      "[I 2025-05-11 12:45:08,028] Trial 19 finished with value: -0.401220484955228 and parameters: {'hidden_dim': 118, 'num_layers': 2, 'dropout': 0.2419760377867559, 'learning_rate': 0.00022406776934549732, 'batch_size': 128}. Best is trial 0 with value: -0.4166321445615794.\n",
      "/venv/main/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/venv/main/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/venv/main/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/venv/main/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/venv/main/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/venv/main/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/venv/main/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/venv/main/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/venv/main/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/venv/main/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "[I 2025-05-11 12:45:18,198] Trial 20 finished with value: -0.4018023501564564 and parameters: {'hidden_dim': 100, 'num_layers': 1, 'dropout': 0.4222096774674535, 'learning_rate': 0.0005560293389577192, 'batch_size': 128}. Best is trial 0 with value: -0.4166321445615794.\n",
      "/venv/main/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/venv/main/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/venv/main/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "[I 2025-05-11 12:45:47,974] Trial 21 finished with value: -0.4018023501564564 and parameters: {'hidden_dim': 121, 'num_layers': 2, 'dropout': 0.4663189027204419, 'learning_rate': 0.0018856162772755971, 'batch_size': 16}. Best is trial 0 with value: -0.4166321445615794.\n",
      "/venv/main/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/venv/main/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/venv/main/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "[I 2025-05-11 12:46:02,254] Trial 22 finished with value: -0.4019288323698952 and parameters: {'hidden_dim': 79, 'num_layers': 2, 'dropout': 0.45894840775061535, 'learning_rate': 0.0009177542289564365, 'batch_size': 32}. Best is trial 0 with value: -0.4166321445615794.\n",
      "[I 2025-05-11 12:46:14,960] Trial 23 finished with value: -0.4019288323698952 and parameters: {'hidden_dim': 100, 'num_layers': 2, 'dropout': 0.39191882813151063, 'learning_rate': 0.0016879461873652545, 'batch_size': 128}. Best is trial 0 with value: -0.4166321445615794.\n",
      "/venv/main/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/venv/main/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/venv/main/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/venv/main/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/venv/main/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/venv/main/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/venv/main/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/venv/main/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "[I 2025-05-11 12:46:25,268] Trial 24 finished with value: -0.4018023501564564 and parameters: {'hidden_dim': 50, 'num_layers': 2, 'dropout': 0.326433063430026, 'learning_rate': 0.00010141026928355125, 'batch_size': 128}. Best is trial 0 with value: -0.4166321445615794.\n",
      "/venv/main/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/venv/main/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/venv/main/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/venv/main/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "[I 2025-05-11 12:46:41,078] Trial 25 finished with value: -0.4018023501564564 and parameters: {'hidden_dim': 77, 'num_layers': 2, 'dropout': 0.48689517869269683, 'learning_rate': 0.0004963699789137771, 'batch_size': 32}. Best is trial 0 with value: -0.4166321445615794.\n",
      "[I 2025-05-11 12:47:01,198] Trial 26 finished with value: -0.4025205371454848 and parameters: {'hidden_dim': 57, 'num_layers': 2, 'dropout': 0.41578743009909647, 'learning_rate': 0.0008356827526531059, 'batch_size': 16}. Best is trial 0 with value: -0.4166321445615794.\n",
      "/venv/main/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/venv/main/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/venv/main/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/venv/main/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/venv/main/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/venv/main/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/venv/main/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/venv/main/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/venv/main/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/venv/main/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "[I 2025-05-11 12:47:15,513] Trial 27 finished with value: -0.4018023501564564 and parameters: {'hidden_dim': 117, 'num_layers': 2, 'dropout': 0.24733147986776818, 'learning_rate': 0.0002447717637452808, 'batch_size': 64}. Best is trial 0 with value: -0.4166321445615794.\n",
      "[I 2025-05-11 12:47:26,244] Trial 28 finished with value: -0.4018023501564564 and parameters: {'hidden_dim': 39, 'num_layers': 2, 'dropout': 0.4671439618402267, 'learning_rate': 0.0005185924543914203, 'batch_size': 128}. Best is trial 0 with value: -0.4166321445615794.\n",
      "/venv/main/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/venv/main/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/venv/main/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/venv/main/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/venv/main/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/venv/main/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/venv/main/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/venv/main/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "[I 2025-05-11 12:47:38,461] Trial 29 finished with value: -0.4018023501564564 and parameters: {'hidden_dim': 107, 'num_layers': 2, 'dropout': 0.3652672602635987, 'learning_rate': 0.0012610108504472332, 'batch_size': 128}. Best is trial 0 with value: -0.4166321445615794.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RESULT] Horizon 252: Best Params = {'hidden_dim': 125, 'num_layers': 2, 'dropout': 0.3018016204054782, 'learning_rate': 0.00063778304976431, 'batch_size': 128}, Best F1 = 0.4166\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "X_df = pd.read_csv(r\"X_df.csv\", index_col=0, parse_dates=True)\n",
    "Y_df_dict = {\n",
    "    1: pd.read_csv(r\"Y_df_change_dir_1.csv\", index_col=0, parse_dates=True),\n",
    "    5: pd.read_csv(r\"Y_df_change_dir_5.csv\", index_col=0, parse_dates=True),\n",
    "    21: pd.read_csv(r\"Y_df_change_dir_21.csv\", index_col=0, parse_dates=True),\n",
    "    63: pd.read_csv(r\"Y_df_change_dir_63.csv\", index_col=0, parse_dates=True),\n",
    "    252: pd.read_csv(r\"Y_df_change_dir_252.csv\", index_col=0, parse_dates=True),\n",
    "}\n",
    "\n",
    "# Run optimization\n",
    "results = run_for_all_horizons(X_df, Y_df_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "046d73f4-73c3-43ff-890a-ad2ddc7c042e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: {'best_params': {'hidden_dim': 84,\n",
       "   'num_layers': 2,\n",
       "   'dropout': 0.45710576033956674,\n",
       "   'learning_rate': 0.004688535399076738,\n",
       "   'batch_size': 16},\n",
       "  'best_f1': 0.5548958508540118},\n",
       " 5: {'best_params': {'hidden_dim': 92,\n",
       "   'num_layers': 1,\n",
       "   'dropout': 0.21557525139220088,\n",
       "   'learning_rate': 0.0004883533160474792,\n",
       "   'batch_size': 16},\n",
       "  'best_f1': 0.5482964955974565},\n",
       " 21: {'best_params': {'hidden_dim': 87,\n",
       "   'num_layers': 2,\n",
       "   'dropout': 0.3618853403120836,\n",
       "   'learning_rate': 0.0005634761029617077,\n",
       "   'batch_size': 64},\n",
       "  'best_f1': 0.5953609544051803},\n",
       " 63: {'best_params': {'hidden_dim': 109,\n",
       "   'num_layers': 1,\n",
       "   'dropout': 0.39267957978771995,\n",
       "   'learning_rate': 0.00018813096222396772,\n",
       "   'batch_size': 128},\n",
       "  'best_f1': 0.5419828679402623},\n",
       " 252: {'best_params': {'hidden_dim': 125,\n",
       "   'num_layers': 2,\n",
       "   'dropout': 0.3018016204054782,\n",
       "   'learning_rate': 0.00063778304976431,\n",
       "   'batch_size': 128},\n",
       "  'best_f1': 0.4166321445615794}}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6c132f59-dbd6-41cc-813c-2ff20c72567c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "\n",
    "def evaluate_on_holdout(X_df, Y_df_dict, best_params_dict):\n",
    "    holdout_results = {}\n",
    "    for h in forecast_horizons:\n",
    "        print(f\"\\n[TEST] Forecast Horizon: {h}\")\n",
    "        params = best_params_dict[h]['best_params']\n",
    "        hidden_dim = int(params['hidden_dim'])\n",
    "        num_layers = int(params['num_layers'])\n",
    "        dropout = float(params['dropout'])\n",
    "        learning_rate = float(params['learning_rate'])\n",
    "        batch_size = int(params['batch_size'])\n",
    "\n",
    "        Y_df = Y_df_dict[h]\n",
    "        X_shifted = shift_X_by_horizon(X_df, h)\n",
    "        Y_aligned = Y_df.loc[X_shifted.index]\n",
    "        X_final, Y_final = X_shifted, Y_aligned\n",
    "\n",
    "        folds, last_val_end, _ = get_expanding_folds(X_final, Y_final, h, sequence_length, val_window_num_sequences, holdout_base)\n",
    "        X_train, Y_train = X_final.iloc[:last_val_end], Y_final.iloc[:last_val_end]\n",
    "        X_test, Y_test = X_final.iloc[last_val_end:], Y_final.iloc[last_val_end:]\n",
    "\n",
    "        print(f\"[INFO] Holdout Label Distribution (0s/1s): {np.bincount(Y_test.values.astype(int).flatten())}\")\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), index=X_train.index, columns=X_train.columns)\n",
    "        X_test_scaled = pd.DataFrame(scaler.transform(X_test), index=X_test.index, columns=X_test.columns)\n",
    "\n",
    "        X_train_seq, Y_train_seq = create_sequences(X_train_scaled, Y_train, sequence_length, h)\n",
    "        X_test_seq, Y_test_seq = create_sequences(X_test_scaled, Y_test, sequence_length, h)\n",
    "\n",
    "        X_train_tensor = torch.tensor(X_train_seq, dtype=torch.float32).to(device)\n",
    "        Y_train_tensor = torch.tensor(Y_train_seq, dtype=torch.float32).to(device)\n",
    "        X_test_tensor = torch.tensor(X_test_seq, dtype=torch.float32).to(device)\n",
    "        Y_test_tensor = torch.tensor(Y_test_seq, dtype=torch.float32).to(device)\n",
    "\n",
    "        model = LSTMClassifier(X_train_seq.shape[2], hidden_dim, num_layers, Y_train_seq.shape[1], dropout).to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "        train_loader = DataLoader(TensorDataset(X_train_tensor, Y_train_tensor), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            for xb, yb in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                loss = criterion(model(xb), yb)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        preds = []\n",
    "        with torch.no_grad():\n",
    "            for xb in DataLoader(X_test_tensor, batch_size=batch_size):\n",
    "                preds.append(torch.sigmoid(model(xb)))\n",
    "\n",
    "        pred_tensor = torch.cat(preds, dim=0).squeeze()\n",
    "        pred_bin = (pred_tensor > 0.5).int().cpu().numpy()\n",
    "        y_true = Y_test_tensor.int().cpu().numpy()\n",
    "\n",
    "        acc = accuracy_score(y_true, pred_bin)\n",
    "        f1 = f1_score(y_true, pred_bin, average='macro')\n",
    "        precision = precision_score(y_true, pred_bin, average='macro')\n",
    "        recall = recall_score(y_true, pred_bin, average='macro')\n",
    "\n",
    "        holdout_results[h] = {\n",
    "            \"accuracy\": acc,\n",
    "            \"f1\": f1,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall\n",
    "        }\n",
    "        print(f\"[RESULT] Horizon {h}: Accuracy = {acc:.4f}, F1 = {f1:.4f}, Precision = {precision:.4f}, Recall = {recall:.4f}\")\n",
    "\n",
    "    return holdout_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d8234fd1-5055-4006-9b11-b36094efb792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[TEST] Forecast Horizon: 1\n",
      "[INFO] Generated 8 folds for forecast horizon 1\n",
      "[INFO] Holdout Label Distribution (0s/1s): [3620 2728]\n",
      "[RESULT] Horizon 1: Accuracy = 0.6365, F1 = 0.4328, Precision = 0.4679, Recall = 0.4908\n",
      "\n",
      "[TEST] Forecast Horizon: 5\n",
      "[INFO] Generated 8 folds for forecast horizon 5\n",
      "[INFO] Holdout Label Distribution (0s/1s): [2856 3252]\n",
      "[RESULT] Horizon 5: Accuracy = 0.6063, F1 = 0.6063, Precision = 0.6099, Recall = 0.6098\n",
      "\n",
      "[TEST] Forecast Horizon: 21\n",
      "[INFO] Generated 8 folds for forecast horizon 21\n",
      "[INFO] Holdout Label Distribution (0s/1s): [2072 3076]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/main/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RESULT] Horizon 21: Accuracy = 0.9010, F1 = 0.4740, Precision = 0.4505, Recall = 0.5000\n",
      "\n",
      "[TEST] Forecast Horizon: 63\n",
      "[INFO] Generated 7 folds for forecast horizon 63\n",
      "[INFO] Holdout Label Distribution (0s/1s): [1979 4045]\n",
      "[RESULT] Horizon 63: Accuracy = 0.3101, F1 = 0.2759, Precision = 0.5606, Recall = 0.5159\n",
      "\n",
      "[TEST] Forecast Horizon: 252\n",
      "[INFO] Generated 5 folds for forecast horizon 252\n",
      "[INFO] Holdout Label Distribution (0s/1s): [ 993 3885]\n",
      "[RESULT] Horizon 252: Accuracy = 0.0839, F1 = 0.0774, Precision = 0.5000, Recall = 0.0419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/main/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "holdout_results = evaluate_on_holdout(X_df, Y_df_dict, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c5ed15-c58b-4d29-b480-b96ef4ab85fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
