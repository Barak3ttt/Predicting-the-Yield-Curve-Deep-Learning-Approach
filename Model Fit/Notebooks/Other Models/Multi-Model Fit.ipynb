{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18963b4e-faa5-42c4-b59e-c234ab36b60d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /venv/main/lib/python3.12/site-packages (2.1.2)\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.2.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /venv/main/lib/python3.12/site-packages (4.67.1)\n",
      "Collecting statsmodels\n",
      "  Downloading statsmodels-0.14.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.2 kB)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.6.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /venv/main/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting scipy!=1.9.2,>=1.8 (from statsmodels)\n",
      "  Downloading scipy-1.15.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hCollecting patsy>=0.5.6 (from statsmodels)\n",
      "  Downloading patsy-1.0.1-py2.py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: packaging>=21.3 in /venv/main/lib/python3.12/site-packages (from statsmodels) (25.0)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.5.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: six>=1.5 in /venv/main/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading pandas-2.2.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.7 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.7/12.7 MB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading statsmodels-0.14.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.7 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.7/10.7 MB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading scikit_learn-1.6.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.5.0-py3-none-any.whl (307 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading patsy-1.0.1-py2.py3-none-any.whl (232 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m232.9/232.9 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m509.2/509.2 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.15.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m37.3/37.3 MB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m347.8/347.8 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pytz, tzdata, threadpoolctl, scipy, patsy, joblib, scikit-learn, pandas, statsmodels\n",
      "Successfully installed joblib-1.5.0 pandas-2.2.3 patsy-1.0.1 pytz-2025.2 scikit-learn-1.6.1 scipy-1.15.3 statsmodels-0.14.4 threadpoolctl-3.6.0 tzdata-2025.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install numpy pandas tqdm statsmodels scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "234922dc-6ec8-41a2-8384-6a05c407b3ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching 20 tasks across 80 processes â€¦\n",
      "âœ… Merged 4 chunks â†’ /model_outputs/rw_h1_fixed_window_results.csv (756 rows)\n",
      "âœ… Merged 4 chunks â†’ /model_outputs/rw_h5_fixed_window_results.csv (756 rows)\n",
      "âœ… Merged 4 chunks â†’ /model_outputs/rw_h21_fixed_window_results.csv (756 rows)\n",
      "âœ… Merged 4 chunks â†’ /model_outputs/rw_h63_fixed_window_results.csv (756 rows)\n",
      "âœ… Merged 4 chunks â†’ /model_outputs/rw_h252_fixed_window_results.csv (756 rows)\n",
      "ğŸ All modelâ€“horizon result files have been merged and saved.\n"
     ]
    }
   ],
   "source": [
    "# rolling_yield_forecasts_multi_horizon.py\n",
    "\"\"\"\n",
    "Rolling multi-horizon yieldâ€‘curve forecasting pipeline\n",
    "with highâ€‘parallel throughput on large machines.\n",
    "\n",
    "Key features\n",
    "------------\n",
    "â€¢ Forecast models implemented\n",
    "    â€“ Random Walk (RW)\n",
    "    â€“ AR(1) perâ€‘maturity via Yuleâ€“Walker (AR_yw)\n",
    "    â€“ Fullâ€‘curve VAR(1) via Yuleâ€“Walker (VAR_yw)\n",
    "    â€“ 2â€‘step DNS + VAR(1) via Yuleâ€“Walker (DNS_VAR_yw)\n",
    "\n",
    "â€¢ Forecast horizons: [1,â€¯5,â€¯21,â€¯63,â€¯252] trading days\n",
    "â€¢ Rolling 3â€‘year (756â€‘day) estimation window\n",
    "â€¢ **Chunkâ€‘based multiprocessing**\n",
    "    â€“ 4 chunks per (model, horizon) â†’ 20â€¯Ã—â€¯4Â =Â 80 tasks\n",
    "    â€“ Up to 80 worker processes (leave headroom)\n",
    "    â€“ `maxtasksperchild=1` prevents memory bloat\n",
    "â€¢ Each chunk writes a temporary CSV â†’ autoâ€‘merged\n",
    "â€¢ Final merged files: ``<model>_h<horizon>_fixed_window_results.csv``\n",
    "\"\"\"\n",
    "\n",
    "# ---------------------- Imports ---------------------- #\n",
    "from __future__ import annotations\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import uuid\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from statsmodels.tsa.api import VAR\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# ---------------------- DNS / Utility ---------------------- #\n",
    "\n",
    "def DNS_formula(x: np.ndarray, f: np.ndarray, lambb: float) -> np.ndarray:\n",
    "    l1, s1, c1 = f\n",
    "    term1 = (1 - np.exp(-lambb * x)) / (lambb * x)\n",
    "    term2 = term1 - np.exp(-lambb * x)\n",
    "    return l1 + s1 * term1 + c1 * term2\n",
    "\n",
    "\n",
    "def DNS_OLS(data: np.ndarray, tau_in: np.ndarray, lamb_i: float) -> np.ndarray:\n",
    "    tau = tau_in.reshape(-1, 1)\n",
    "    dummy = lamb_i * tau\n",
    "    col2 = (1 - np.exp(-dummy)) / dummy\n",
    "    col3 = col2 - np.exp(-dummy)\n",
    "    X = np.hstack([np.ones_like(tau), col2, col3])\n",
    "    XtX_inv_Xt = np.linalg.pinv(X.T @ X) @ X.T\n",
    "    return (XtX_inv_Xt @ data.T).T  # (T Ã— 3)\n",
    "\n",
    "# ---------------------- Forecast Functions ---------------------- #\n",
    "\n",
    "def forecast_RW_fct(da: np.ndarray, pred: int = 1) -> np.ndarray:\n",
    "    return np.tile(da[-1], (pred, 1))\n",
    "\n",
    "\n",
    "def forecast_AR_yw(da: np.ndarray, pred: int) -> np.ndarray:\n",
    "    T, n = da.shape\n",
    "    mu = da.mean(axis=0)\n",
    "    cov0 = (da - mu).T @ (da - mu) / T\n",
    "    cov1 = (da[1:] - mu).T @ (da[:-1] - mu) / T\n",
    "    A = np.diag(np.diag(cov1 @ np.linalg.inv(np.diag(np.diag(cov0)))))\n",
    "    fc = np.zeros((pred, n))\n",
    "    fc[0] = mu + A @ (da[-1] - mu)\n",
    "    for t in range(1, pred):\n",
    "        fc[t] = mu + A @ (fc[t-1] - mu)\n",
    "    return fc\n",
    "\n",
    "\n",
    "def forecast_VAR_yw(da: np.ndarray, pred: int) -> np.ndarray:\n",
    "    T, n = da.shape\n",
    "    mu = da.mean(axis=0)\n",
    "    cov0 = (da - mu).T @ (da - mu) / T\n",
    "    cov1 = (da[1:] - mu).T @ (da[:-1] - mu) / T\n",
    "    A = cov1 @ np.linalg.inv(cov0)\n",
    "    fc = np.zeros((pred, n))\n",
    "    fc[0] = mu + A @ (da[-1] - mu)\n",
    "    for t in range(1, pred):\n",
    "        fc[t] = mu + A @ (fc[t-1] - mu)\n",
    "    return fc\n",
    "\n",
    "\n",
    "def forecast_DNS_VAR_yw(da: np.ndarray, tau: np.ndarray, lamb_i: float, pred: int) -> np.ndarray:\n",
    "    betas = DNS_OLS(da, tau, lamb_i)\n",
    "    betas_fcst = forecast_VAR_yw(betas, pred)\n",
    "    return betas_fcst\n",
    "\n",
    "# ---------------------- Config ---------------------- #\n",
    "\n",
    "FORECAST_HORIZONS = [1, 5, 21, 63, 252]\n",
    "WINDOW_SIZE = 3 * 252  # trading days\n",
    "TAU = np.array([0.25, 0.5, 1, 3, 5, 10])\n",
    "LAMBDA = 0.496\n",
    "\n",
    "DATA_PATH = Path(\"Y_df.csv\")\n",
    "OUT_DIR = Path(\"./model_outputs\").resolve()\n",
    "TMP_DIR = OUT_DIR / \"chunks\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "TMP_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "START_DATE = pd.to_datetime(\"2022-04-13\")\n",
    "END_DATE = pd.to_datetime(\"2025-03-05\")\n",
    "\n",
    "CHUNKS_PER_COMBO = 4  # 20 combos Ã— 4 = 80 tasks\n",
    "MAX_PROCS = min(80, cpu_count() - 4)  # leave headâ€‘room\n",
    "\n",
    "# ---------------------- Utilities ---------------------- #\n",
    "\n",
    "def load_data() -> pd.DataFrame:\n",
    "    return pd.read_csv(DATA_PATH, index_col=0, parse_dates=True).sort_index()\n",
    "\n",
    "\n",
    "def rolling_indices(date_index: pd.DatetimeIndex) -> List[int]:\n",
    "    return list(range(date_index.get_loc(START_DATE), date_index.get_loc(END_DATE) + 1))\n",
    "\n",
    "\n",
    "def chunk_list(lst: List[int], n_chunks: int) -> List[List[int]]:\n",
    "    k, m = divmod(len(lst), n_chunks)\n",
    "    return [lst[i * k + min(i, m):(i + 1) * k + min(i + 1, m)] for i in range(n_chunks)]\n",
    "\n",
    "# ---------------------- Worker ---------------------- #\n",
    "\n",
    "def forecast_worker(args: Tuple[str, int, List[int], np.ndarray, pd.DatetimeIndex]):\n",
    "    model_name, h, rows_chunk, y_all, date_index = args\n",
    "    model_func = MODELS[model_name]\n",
    "    results: List[Dict] = []\n",
    "\n",
    "    for i in rows_chunk:\n",
    "        if i - WINDOW_SIZE < 0:\n",
    "            continue\n",
    "        y_hist = y_all[i - WINDOW_SIZE:i]\n",
    "        y_true = y_all[i]\n",
    "\n",
    "        if model_name == \"DNS_VAR_yw\":\n",
    "            betas_fcst = forecast_DNS_VAR_yw(y_hist, TAU, LAMBDA, h)\n",
    "            y_pred = DNS_formula(TAU, betas_fcst[-1], LAMBDA)\n",
    "        else:\n",
    "          if model_name == \"RW\":\n",
    "              y_pred = model_func(y_hist, 1)[-1]  # always use last observation\n",
    "          else:\n",
    "              y_pred = model_func(y_hist, h)[-1]\n",
    "\n",
    "        results.append({\n",
    "            \"eval_date\": date_index[i].strftime(\"%Y-%m-%d\"),\n",
    "            \"horizon\": h,\n",
    "            \"true_yields\": y_true.tolist(),\n",
    "            \"forecast_yields\": y_pred.tolist(),\n",
    "            \"mae\": mean_absolute_error(y_true, y_pred)\n",
    "        })\n",
    "\n",
    "    # Write chunk to temp file\n",
    "    chunk_id = uuid.uuid4().hex\n",
    "    tmp_file = TMP_DIR / f\"{model_name.lower()}_h{h}_{chunk_id}.csv\"\n",
    "    pd.DataFrame(results).to_csv(tmp_file, index=False)\n",
    "    return str(tmp_file)\n",
    "\n",
    "# ---------------------- Merge Helper ---------------------- #\n",
    "\n",
    "def merge_chunks(model_name: str, h: int):\n",
    "    pattern = f\"{model_name.lower()}_h{h}_*.csv\"\n",
    "    files = list(TMP_DIR.glob(pattern))\n",
    "    if not files:\n",
    "        return\n",
    "    df_all = pd.concat([pd.read_csv(f) for f in files], ignore_index=True)\n",
    "    out_file = OUT_DIR / f\"{model_name.lower()}_h{h}_fixed_window_results.csv\"\n",
    "    df_all.to_csv(out_file, index=False)\n",
    "    for f in files:\n",
    "        f.unlink()  # remove temp\n",
    "    print(f\"âœ… Merged {len(files)} chunks â†’ {out_file} ({len(df_all)} rows)\")\n",
    "\n",
    "# ---------------------- Main ---------------------- #\n",
    "if __name__ == \"__main__\":\n",
    "    y_df = load_data()\n",
    "    date_index = y_df.index\n",
    "    y_all = y_df.values\n",
    "\n",
    "    MODELS = {\n",
    "        \"RW\": forecast_RW_fct,\n",
    "        \"AR_yw\": forecast_AR_yw,\n",
    "        \"VAR_yw\": forecast_VAR_yw,\n",
    "        \"DNS_VAR_yw\": forecast_DNS_VAR_yw,\n",
    "    }\n",
    "\n",
    "    base_rows = rolling_indices(date_index)\n",
    "\n",
    "    args_list = []\n",
    "    for model_name in MODELS.keys():\n",
    "        for h in FORECAST_HORIZONS:\n",
    "            for rows_chunk in chunk_list(base_rows, CHUNKS_PER_COMBO):\n",
    "                args_list.append((model_name, h, rows_chunk, y_all, date_index))\n",
    "    \n",
    "    # Run all workers in parallel\n",
    "    print(f\"Launching {len(args_list)} tasks across {MAX_PROCS} processes â€¦\")\n",
    "    with Pool(processes=MAX_PROCS, maxtasksperchild=1) as pool:\n",
    "        pool.map(forecast_worker, args_list)\n",
    "    \n",
    "    # Merge chunked output files into final results\n",
    "    for model_name in MODELS.keys():\n",
    "        for h in FORECAST_HORIZONS:\n",
    "            merge_chunks(model_name, h)\n",
    "    \n",
    "    print(\"ğŸ All modelâ€“horizon result files have been merged and saved.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
