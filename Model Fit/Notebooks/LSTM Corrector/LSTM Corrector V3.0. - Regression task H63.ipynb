{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba3d7190-9c03-4654-89a8-6986aeaad9a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /venv/main/lib/python3.12/site-packages (2.1.2)\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.2.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /venv/main/lib/python3.12/site-packages (4.67.1)\n",
      "Requirement already satisfied: torch in /venv/main/lib/python3.12/site-packages (2.6.0+cu126)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.6.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Collecting optuna\n",
      "  Downloading optuna-4.3.0-py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /venv/main/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: filelock in /venv/main/lib/python3.12/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /venv/main/lib/python3.12/site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: setuptools in /venv/main/lib/python3.12/site-packages (from torch) (70.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /venv/main/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: networkx in /venv/main/lib/python3.12/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /venv/main/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /venv/main/lib/python3.12/site-packages (from torch) (2025.3.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /venv/main/lib/python3.12/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /venv/main/lib/python3.12/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /venv/main/lib/python3.12/site-packages (from torch) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /venv/main/lib/python3.12/site-packages (from torch) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /venv/main/lib/python3.12/site-packages (from torch) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /venv/main/lib/python3.12/site-packages (from torch) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /venv/main/lib/python3.12/site-packages (from torch) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /venv/main/lib/python3.12/site-packages (from torch) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /venv/main/lib/python3.12/site-packages (from torch) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /venv/main/lib/python3.12/site-packages (from torch) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /venv/main/lib/python3.12/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /venv/main/lib/python3.12/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /venv/main/lib/python3.12/site-packages (from torch) (12.6.85)\n",
      "Requirement already satisfied: triton==3.2.0 in /venv/main/lib/python3.12/site-packages (from torch) (3.2.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /venv/main/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Downloading scipy-1.15.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.5.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting alembic>=1.5.0 (from optuna)\n",
      "  Downloading alembic-1.15.2-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting colorlog (from optuna)\n",
      "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /venv/main/lib/python3.12/site-packages (from optuna) (25.0)\n",
      "Collecting sqlalchemy>=1.4.2 (from optuna)\n",
      "  Downloading sqlalchemy-2.0.41-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
      "Requirement already satisfied: PyYAML in /venv/main/lib/python3.12/site-packages (from optuna) (6.0.2)\n",
      "Collecting Mako (from alembic>=1.5.0->optuna)\n",
      "  Downloading mako-1.3.10-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: six>=1.5 in /venv/main/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Collecting greenlet>=1 (from sqlalchemy>=1.4.2->optuna)\n",
      "  Downloading greenlet-3.2.2-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /venv/main/lib/python3.12/site-packages (from jinja2->torch) (2.1.5)\n",
      "Downloading pandas-2.2.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.7/12.7 MB\u001b[0m \u001b[31m51.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading scikit_learn-1.6.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m97.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading optuna-4.3.0-py3-none-any.whl (386 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m386.6/386.6 kB\u001b[0m \u001b[31m62.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading alembic-1.15.2-py3-none-any.whl (231 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m231.9/231.9 kB\u001b[0m \u001b[31m50.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.5.0-py3-none-any.whl (307 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m54.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m509.2/509.2 kB\u001b[0m \u001b[31m79.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.15.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.3/37.3 MB\u001b[0m \u001b[31m83.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading sqlalchemy-2.0.41-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m108.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m347.8/347.8 kB\u001b[0m \u001b[31m68.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
      "Downloading greenlet-3.2.2-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (603 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m603.9/603.9 kB\u001b[0m \u001b[31m70.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mako-1.3.10-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pytz, tzdata, threadpoolctl, scipy, Mako, joblib, greenlet, colorlog, sqlalchemy, scikit-learn, pandas, alembic, optuna\n",
      "Successfully installed Mako-1.3.10 alembic-1.15.2 colorlog-6.9.0 greenlet-3.2.2 joblib-1.5.0 optuna-4.3.0 pandas-2.2.3 pytz-2025.2 scikit-learn-1.6.1 scipy-1.15.3 sqlalchemy-2.0.41 threadpoolctl-3.6.0 tzdata-2025.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install numpy pandas tqdm torch scikit-learn optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ca08370-cf0e-4d95-a40d-d0fe372f3306",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T08:58:29.457521Z",
     "start_time": "2025-05-16T08:58:27.714738Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Device: cuda\n",
      "  • GPU: NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================\n",
    "#  LSTM Regression on DNS_KF Forecast Errors\n",
    "#  --------------------------------------------------------------\n",
    "#  • Expanding‑window CV (train → val blocks)\n",
    "#  • Rolling look‑back  = 756 b‑days  (3 yrs)\n",
    "#  • Validation block   = 252 b‑days  (≈1 yr)\n",
    "#  • Forecast horizon h = configurable (here default = 1)\n",
    "#  --------------------------------------------------------------\n",
    "#  This file merges the working CV logic from the “second model”\n",
    "#  into the original DNS_KF error‑prediction script.\n",
    "# ==============================================================\n",
    "\n",
    "import os, time, random, ast, gc\n",
    "from typing import List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tqdm import tqdm\n",
    "\n",
    "import optuna\n",
    "from optuna.pruners import MedianPruner\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "# -------------------------- Repro ----------------------------- #\n",
    "RNG_SEED = 42\n",
    "random.seed(RNG_SEED); np.random.seed(RNG_SEED); torch.manual_seed(RNG_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(RNG_SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"[INFO] Device: {device}\")\n",
    "if device.type == \"cuda\":\n",
    "    print(f\"  • GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# --------------------------- Config --------------------------- #\n",
    "BUSINESS_DAYS_YEAR = 252\n",
    "ROLL_YEARS         = 3\n",
    "SEQ_LEN_DEFAULT    = BUSINESS_DAYS_YEAR * ROLL_YEARS   # 756\n",
    "VAL_WINDOW         = BUSINESS_DAYS_YEAR                # 252\n",
    "HOLDOUT_WINDOW     = BUSINESS_DAYS_YEAR * 3            # 756  (≈ 3 yrs)\n",
    "\n",
    "# Forecast‑horizon‑dependent sequence length (map if needed)\n",
    "SEQ_LEN_MAP = {\n",
    "    1: 756,   # 3 yrs\n",
    "    5: 756,\n",
    "    21: 756,\n",
    "    63: 756,\n",
    "    252: 756,\n",
    "}\n",
    "\n",
    "EARLY_STOP_PATIENCE = 20\n",
    "\n",
    "HSPACE = {\n",
    "    \"hidden_dim\":   (32, 192),\n",
    "    \"num_layers\":   [1, 2, 3],\n",
    "    \"dropout\":      (0.0, 0.6),\n",
    "    \"learning_rate\":(1e-4, 5e-3),\n",
    "    \"batch_size\":   [32, 64, 128],\n",
    "    \"epochs\":       (40, 80),\n",
    "}\n",
    "\n",
    "# --------------------------- Model --------------------------- #\n",
    "class LSTMRegressor(nn.Module):\n",
    "    def __init__(self, in_dim:int, hid:int, layers:int, out_dim:int=1, drop:float=0.0):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(in_dim, hid, layers, batch_first=True,\n",
    "                            dropout=(drop if layers>1 else 0.0))\n",
    "        self.drop = nn.Dropout(drop)\n",
    "        self.norm = nn.LayerNorm(hid)\n",
    "        self.fc   = nn.Linear(hid, out_dim, bias=False)\n",
    "    def forward(self, x:torch.Tensor)->torch.Tensor:\n",
    "        _, (h_n, _) = self.lstm(x)\n",
    "        return self.fc(self.norm(self.drop(h_n[-1])))\n",
    "\n",
    "# --------------------------- Utility Functions --------------------------- #\n",
    "\n",
    "def load_target(horizon: int) -> pd.DataFrame:\n",
    "    path = fr\"dns_kf_total_h{horizon}_full_dataset.csv\"\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(path)\n",
    "    df = pd.read_csv(path, parse_dates=[\"eval_date\"]).sort_values(\"eval_date\")\n",
    "\n",
    "    true = df[\"true_yields\"].apply(_parse_vec)\n",
    "    pred = df[\"forecast_yields\"].apply(_parse_vec)\n",
    "    errors = pred.subtract(true)\n",
    "\n",
    "    return pd.DataFrame(errors.tolist(),\n",
    "                        index=df[\"eval_date\"],\n",
    "                        columns=[f\"err_{i}\" for i in range(6)])\n",
    "\n",
    "def _parse_vec(col: str) -> np.ndarray:\n",
    "    import ast\n",
    "    return np.asarray(ast.literal_eval(col), dtype=np.float32)\n",
    "\n",
    "# -------------------- Debug Fold Info ------------------------ #\n",
    "\n",
    "def debug_cv_folds(folds):\n",
    "    print(\"\\n[DEBUG] Fold summary:\\n\")\n",
    "    for i, f in enumerate(folds, 1):\n",
    "        print(f\"--- Fold {i} ---\")\n",
    "        print(f\"Train X: {f['X_tr'].index[0].date()} → {f['X_tr'].index[-1].date()} ({len(f['X_tr'])} rows)\")\n",
    "        print(f\"Val   X: {f['X_va'].index[0].date()} → {f['X_va'].index[-1].date()} ({len(f['X_va'])} rows)\")\n",
    "        print(f\"Train Y: {f['Y_tr'].index[0].date()} → {f['Y_tr'].index[-1].date()} ({len(f['Y_tr'])} rows)\")\n",
    "        print(f\"Val   Y: {f['Y_va'].index[0].date()} → {f['Y_va'].index[-1].date()} ({len(f['Y_va'])} rows)\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "# --------------------- Sequence Generator ------------------------ #\n",
    "\n",
    "def gen_seq(X_df: pd.DataFrame, Y_fold: pd.Series, seq_len: int, h: int):\n",
    "    X_arr = X_df.values.astype(np.float32)\n",
    "    idx_map = {ts: i for i, ts in enumerate(X_df.index)}\n",
    "\n",
    "    X_seq, Y_seq = [], []\n",
    "\n",
    "    for target_ts in Y_fold.index:\n",
    "        t = idx_map.get(target_ts)\n",
    "        if t is None or t + h >= len(X_df):  # ensure X has h-step-ahead\n",
    "            continue\n",
    "\n",
    "        # Predict y at time t+h from X[t - seq_len + 1 to t]\n",
    "        start = t - seq_len + 1\n",
    "        end   = t + 1\n",
    "        if start < 0:\n",
    "            continue\n",
    "\n",
    "        window = X_arr[start:end]\n",
    "        if np.isnan(window).any():\n",
    "            continue\n",
    "\n",
    "        future_ts = X_df.index[t + h]\n",
    "        if future_ts not in Y_fold:\n",
    "            continue\n",
    "\n",
    "        X_seq.append(window)\n",
    "        Y_seq.append(np.float32(Y_fold.loc[future_ts]))\n",
    "\n",
    "    if not X_seq:\n",
    "        print(\"[WARN] No sequences generated.\")\n",
    "        return np.empty((0, seq_len, X_arr.shape[1]), dtype=np.float32), np.empty((0, 1), dtype=np.float32)\n",
    "\n",
    "    return np.stack(X_seq), np.asarray(Y_seq)[:, None]\n",
    "\n",
    "# ----------------------- CV Generator ------------------------ #\n",
    "\n",
    "def make_folds(X: pd.DataFrame, Y: pd.Series, horizon: int):\n",
    "    \"\"\"Expanding‑window folds with correct alignment.\n",
    "       Each fold dict contains X_tr, Y_tr, X_va, Y_va, seq_len.\n",
    "    \"\"\"\n",
    "    seq_len = SEQ_LEN_MAP.get(horizon, SEQ_LEN_DEFAULT)\n",
    "    total   = len(X)\n",
    "\n",
    "    train_y_len   = 252\n",
    "    val_y_len     = 504\n",
    "    holdout_len   = 756\n",
    "\n",
    "    folds = []\n",
    "    min_required = seq_len + horizon + train_y_len + 504\n",
    "    val_start = min_required\n",
    "\n",
    "    while val_start + val_y_len + holdout_len <= total:\n",
    "        # Training set (expanding up to val_start - val_y_len)\n",
    "        y_tr_end   = val_start - val_y_len\n",
    "        y_tr_start = y_tr_end - train_y_len\n",
    "        x_tr_end   = y_tr_end - horizon\n",
    "        x_tr_start = max(0, x_tr_end - seq_len - train_y_len)\n",
    "\n",
    "        # Validation set\n",
    "        y_va_start = y_tr_end\n",
    "        y_va_end   = y_va_start + val_y_len\n",
    "        x_va_end   = y_va_end - horizon\n",
    "        x_va_start = max(0, y_va_start - seq_len - horizon)\n",
    "\n",
    "        if y_tr_start < 0 or x_tr_start < 0:\n",
    "            break\n",
    "\n",
    "        folds.append({\n",
    "            \"X_tr\": X.iloc[x_tr_start:x_tr_end].copy(),\n",
    "            \"Y_tr\": Y.iloc[y_tr_start:y_tr_end].copy(),\n",
    "            \"X_va\": X.iloc[x_va_start:x_va_end].copy(),\n",
    "            \"Y_va\": Y.iloc[y_va_start:y_va_end].copy(),\n",
    "            \"seq_len\": seq_len,\n",
    "        })\n",
    "\n",
    "        val_start += val_y_len\n",
    "        train_y_len += val_y_len\n",
    "\n",
    "    return folds\n",
    "\n",
    "# ---------------------- Debug Printer ------------------------ #\n",
    "\n",
    "def debug_folds(folds:List[dict]):\n",
    "    print(f\"[DEBUG] Created {len(folds)} folds\\n\")\n",
    "    for i,f in enumerate(folds,1):\n",
    "        tr_x, tr_y = len(f[\"X_tr\"]), len(f[\"Y_tr\"])\n",
    "        va_x, va_y = len(f[\"X_va\"]), len(f[\"Y_va\"])\n",
    "        print(f\"--- Fold {i} ---\")\n",
    "        print(f\"Train‑Y rows : {tr_y:4d}   ({f['Y_tr'].index[0].date()} → {f['Y_tr'].index[-1].date()})\")\n",
    "        print(f\"Train‑X rows : {tr_x:4d}   ({f['X_tr'].index[0].date()} → {f['X_tr'].index[-1].date()})\")\n",
    "        print(f\"Val‑Y rows   : {va_y:4d}   ({f['Y_va'].index[0].date()} → {f['Y_va'].index[-1].date()})\")\n",
    "        print(f\"Val‑X rows   : {va_x:4d}   ({f['X_va'].index[0].date()} → {f['X_va'].index[-1].date()})\")\n",
    "        print(\"-\")\n",
    "\n",
    "# -------------------- Optuna Objective ----------------------- #\n",
    "\n",
    "def optuna_objective(trial, folds, horizon):\n",
    "    p = {\n",
    "        \"hid\": trial.suggest_int(\"hidden_dim\", *HSPACE[\"hidden_dim\"]),\n",
    "        \"lay\": trial.suggest_categorical(\"num_layers\", HSPACE[\"num_layers\"]),\n",
    "        \"drp\": trial.suggest_float(\"dropout\", *HSPACE[\"dropout\"]),\n",
    "        \"lr\" : trial.suggest_float(\"learning_rate\", *HSPACE[\"learning_rate\"], log=True),\n",
    "        \"bs\" : trial.suggest_categorical(\"batch_size\", HSPACE[\"batch_size\"]),\n",
    "        \"ep\" : trial.suggest_int(\"epochs\", *HSPACE[\"epochs\"]),\n",
    "    }\n",
    "\n",
    "    fold_mse = []\n",
    "    scaler = torch.amp.GradScaler('cuda')\n",
    "    global_step = 0  # ✅ unique and flat step counter\n",
    "\n",
    "    for fold_idx, f in enumerate(tqdm(folds, desc=f\"[Trial {trial.number}] Evaluating folds\"), 1):\n",
    "        sc = StandardScaler()\n",
    "        X_tr_s = pd.DataFrame(sc.fit_transform(f[\"X_tr\"]), index=f[\"X_tr\"].index, columns=f[\"X_tr\"].columns)\n",
    "        X_va_s = pd.DataFrame(sc.transform(f[\"X_va\"]),     index=f[\"X_va\"].index, columns=f[\"X_va\"].columns)\n",
    "\n",
    "        Xtr, Ytr = gen_seq(X_tr_s, f[\"Y_tr\"], f[\"seq_len\"], horizon)\n",
    "        Xva, Yva = gen_seq(X_va_s, f[\"Y_va\"], f[\"seq_len\"], horizon)\n",
    "        if len(Xtr) == 0 or len(Xva) == 0:\n",
    "            continue\n",
    "\n",
    "        model = LSTMRegressor(Xtr.shape[2], p[\"hid\"], p[\"lay\"], Ytr.shape[1], p[\"drp\"]).to(device)\n",
    "        opt   = torch.optim.Adam(model.parameters(), lr=p[\"lr\"])\n",
    "        best  = np.inf\n",
    "        patience = 0\n",
    "\n",
    "        train_loader = DataLoader(TensorDataset(torch.tensor(Xtr), torch.tensor(Ytr)), batch_size=p[\"bs\"], shuffle=True)\n",
    "        va_loader    = DataLoader(TensorDataset(torch.tensor(Xva), torch.tensor(Yva)), batch_size=p[\"bs\"])\n",
    "\n",
    "        for epoch in range(p[\"ep\"]):\n",
    "            model.train()\n",
    "            for xb, yb in train_loader:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                opt.zero_grad()\n",
    "                with torch.amp.autocast(device_type='cuda'):\n",
    "                    pred = model(xb)\n",
    "                    loss = nn.functional.mse_loss(pred, yb)\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(opt)\n",
    "                scaler.update()\n",
    "\n",
    "            # Evaluation\n",
    "            model.eval()\n",
    "            preds, gts = [], []\n",
    "            with torch.no_grad(), torch.amp.autocast(device_type='cuda'):\n",
    "                for xb, yb in va_loader:\n",
    "                    preds.append(model(xb.to(device)).cpu())\n",
    "                    gts.append(yb)\n",
    "\n",
    "            mse = mean_squared_error(torch.cat(gts).numpy(), torch.cat(preds).numpy())\n",
    "\n",
    "            # ✅ Global step is guaranteed to be unique now\n",
    "            trial.report(mse, step=global_step)\n",
    "            global_step += 1\n",
    "\n",
    "            if trial.should_prune():\n",
    "                raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "            if mse < best:\n",
    "                best = mse\n",
    "                patience = 0\n",
    "            else:\n",
    "                patience += 1\n",
    "                if patience >= EARLY_STOP_PATIENCE:\n",
    "                    break\n",
    "\n",
    "\n",
    "        fold_mse.append(best)\n",
    "\n",
    "    return np.mean(fold_mse) if fold_mse else float(\"inf\")\n",
    "\n",
    "# -------------------- Run Experiment ------------------------ #\n",
    "\n",
    "def main_notebook(horizon: int, trials: int = 30, n_jobs: int = 1):\n",
    "    # 1. Load features + target\n",
    "    X_df = pd.read_csv(\"X_df_filtered_shap.csv\", index_col=0, parse_dates=True)\n",
    "    y_df = load_target(horizon)\n",
    "\n",
    "    # 2. Join and clean\n",
    "    X_df = X_df.join(y_df)\n",
    "    X_df.dropna(inplace=True)\n",
    "    \n",
    "    common_dates = X_df.index.intersection(y_df.index)\n",
    "    X_df = X_df.loc[common_dates]\n",
    "    y_df = y_df.loc[common_dates]\n",
    "    \n",
    "    y_ser = y_df.mean(axis=1).rename(\"err\")  # raw directional error\n",
    "\n",
    "    # 3. Generate CV folds\n",
    "    folds = make_folds(X_df, y_ser, horizon)\n",
    "    print(f\"Generated {len(folds)} folds\\n\")\n",
    "    debug_cv_folds(folds)\n",
    "\n",
    "    # 4. Run Optuna\n",
    "    study = optuna.create_study(\n",
    "        direction=\"minimize\",\n",
    "        sampler=TPESampler(seed=RNG_SEED),\n",
    "        pruner=MedianPruner(n_startup_trials=8, n_warmup_steps=15)\n",
    "    )\n",
    "\n",
    "    t0 = time.time()\n",
    "    study.optimize(lambda tr: optuna_objective(tr, folds, horizon),\n",
    "                   n_trials=trials, n_jobs=n_jobs, show_progress_bar=True)\n",
    "    duration = time.time() - t0\n",
    "\n",
    "    # 5. Print results\n",
    "    print(\"=== Best Trial ===\")\n",
    "    print(f\"MSE   : {study.best_value:.6f}\")\n",
    "    print(f\"Params: {study.best_trial.params}\")\n",
    "    print(f\"Duration: {duration/60:.1f} min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8aed198ecfba14c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-17 20:22:16,612] A new study created in memory with name: no-name-334e9434-1c50-46e0-b5c7-7518428d0639\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 4 folds\n",
      "\n",
      "\n",
      "[DEBUG] Fold summary:\n",
      "\n",
      "--- Fold 1 ---\n",
      "Train X: 2006-11-15 → 2010-09-27 (1008 rows)\n",
      "Val   X: 2007-11-02 → 2012-09-03 (1260 rows)\n",
      "Train Y: 2010-01-06 → 2010-12-23 (252 rows)\n",
      "Val   Y: 2010-12-24 → 2012-11-29 (504 rows)\n",
      "------------------------------------------------------------\n",
      "--- Fold 2 ---\n",
      "Train X: 2006-11-15 → 2012-09-03 (1512 rows)\n",
      "Val   X: 2009-10-09 → 2014-08-08 (1260 rows)\n",
      "Train Y: 2010-01-06 → 2012-11-29 (756 rows)\n",
      "Val   Y: 2012-11-30 → 2014-11-05 (504 rows)\n",
      "------------------------------------------------------------\n",
      "--- Fold 3 ---\n",
      "Train X: 2006-11-15 → 2014-08-08 (2016 rows)\n",
      "Val   X: 2011-09-15 → 2016-07-20 (1260 rows)\n",
      "Train Y: 2010-01-06 → 2014-11-05 (1260 rows)\n",
      "Val   Y: 2014-11-06 → 2016-10-17 (504 rows)\n",
      "------------------------------------------------------------\n",
      "--- Fold 4 ---\n",
      "Train X: 2006-11-15 → 2016-07-20 (2520 rows)\n",
      "Val   X: 2013-08-22 → 2018-06-26 (1260 rows)\n",
      "Train Y: 2010-01-06 → 2016-10-17 (1764 rows)\n",
      "Val   Y: 2016-10-18 → 2018-09-21 (504 rows)\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61aca9bc6f6f49cea33206c497916929",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[Trial 0] Evaluating folds:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "[Trial 0] Evaluating folds:  25%|██▌       | 1/4 [00:11<00:35, 11.75s/it]\u001b[A\n",
      "[Trial 0] Evaluating folds:  50%|█████     | 2/4 [00:30<00:32, 16.12s/it]\u001b[A\n",
      "[Trial 0] Evaluating folds:  75%|███████▌  | 3/4 [02:08<00:53, 53.24s/it]\u001b[A\n",
      "[Trial 0] Evaluating folds: 100%|██████████| 4/4 [02:51<00:00, 42.88s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-05-17 20:25:08,200] Trial 0 finished with value: 0.115943418815732 and parameters: {'hidden_dim': 92, 'num_layers': 1, 'dropout': 0.0936111842654619, 'learning_rate': 0.00018408992080552527, 'batch_size': 64, 'epochs': 69}. Best is trial 0 with value: 0.115943418815732.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[Trial 1] Evaluating folds:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "[Trial 1] Evaluating folds:  25%|██▌       | 1/4 [00:19<00:58, 19.45s/it]\u001b[A\n",
      "[Trial 1] Evaluating folds:  50%|█████     | 2/4 [00:59<01:03, 31.72s/it]\u001b[A\n",
      "[Trial 1] Evaluating folds:  75%|███████▌  | 3/4 [02:06<00:47, 47.79s/it]\u001b[A\n",
      "[Trial 1] Evaluating folds: 100%|██████████| 4/4 [03:22<00:00, 50.56s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-05-17 20:28:30,497] Trial 1 finished with value: 0.2480349913239479 and parameters: {'hidden_dim': 35, 'num_layers': 1, 'dropout': 0.10909498032426036, 'learning_rate': 0.0002049268011541737, 'batch_size': 64, 'epochs': 51}. Best is trial 0 with value: 0.115943418815732.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[Trial 2] Evaluating folds:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "[Trial 2] Evaluating folds:  25%|██▌       | 1/4 [00:12<00:36, 12.19s/it]\u001b[A\n",
      "[Trial 2] Evaluating folds:  50%|█████     | 2/4 [00:32<00:33, 16.90s/it]\u001b[A\n",
      "[Trial 2] Evaluating folds:  75%|███████▌  | 3/4 [01:21<00:31, 31.57s/it]\u001b[A\n",
      "[Trial 2] Evaluating folds: 100%|██████████| 4/4 [02:00<00:00, 30.05s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-05-17 20:30:30,729] Trial 2 finished with value: 0.03647817112505436 and parameters: {'hidden_dim': 130, 'num_layers': 3, 'dropout': 0.27364199053022153, 'learning_rate': 0.0021576967455896826, 'batch_size': 128, 'epochs': 41}. Best is trial 2 with value: 0.03647817112505436.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[Trial 3] Evaluating folds:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "[Trial 3] Evaluating folds:  25%|██▌       | 1/4 [00:18<00:54, 18.26s/it]\u001b[A\n",
      "[Trial 3] Evaluating folds:  50%|█████     | 2/4 [00:37<00:38, 19.11s/it]\u001b[A\n",
      "[Trial 3] Evaluating folds:  75%|███████▌  | 3/4 [01:22<00:30, 30.52s/it]\u001b[A\n",
      "[Trial 3] Evaluating folds: 100%|██████████| 4/4 [02:05<00:00, 31.29s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-05-17 20:32:35,932] Trial 3 finished with value: 0.029958782717585564 and parameters: {'hidden_dim': 129, 'num_layers': 3, 'dropout': 0.5793792198447356, 'learning_rate': 0.0023628864184236428, 'batch_size': 128, 'epochs': 58}. Best is trial 3 with value: 0.029958782717585564.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[Trial 4] Evaluating folds:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "[Trial 4] Evaluating folds:  25%|██▌       | 1/4 [00:12<00:38, 12.74s/it]\u001b[A\n",
      "[Trial 4] Evaluating folds:  50%|█████     | 2/4 [00:31<00:32, 16.49s/it]\u001b[A\n",
      "[Trial 4] Evaluating folds:  75%|███████▌  | 3/4 [01:31<00:36, 36.17s/it]\u001b[A\n",
      "[Trial 4] Evaluating folds: 100%|██████████| 4/4 [02:11<00:00, 32.76s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-05-17 20:34:47,013] Trial 4 finished with value: 0.02706875652074814 and parameters: {'hidden_dim': 51, 'num_layers': 3, 'dropout': 0.15526798896001015, 'learning_rate': 0.0013353819088790589, 'batch_size': 128, 'epochs': 47}. Best is trial 4 with value: 0.02706875652074814.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[Trial 5] Evaluating folds:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "[Trial 5] Evaluating folds:  25%|██▌       | 1/4 [00:13<00:38, 12.98s/it]\u001b[A\n",
      "[Trial 5] Evaluating folds:  50%|█████     | 2/4 [00:35<00:36, 18.44s/it]\u001b[A\n",
      "[Trial 5] Evaluating folds:  75%|███████▌  | 3/4 [01:47<00:43, 43.00s/it]\u001b[A\n",
      "[Trial 5] Evaluating folds: 100%|██████████| 4/4 [02:43<00:00, 40.89s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-05-17 20:37:30,594] Trial 5 finished with value: 0.025974877178668976 and parameters: {'hidden_dim': 188, 'num_layers': 2, 'dropout': 0.3587399872866511, 'learning_rate': 0.0036832964384234204, 'batch_size': 64, 'epochs': 53}. Best is trial 5 with value: 0.025974877178668976.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[Trial 6] Evaluating folds:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "[Trial 6] Evaluating folds:  25%|██▌       | 1/4 [00:09<00:27,  9.26s/it]\u001b[A\n",
      "[Trial 6] Evaluating folds:  50%|█████     | 2/4 [00:32<00:35, 17.58s/it]\u001b[A\n",
      "[Trial 6] Evaluating folds:  75%|███████▌  | 3/4 [01:36<00:38, 38.92s/it]\u001b[A\n",
      "[Trial 6] Evaluating folds: 100%|██████████| 4/4 [02:22<00:00, 35.72s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-05-17 20:39:53,589] Trial 6 finished with value: 0.056067117024213076 and parameters: {'hidden_dim': 94, 'num_layers': 2, 'dropout': 0.16856070581242846, 'learning_rate': 0.0008356499023325525, 'batch_size': 64, 'epochs': 80}. Best is trial 5 with value: 0.025974877178668976.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[Trial 7] Evaluating folds:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "[Trial 7] Evaluating folds:  25%|██▌       | 1/4 [00:20<01:02, 20.69s/it]\u001b[A\n",
      "[Trial 7] Evaluating folds:  50%|█████     | 2/4 [00:57<01:00, 30.17s/it]\u001b[A\n",
      "[Trial 7] Evaluating folds:  75%|███████▌  | 3/4 [02:28<00:58, 58.07s/it]\u001b[A\n",
      "[Trial 7] Evaluating folds: 100%|██████████| 4/4 [03:42<00:00, 55.59s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-05-17 20:43:36,009] Trial 7 finished with value: 0.02843462862074375 and parameters: {'hidden_dim': 156, 'num_layers': 3, 'dropout': 0.4241144063085703, 'learning_rate': 0.001732053535845956, 'batch_size': 32, 'epochs': 44}. Best is trial 5 with value: 0.025974877178668976.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[Trial 8] Evaluating folds:   0%|          | 0/4 [00:06<?, ?it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-05-17 20:43:42,289] Trial 8 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[Trial 9] Evaluating folds:   0%|          | 0/4 [00:08<?, ?it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-05-17 20:43:51,275] Trial 9 pruned. \n",
      "=== Best Trial ===\n",
      "MSE   : 0.025975\n",
      "Params: {'hidden_dim': 188, 'num_layers': 2, 'dropout': 0.3587399872866511, 'learning_rate': 0.0036832964384234204, 'batch_size': 64, 'epochs': 53}\n",
      "Duration: 21.6 min\n"
     ]
    }
   ],
   "source": [
    "# -------------------- Execution ------------------------ #\n",
    "\n",
    "if __name__ == \"__main__\": \n",
    "    main_notebook(horizon=63, trials=10, n_jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1b127d15-345e-4d97-a928-3d7485ea6712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/53 - Train MSE: 0.173504\n",
      "Epoch 2/53 - Train MSE: 0.003445\n",
      "Epoch 3/53 - Train MSE: 0.001817\n",
      "Epoch 4/53 - Train MSE: 0.001521\n",
      "Epoch 5/53 - Train MSE: 0.001462\n",
      "Epoch 6/53 - Train MSE: 0.001432\n",
      "Epoch 7/53 - Train MSE: 0.001465\n",
      "Epoch 8/53 - Train MSE: 0.001478\n",
      "Epoch 9/53 - Train MSE: 0.001358\n",
      "Epoch 10/53 - Train MSE: 0.001600\n",
      "Epoch 11/53 - Train MSE: 0.001446\n",
      "Epoch 12/53 - Train MSE: 0.001324\n",
      "Epoch 13/53 - Train MSE: 0.001291\n",
      "Epoch 14/53 - Train MSE: 0.001273\n",
      "Epoch 15/53 - Train MSE: 0.001455\n",
      "Epoch 16/53 - Train MSE: 0.001293\n",
      "Epoch 17/53 - Train MSE: 0.001279\n",
      "Epoch 18/53 - Train MSE: 0.001223\n",
      "Epoch 19/53 - Train MSE: 0.001139\n",
      "Epoch 20/53 - Train MSE: 0.001122\n",
      "Epoch 21/53 - Train MSE: 0.001156\n",
      "Epoch 22/53 - Train MSE: 0.001129\n",
      "Epoch 23/53 - Train MSE: 0.001071\n",
      "Epoch 24/53 - Train MSE: 0.001085\n",
      "Epoch 25/53 - Train MSE: 0.001196\n",
      "Epoch 26/53 - Train MSE: 0.001016\n",
      "Epoch 27/53 - Train MSE: 0.001163\n",
      "Epoch 28/53 - Train MSE: 0.001086\n",
      "Epoch 29/53 - Train MSE: 0.001034\n",
      "Epoch 30/53 - Train MSE: 0.001186\n",
      "Epoch 31/53 - Train MSE: 0.001016\n",
      "Epoch 32/53 - Train MSE: 0.000979\n",
      "Epoch 33/53 - Train MSE: 0.001063\n",
      "Epoch 34/53 - Train MSE: 0.000938\n",
      "Epoch 35/53 - Train MSE: 0.001038\n",
      "Epoch 36/53 - Train MSE: 0.000903\n",
      "Epoch 37/53 - Train MSE: 0.000909\n",
      "Epoch 38/53 - Train MSE: 0.000848\n",
      "Epoch 39/53 - Train MSE: 0.000918\n",
      "Epoch 40/53 - Train MSE: 0.000876\n",
      "Epoch 41/53 - Train MSE: 0.000848\n",
      "Epoch 42/53 - Train MSE: 0.000817\n",
      "Epoch 43/53 - Train MSE: 0.000847\n",
      "Epoch 44/53 - Train MSE: 0.001037\n",
      "Epoch 45/53 - Train MSE: 0.000965\n",
      "Epoch 46/53 - Train MSE: 0.000869\n",
      "Epoch 47/53 - Train MSE: 0.000808\n",
      "Epoch 48/53 - Train MSE: 0.000777\n",
      "Epoch 49/53 - Train MSE: 0.000765\n",
      "Epoch 50/53 - Train MSE: 0.000829\n",
      "Epoch 51/53 - Train MSE: 0.000848\n",
      "Epoch 52/53 - Train MSE: 0.000853\n",
      "Epoch 53/53 - Train MSE: 0.000821\n",
      "Holdout MSE (h=1): 0.003819\n"
     ]
    }
   ],
   "source": [
    "# -------------------- Holdout Evaluation --------------------\n",
    "# Ensure best hyperparameters are defined (e.g., from previous Optuna run)\n",
    "best_params = {\n",
    "    'hidden_dim': 188,\n",
    "    'num_layers': 2,\n",
    "    'dropout': 0.3587399872866511,\n",
    "    'learning_rate': 0.0036832964384234204,\n",
    "    'batch_size': 64,\n",
    "    'epochs': 53\n",
    "}\n",
    "\n",
    "# Define forecast horizon (must match your Optuna setup)\n",
    "horizon = 1\n",
    "\n",
    "# 1. Split data into full-training and holdout\n",
    "HOLDOUT_LEN = HOLDOUT_WINDOW  # 756 business days\n",
    "holdout_dates = y_ser.index[-HOLDOUT_LEN:]\n",
    "holdout_start = holdout_dates[0]\n",
    "\n",
    "# Training set: before holdout\n",
    "train_dates = y_ser.index[y_ser.index < holdout_start]\n",
    "X_train = X_df.loc[train_dates]\n",
    "y_train = y_ser.loc[train_dates]\n",
    "\n",
    "# Context for holdout: include seq_len history before holdout start\\seq_len = SEQ_LEN_MAP.get(horizon, SEQ_LEN_DEFAULT)\n",
    "hold_context_start = X_df.index.get_loc(holdout_start) - seq_len + 1\n",
    "X_hold_context = X_df.iloc[hold_context_start:]\n",
    "y_hold = y_ser.loc[holdout_dates]\n",
    "\n",
    "# 2. Scale features\n",
    "scaler = StandardScaler()\n",
    "X_tr_scaled = pd.DataFrame(scaler.fit_transform(X_train), index=X_train.index, columns=X_train.columns)\n",
    "X_hold_scaled = pd.DataFrame(scaler.transform(X_hold_context), index=X_hold_context.index, columns=X_hold_context.columns)\n",
    "\n",
    "# 3. Generate sequences and capture timestamps for holdout\n",
    "# Training sequences\n",
    "X_tr_seq, Y_tr_seq = gen_seq(X_tr_scaled, y_train, seq_len, horizon)\n",
    "# Holdout sequences with timestamps\n",
    "# We'll replicate gen_seq logic to also record eval timestamps\n",
    "def gen_seq_with_times(X_df, Y_fold, seq_len, h):\n",
    "    X_arr = X_df.values.astype(np.float32)\n",
    "    idx_map = {ts: i for i, ts in enumerate(X_df.index)}\n",
    "    X_seq, Y_seq, times = [], [], []\n",
    "    for target_ts in Y_fold.index:\n",
    "        t = idx_map.get(target_ts)\n",
    "        if t is None or t + h >= len(X_df):\n",
    "            continue\n",
    "        start = t - seq_len + 1\n",
    "        end = t + 1\n",
    "        if start < 0:\n",
    "            continue\n",
    "        window = X_arr[start:end]\n",
    "        if np.isnan(window).any():\n",
    "            continue\n",
    "        future_ts = X_df.index[t + h]\n",
    "        if future_ts not in Y_fold:\n",
    "            continue\n",
    "        X_seq.append(window)\n",
    "        Y_seq.append(np.float32(Y_fold.loc[future_ts]))\n",
    "        times.append(future_ts)\n",
    "    return np.stack(X_seq), np.asarray(Y_seq)[:, None], times\n",
    "\n",
    "X_hold_seq, Y_hold_seq, hold_times = gen_seq_with_times(X_hold_scaled, y_hold, seq_len, horizon)\n",
    "if X_hold_seq.shape[0] == 0:\n",
    "    raise ValueError(\"No holdout sequences generated: ensure data length >= seq_len + horizon.\")\n",
    "\n",
    "# 4. Instantiate model\n",
    "model = LSTMRegressor(\n",
    "    in_dim=X_tr_seq.shape[2], hid=best_params['hidden_dim'], layers=best_params['num_layers'],\n",
    "    out_dim=Y_tr_seq.shape[1], drop=best_params['dropout']\n",
    ").to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=best_params['learning_rate'])\n",
    "\n",
    "# 5. Train on full training\n",
    "train_loader = DataLoader(TensorDataset(torch.tensor(X_tr_seq), torch.tensor(Y_tr_seq)),\n",
    "                          batch_size=best_params['batch_size'], shuffle=True)\n",
    "for epoch in range(1, best_params['epochs'] + 1):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(xb)\n",
    "        loss = nn.functional.mse_loss(pred, yb)\n",
    "        loss.backward(); optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "    print(f\"Epoch {epoch}/{best_params['epochs']} - Train MSE: {np.mean(losses):.6f}\")\n",
    "\n",
    "# 6. Evaluate on holdout and save results\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    Xh = torch.tensor(X_hold_seq).to(device)\n",
    "    preds = model(Xh).cpu().numpy().flatten()\n",
    "# Build results DataFrame\n",
    "df_results = pd.DataFrame({\n",
    "    'eval_date': hold_times,\n",
    "    'horizon': horizon,\n",
    "    'forecasted_error': preds,\n",
    "    'true_error': Y_hold_seq.flatten()\n",
    "})\n",
    "df_results.to_csv(f'holdout_forecasts_h{horizon}.csv', index=False)\n",
    "\n",
    "# 7. Report holdout MSE\n",
    "mse_hold = mean_squared_error(Y_hold_seq, preds)\n",
    "print(f\"Holdout MSE (h={horizon}): {mse_hold:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1810c052-ad7f-4757-8163-f4af984ecd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon = 1\n",
    "\n",
    "# 1. Load features + target\n",
    "X_df = pd.read_csv(\"X_df_filtered_shap.csv\", index_col=0, parse_dates=True)\n",
    "y_df = load_target(horizon)\n",
    "\n",
    "# 2. Join and clean\n",
    "X_df = X_df.join(y_df)\n",
    "X_df.dropna(inplace=True)\n",
    "\n",
    "common_dates = X_df.index.intersection(y_df.index)\n",
    "X_df = X_df.loc[common_dates]\n",
    "y_df = y_df.loc[common_dates]\n",
    "\n",
    "y_ser = y_df.mean(axis=1).rename(\"err\")  # raw directional error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c3f99dcf-33ee-4f09-beb2-646df70b3fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 dates in y_df not found in X_df:\n",
      " DatetimeIndex(['2008-12-25', '2011-12-16', '2015-10-01', '2015-10-02',\n",
      "               '2015-10-07', '2015-10-08', '2020-03-26'],\n",
      "              dtype='datetime64[ns]', freq=None)\n"
     ]
    }
   ],
   "source": [
    "def find_y_not_in_x(X_df: pd.DataFrame, y_df: pd.DataFrame) -> pd.DatetimeIndex:\n",
    "    \"\"\"\n",
    "    Returns dates present in y_df index but not in X_df index.\n",
    "    \"\"\"\n",
    "    x_dates = pd.to_datetime(X_df.index)\n",
    "    y_dates = pd.to_datetime(y_df.index)\n",
    "    missing_dates = y_dates.difference(x_dates)\n",
    "    return missing_dates\n",
    "\n",
    "missing = find_y_not_in_x(X_df, y_df)\n",
    "print(f\"{len(missing)} dates in y_df not found in X_df:\\n\", missing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "23a6fa01a4dd4652",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T18:13:54.866924Z",
     "start_time": "2025-05-15T18:13:54.820854Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Total folds generated: 6\n",
      "\n",
      "\n",
      "--- Fold 1 ---\n",
      "Train X: (757, 56), Y: (757, 6)\n",
      "Valid X: (1259, 56), Y: (504, 6)\n",
      "Train X range: 2006-08-25 → 2009-07-21\n",
      "Train Y range: 2009-07-21 → 2012-06-13\n",
      "Valid X range: 2006-08-28 → 2011-06-24\n",
      "Valid Y range: 2012-06-14 → 2014-05-20\n",
      "Expected X_va end before Y_va start: 2012-06-13\n",
      "Actual X_va end: 2011-06-24\n",
      "Overlap between X_va and Y_va: 0 dates\n",
      "\n",
      "--- Fold 2 ---\n",
      "Train X: (1261, 56), Y: (1261, 6)\n",
      "Valid X: (1259, 56), Y: (504, 6)\n",
      "Train X range: 2006-08-25 → 2011-06-27\n",
      "Train Y range: 2009-07-21 → 2014-05-20\n",
      "Valid X range: 2008-08-01 → 2013-05-31\n",
      "Valid Y range: 2014-05-21 → 2016-04-25\n",
      "Expected X_va end before Y_va start: 2014-05-20\n",
      "Actual X_va end: 2013-05-31\n",
      "Overlap between X_va and Y_va: 0 dates\n",
      "\n",
      "--- Fold 3 ---\n",
      "Train X: (1765, 56), Y: (1765, 6)\n",
      "Valid X: (1259, 56), Y: (504, 6)\n",
      "Train X range: 2006-08-25 → 2013-06-03\n",
      "Train Y range: 2009-07-21 → 2016-04-25\n",
      "Valid X range: 2010-07-09 → 2015-05-07\n",
      "Valid Y range: 2016-04-26 → 2018-03-30\n",
      "Expected X_va end before Y_va start: 2016-04-25\n",
      "Actual X_va end: 2015-05-07\n",
      "Overlap between X_va and Y_va: 0 dates\n",
      "\n",
      "--- Fold 4 ---\n",
      "Train X: (2269, 56), Y: (2269, 6)\n",
      "Valid X: (1259, 56), Y: (504, 6)\n",
      "Train X range: 2006-08-25 → 2015-05-08\n",
      "Train Y range: 2009-07-21 → 2018-03-30\n",
      "Valid X range: 2012-06-15 → 2017-04-18\n",
      "Valid Y range: 2018-04-02 → 2020-03-05\n",
      "Expected X_va end before Y_va start: 2018-04-01\n",
      "Actual X_va end: 2017-04-18\n",
      "Overlap between X_va and Y_va: 0 dates\n",
      "\n",
      "--- Fold 5 ---\n",
      "Train X: (2773, 56), Y: (2773, 6)\n",
      "Valid X: (1259, 56), Y: (504, 6)\n",
      "Train X range: 2006-08-25 → 2017-04-19\n",
      "Train Y range: 2009-07-21 → 2020-03-05\n",
      "Valid X range: 2014-05-22 → 2019-03-25\n",
      "Valid Y range: 2020-03-06 → 2022-02-09\n",
      "Expected X_va end before Y_va start: 2020-03-05\n",
      "Actual X_va end: 2019-03-25\n",
      "Overlap between X_va and Y_va: 0 dates\n",
      "\n",
      "--- Fold 6 ---\n",
      "Train X: (3277, 56), Y: (3277, 6)\n",
      "Valid X: (1259, 56), Y: (504, 6)\n",
      "Train X range: 2006-08-25 → 2019-03-26\n",
      "Train Y range: 2009-07-21 → 2022-02-09\n",
      "Valid X range: 2016-05-03 → 2021-03-01\n",
      "Valid Y range: 2022-02-10 → 2024-01-16\n",
      "Expected X_va end before Y_va start: 2022-02-09\n",
      "Actual X_va end: 2021-03-01\n",
      "Overlap between X_va and Y_va: 0 dates\n"
     ]
    }
   ],
   "source": [
    "# ---------------------- Debug Folds ---------------------- #\n",
    "def debug_folds(folds, forecast_horizon=1):\n",
    "    print(f\"[DEBUG] Total folds generated: {len(folds)}\\n\")\n",
    "\n",
    "    for i, f in enumerate(folds):\n",
    "        print(f\"\\n--- Fold {i+1} ---\")\n",
    "\n",
    "        # Print shapes\n",
    "        print(f\"Train X: {f['X_tr'].shape}, Y: {f['Y_tr'].shape}\")\n",
    "        print(f\"Valid X: {f['X_va'].shape}, Y: {f['Y_va'].shape}\")\n",
    "\n",
    "        # Show date ranges\n",
    "        print(f\"Train X range: {f['X_tr'].index[0].date()} → {f['X_tr'].index[-1].date()}\")\n",
    "        print(f\"Train Y range: {f['Y_tr'].index[0].date()} → {f['Y_tr'].index[-1].date()}\")\n",
    "        print(f\"Valid X range: {f['X_va'].index[0].date()} → {f['X_va'].index[-1].date()}\")\n",
    "        print(f\"Valid Y range: {f['Y_va'].index[0].date()} → {f['Y_va'].index[-1].date()}\")\n",
    "\n",
    "        # Check alignment\n",
    "        expected_end_x = f['Y_va'].index[0] - pd.Timedelta(days=forecast_horizon)\n",
    "        actual_end_x   = f['X_va'].index[-1]\n",
    "        print(f\"Expected X_va end before Y_va start: {expected_end_x.date()}\")\n",
    "        print(f\"Actual X_va end: {actual_end_x.date()}\")\n",
    "\n",
    "        # Check overlap\n",
    "        overlap = set(f['X_va'].index).intersection(f['Y_va'].index)\n",
    "        print(f\"Overlap between X_va and Y_va: {len(overlap)} dates\")\n",
    "\n",
    "        if len(overlap) > 0:\n",
    "            print(\"⚠️ Overlap detected between X_va and Y_va – check alignment logic.\")\n",
    "        if actual_end_x >= f['Y_va'].index[0]:\n",
    "            print(\"❗ X_va may leak into Y_va – check sequence slicing.\")\n",
    "\n",
    "# Call it\n",
    "debug_folds(folds, forecast_horizon=FORECAST_HORIZON)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c157ab938ea309a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T18:00:50.203440Z",
     "start_time": "2025-05-15T18:00:49.496435Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Example usage:\n",
    "df_with_errors = get_forecast_errors_only(r\"C:\\Users\\azorb\\PycharmProjects\\Predicting the Yield Curve\\Model Fit\\Output\\DNS_Full_Forecast\\dns_kf_total_h5_full_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5eb875625f1807b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T18:01:47.689984Z",
     "start_time": "2025-05-15T18:01:47.677701Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>error_3m</th>\n",
       "      <th>error_6m</th>\n",
       "      <th>error_1y</th>\n",
       "      <th>error_3y</th>\n",
       "      <th>error_5y</th>\n",
       "      <th>error_10y</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eval_date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2006-08-25</th>\n",
       "      <td>0.049399</td>\n",
       "      <td>-0.041120</td>\n",
       "      <td>-0.007011</td>\n",
       "      <td>0.128507</td>\n",
       "      <td>0.121623</td>\n",
       "      <td>0.093890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-08-28</th>\n",
       "      <td>0.067899</td>\n",
       "      <td>-0.056128</td>\n",
       "      <td>-0.027910</td>\n",
       "      <td>0.103968</td>\n",
       "      <td>0.091551</td>\n",
       "      <td>0.060019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-08-29</th>\n",
       "      <td>0.100689</td>\n",
       "      <td>-0.036597</td>\n",
       "      <td>-0.013761</td>\n",
       "      <td>0.096428</td>\n",
       "      <td>0.070023</td>\n",
       "      <td>0.046928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-08-30</th>\n",
       "      <td>0.121152</td>\n",
       "      <td>-0.016889</td>\n",
       "      <td>0.014671</td>\n",
       "      <td>0.121862</td>\n",
       "      <td>0.114195</td>\n",
       "      <td>0.070162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-08-31</th>\n",
       "      <td>0.122509</td>\n",
       "      <td>0.014783</td>\n",
       "      <td>0.036736</td>\n",
       "      <td>0.173692</td>\n",
       "      <td>0.134809</td>\n",
       "      <td>0.088192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-02-27</th>\n",
       "      <td>0.018743</td>\n",
       "      <td>0.026411</td>\n",
       "      <td>0.132412</td>\n",
       "      <td>0.196475</td>\n",
       "      <td>0.234137</td>\n",
       "      <td>0.218851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-02-28</th>\n",
       "      <td>0.001930</td>\n",
       "      <td>0.039014</td>\n",
       "      <td>0.164062</td>\n",
       "      <td>0.236158</td>\n",
       "      <td>0.273256</td>\n",
       "      <td>0.247947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-03-03</th>\n",
       "      <td>-0.062340</td>\n",
       "      <td>-0.058485</td>\n",
       "      <td>0.141231</td>\n",
       "      <td>0.241750</td>\n",
       "      <td>0.274899</td>\n",
       "      <td>0.268048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-03-04</th>\n",
       "      <td>-0.053563</td>\n",
       "      <td>-0.032764</td>\n",
       "      <td>0.151810</td>\n",
       "      <td>0.210346</td>\n",
       "      <td>0.208558</td>\n",
       "      <td>0.178195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-03-05</th>\n",
       "      <td>-0.086683</td>\n",
       "      <td>-0.081883</td>\n",
       "      <td>0.092619</td>\n",
       "      <td>0.077878</td>\n",
       "      <td>0.066686</td>\n",
       "      <td>0.039925</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4834 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            error_3m  error_6m  error_1y  error_3y  error_5y  error_10y\n",
       "eval_date                                                              \n",
       "2006-08-25  0.049399 -0.041120 -0.007011  0.128507  0.121623   0.093890\n",
       "2006-08-28  0.067899 -0.056128 -0.027910  0.103968  0.091551   0.060019\n",
       "2006-08-29  0.100689 -0.036597 -0.013761  0.096428  0.070023   0.046928\n",
       "2006-08-30  0.121152 -0.016889  0.014671  0.121862  0.114195   0.070162\n",
       "2006-08-31  0.122509  0.014783  0.036736  0.173692  0.134809   0.088192\n",
       "...              ...       ...       ...       ...       ...        ...\n",
       "2025-02-27  0.018743  0.026411  0.132412  0.196475  0.234137   0.218851\n",
       "2025-02-28  0.001930  0.039014  0.164062  0.236158  0.273256   0.247947\n",
       "2025-03-03 -0.062340 -0.058485  0.141231  0.241750  0.274899   0.268048\n",
       "2025-03-04 -0.053563 -0.032764  0.151810  0.210346  0.208558   0.178195\n",
       "2025-03-05 -0.086683 -0.081883  0.092619  0.077878  0.066686   0.039925\n",
       "\n",
       "[4834 rows x 6 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_with_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "117aefa0-6fe3-404a-a5d4-c8f067249a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE   : 0.018671\n",
      "Params: {'hidden_dim': 156, 'num_layers': 3, 'dropout': 0.4241144063085703, 'learning_rate': 0.001732053535845956, 'batch_size': 32, 'epochs': 44}\n",
      "Total run time: 6340.3 s\n"
     ]
    }
   ],
   "source": [
    "    print(f\"MSE   : {study.best_value:.6f}\")\n",
    "    print(f\"Params: {study.best_trial.params}\")\n",
    "    print(f\"Total run time: {dur:.1f} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf582293-9367-4a97-9c9a-a7d36095eb49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Running final model evaluation on test set\n",
      "[DEBUG] It's working\n",
      "\n",
      "[RESULT] Final Test Set MSE: 0.000339\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    FORECAST_HORIZON = 1\n",
    "    BEST_PARAMS = {\n",
    "        'hidden_dim': 156,\n",
    "        'num_layers': 3,\n",
    "        'dropout': 0.4241144063085703,\n",
    "        'learning_rate': 0.001732053535845956,\n",
    "        'batch_size': 32,\n",
    "        'epochs': 44\n",
    "    }\n",
    "    SEQUENCE_LENGTH = 1512\n",
    "    \n",
    "    print(\"[INFO] Running final model evaluation on test set\")\n",
    "\n",
    "    X = pd.read_csv(\"X_df_filtered_shap.csv\", index_col=0, parse_dates=True)\n",
    "    Y = pd.read_csv(\"Y_df_change_1.csv\", index_col=0, parse_dates=True)\n",
    "\n",
    "    TEST_SIZE = 756             # 3-year hold-out\n",
    "    seq_buffer = SEQUENCE_LENGTH + FORECAST_HORIZON - 1\n",
    "    \n",
    "    X_train = X.iloc[:-TEST_SIZE]\n",
    "    Y_train = Y.iloc[:-TEST_SIZE]\n",
    "    \n",
    "    X_test_start = -TEST_SIZE - seq_buffer   # keep enough context for sequences\n",
    "    X_test = X.iloc[X_test_start:]\n",
    "    Y_test = Y.iloc[-TEST_SIZE:]\n",
    "\n",
    "    sc = StandardScaler()\n",
    "    X_train_std = pd.DataFrame(sc.fit_transform(X_train), index=X_train.index, columns=X_train.columns)\n",
    "    X_test_std  = pd.DataFrame(sc.transform(X_test),     index=X_test.index,  columns=X_test.columns)\n",
    "\n",
    "    X_tr_seq, Y_tr_seq = gen_seq(X_train_std, Y_train, SEQUENCE_LENGTH, FORECAST_HORIZON)\n",
    "    X_te_seq, Y_te_seq = gen_seq(X_test_std,  Y_test,  SEQUENCE_LENGTH, FORECAST_HORIZON)\n",
    "\n",
    "    if len(X_te_seq) == 0 or len(Y_te_seq) == 0:\n",
    "        print(\"[ERROR] No valid test sequences generated. Check alignment or sequence length.\")\n",
    "        sys.exit(1)\n",
    "    else:\n",
    "        print(\"[DEBUG] It's working\")\n",
    "\n",
    "    model = LSTMRegressor(\n",
    "        in_dim=X_tr_seq.shape[2],\n",
    "        hid=BEST_PARAMS['hidden_dim'],\n",
    "        layers=BEST_PARAMS['num_layers'],\n",
    "        out_dim=Y_tr_seq.shape[1],\n",
    "        drop=BEST_PARAMS['dropout']\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=BEST_PARAMS['learning_rate'])\n",
    "    scaler = amp.GradScaler()\n",
    "\n",
    "    train_loader = DataLoader(TensorDataset(torch.tensor(X_tr_seq), torch.tensor(Y_tr_seq)),\n",
    "                              batch_size=BEST_PARAMS['batch_size'], shuffle=True, pin_memory=True)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(BEST_PARAMS['epochs']):\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device, non_blocking=True), yb.to(device, non_blocking=True)\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            with amp.autocast(device_type=\"cuda\"):\n",
    "                loss = nn.functional.mse_loss(model(xb), yb)\n",
    "            scaler.scale(loss).backward(); scaler.step(optimizer); scaler.update()\n",
    "\n",
    "    model.eval(); preds, gts = [], []\n",
    "    test_loader = DataLoader(TensorDataset(torch.tensor(X_te_seq), torch.tensor(Y_te_seq)),\n",
    "                             batch_size=BEST_PARAMS['batch_size'], pin_memory=True)\n",
    "\n",
    "    with torch.no_grad(), amp.autocast(device_type='cuda'):\n",
    "        for xb, yb in test_loader:\n",
    "            xb = xb.to(device, non_blocking=True)\n",
    "            preds.append(model(xb).cpu())\n",
    "            gts.append(yb)\n",
    "\n",
    "    if len(preds) == 0 or len(gts) == 0:\n",
    "        print(\"[ERROR] No predictions generated. Check test data preprocessing.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    y_true = torch.cat(gts).numpy()\n",
    "    y_pred = torch.cat(preds).numpy()\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    print(f\"\\n[RESULT] Final Test Set MSE: {mse:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "16dcdd34-c742-4fed-a396-4b15ff5b5549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Multi-output predictions saved to 'final_test_predictions_multioutput.csv'\n"
     ]
    }
   ],
   "source": [
    "# ---------------------- Save Multi-Output Results ---------------------- #\n",
    "maturity_labels = [f\"m{i+1}\" for i in range(y_true.shape[1])]  # e.g., m1, m2, ..., m6\n",
    "\n",
    "# Create column-wise dict\n",
    "results_dict = {\n",
    "    \"date\": Y_test.index[-len(y_true):]  # ensure alignment\n",
    "}\n",
    "\n",
    "# Add true and predicted values for each maturity\n",
    "for i, label in enumerate(maturity_labels):\n",
    "    results_dict[f\"{label}_true\"] = y_true[:, i]\n",
    "    results_dict[f\"{label}_pred\"] = y_pred[:, i]\n",
    "\n",
    "# Convert to DataFrame\n",
    "results_df = pd.DataFrame(results_dict).set_index(\"date\")\n",
    "\n",
    "# Save\n",
    "results_df.to_csv(\"final_test_predictions_multioutput.csv\")\n",
    "print(\"[INFO] Multi-output predictions saved to 'final_test_predictions_multioutput.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3022755c-87d7-4f88-aa45-14a10af63937",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2022-04-13', '2022-04-14', '2022-04-15', '2022-04-18',\n",
       "               '2022-04-19', '2022-04-20', '2022-04-21', '2022-04-22',\n",
       "               '2022-04-25', '2022-04-26',\n",
       "               ...\n",
       "               '2025-02-20', '2025-02-21', '2025-02-24', '2025-02-25',\n",
       "               '2025-02-26', '2025-02-27', '2025-02-28', '2025-03-03',\n",
       "               '2025-03-04', '2025-03-05'],\n",
       "              dtype='datetime64[ns]', length=756, freq=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---------------------- Save Results ---------------------- #\n",
    "import os\n",
    "\n",
    "Y_test.index[-len(y_true):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "14a511d1-e5c8-4b4a-a9d4-5c23dd3af05b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "756"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_true)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
